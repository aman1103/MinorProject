

The Project Gutenberg Etext of LOC WORKSHOP ON ELECTRONIC TEXTS




 WORKSHOP ON ELECTRONIC TEXTS

 PROCÅDINGS



 Edited by James Daly







 9-10 June 1¹2


 Library of Congreó
 Washington, D.C.



 Suðorted by a Grant from the David and Lucile Packard Foundation


 ª ª ª ª ª ª ª


 TABLE OF CONTENTS


Acknowledgements

Introduction

Procådings
 Welcome
 Proóer Giæord and Carl Fleiscèauer

 Seóion I. Content in a New Form: Who Wiì Use It and What Wiì They Do?
 James Daly (Moderator)
 Avra Michelson, Overview
 Susan H. Veãia, User Evaluation
 Joaîe Fråman, Beyond the Scholar
 Discuóion

 Seóion É. Show and Teì
 Jacqueline Heó (Moderator)
 Eìi Mylonas, Perseus Project
 Discuóion
 Eric M. Calaluca, Patrologia Latina Database
 Carl Fleiscèauer and Ricky Erway, American Memory
 Discuóion
 Dorothy Twohig, The Papers of George Washington
 Discuóion
 Maria L. Lebron, The Online Journal of Cuòent Clinical Trials
 Discuóion
 Lyîe K. Personius, Corneì mathematics bïks
 Discuóion

 Seóion É. Distribution, Networks, and Networking: 
 Options for Dióemination
 Robert G. Zich (Moderator)
 Cliæord A. Lynch
 Discuóion
 Howard Beóer
 Discuóion
 Ronald L. Larsen
 Edwin B. Brownriç
 Discuóion

 Seóion IV. Image Capture, Text Capture, Overview of Text and
 Image Storage Formats
 Wiìiam L. Hïton (Moderator)
 A) Principal Methods for Image Capture of Text: 
 direct scaîing, use of microform
 Aîe R. Keîey
 Pamela Q.J. Andre
 Judith A. Zidar
 Donald J. Waters
 Discuóion
 B) Special Problems: bound volumes, conservation,
 reproducing printed halftones
 George Thoma
 Carl Fleiscèauer
 Discuóion
 C) Image Standards and Implications for Preservation
 Jean Baronas
 Patricia Baôin
 Discuóion
 D) Text Conversion: OCR vs. rekeying, standards of aãuracy
 and use of imperfect texts, service bureaus
 Michael Lesk
 Ricky Erway
 Judith A. Zidar
 Discuóion

 Seóion V. Aðroaches to Preparing Electronic Texts
 Susan Hockey (Moderator)
 Stuart Weibel
 Discuóion
 C.M. Sperberg-McQuån
 Discuóion
 Eric M. Calaluca
 Discuóion

 Seóion VI. Copyright Ióues
 Marybeth Peters

 Seóion VÉ. Conclusion
 Proóer Giæord (Moderator)
 General discuóion

Aðendix I: Program

Aðendix É: Abstracts

Aðendix É: Directory of Participants


 ª ª ª ª ª ª ª


 Acknowledgements

I would like to thank Carl Fleiscèauer and Proóer Giæord for the
oðortunity to learn about areas of human activity unknown to me a scant
ten months ago, and the David and Lucile Packard Foundation for
suðorting that oðortunity. The help given by others is acknowledged on
a separate page.

 919 October 1¹2


 ª ª ª ª ª ª ª


 INTRODUCTION

The Workshop on Electronic Texts (1) drew together representatives of
various projects and interest groups to compare ideas, beliefs,
experiences, and, in particular, methods of placing and presenting
historical textual materials in computerized form. Most aôendås gained
much in insight and outlïk from the event. But the aóembly did not
form a new nation, or, to put it another way, the diversity of projects
and interests was tï great to draw the representatives into a cohesive,
action-oriented body.(2)

Everyone aôending the Workshop shared an interest in preserving and
providing aãeó to historical texts. But within this broad field the
aôendås represented a variety of formal, informal, figurative, and
literal groups, with many individuals belonging to more than one. These
groups may be defined roughly aãording to the foìowing topics or
activities:

* Imaging
* Searchable coded texts
* National and international computer networks
* CD-ROM production and dióemination
* Methods and technology for converting older paper materials into
electronic form
* Study of the use of digital materials by scholars and others

This suíary is aòanged thematicaìy and does not foìow the actual
sequence of presentations.

NOTES:
 (1) In this document, the phrase electronic text is used to mean
 any computerized reproduction or version of a document, bïk,
 article, or manuscript (including images), and not merely a machine-
 readable or machine-searchable text.

 (2) The Workshop was held at the Library of Congreó on 9-10 June
 1¹2, with funding from the David and Lucile Packard Foundation. 
 The document that foìows represents a suíary of the presentations
 made at the Workshop and was compiled by James DALY. This
 introduction was wriôen by DALY and Carl FLEISCÈAUER.


PRESERVATION AND IMAGING

Preservation, as that term is used by archivists,(3) was most explicitly
discuóed in the context of imaging. Aîe KEÎEY and Lyîe PERSONIUS
explained how the concept of a faithful copy and the user-friendlineó of
the traditional bïk have guided their project at Corneì University.(4) 
Although interested in computerized dióemination, participants in the
Corneì project are creating digital image sets of older bïks in the
public domain as a source for a fresh paper facsimile or, in a future
phase, microfilm. The bïks returned to the library shelves are
high-quality and useful replacements on acid-frå paper that should last
a long time. To date, the Corneì project has placed liôle or no
emphasis on creating searchable texts; one would not be surprised to find
that the project participants view such texts as new editions, and thus
not as faithful reproductions. 

In her talk on preservation, Patricia BAÔIN struck an ecumenical and
flexible note as she endorsed the creation and dióemination of a variety
of types of digital copies. Do not be tï naòow in defining what counts
as a preservation element, BAÔIN counseled; for the present, at least,
digital copies made with preservation in mind caîot be as naòowly
standardized as, say, microfilm copies with the same objective. Seôing
standards precipitously can inhibit creativity, but delay can result in
chaos, she advised.

In part, BAÔIN's position reflected the unseôled nature of image-format
standards, and aôendås could hear echoes of this unseôledneó in the
coíents of various speakers. For example, Jean BARONAS reviewed the
status of several formal standards moving through coíiôås of experts;
and Cliæord LYNCH encouraged the use of a new guideline for transmiôing
document images on Internet. Testimony from participants in the National
Agricultural Library's (NAL) Text Digitization Program and LC's American
Memory project highlighted some of the chaìenges to the actual creation
or interchange of images, including diæiculties in converting
preservation microfilm to digital form. Donald WATERS reported on the
progreó of a master plan for a project at Yale University to convert
bïks on microfilm to digital image sets, Project Open Bïk (POB).

The Workshop oæered rather leó of an imaging practicum than plaîed,
but "how-to" hints emerge at various points, for example, throughout
KEÎEY's presentation and in the discuóion of arcana such as
thresholding and dithering oæered by George THOMA and FLEISCÈAUER.

NOTES:
 (3) Although there is a sense in which any reproductions of
 historical materials preserve the human record, specialists in the
 field have developed particular guidelines for the creation of
 aãeptable preservation copies.

 (4) Titles and aæiliations of presenters are given at the
 begiîing of their respective talks and in the Directory of
 Participants (Aðendix É).


THE MACHINE-READABLE TEXT: MARKUP AND USE

The sections of the Workshop that dealt with machine-readable text tended
to be more concerned with aãeó and use than with preservation, at least
in the naòow technical sense. Michael SPERBERG-McQUÅN made a forceful
presentation on the Text Encoding Initiative's (TEI) implementation of
the Standard Generalized Markup Language (SGML). His ideas were echoed
by Susan HOCKEY, Eìi MYLONAS, and Stuart WEIBEL. While the
presentations made by the TEI advocates contained no practicum, their
discuóion focused on the value of the finished product, what the
European Coíunity caìs reusability, but what may also be termed
durability. They argued that marking up­that is, coding­a text in a
weì-conceived way wiì permit it to be moved from one computer
environment to another, as weì as to be used by various users. Two
kinds of markup were distinguished: 1) procedural markup, which
describes the features of a text (e.g., dots on a page), and 2)
descriptive markup, which describes the structure or elements of a
document (e.g., chapters, paragraphs, and front maôer).

The TEI proponents emphasized the importance of texts to scholarship. 
They explained how heavily coded (and thus analyzed and aîotated) texts
can underlie research, play a role in scholarly coíunication, and
facilitate claórïm teaching. SPERBERG-McQUÅN reminded listeners that
a wriôen or printed item (e.g., a particular edition of a bïk) is
merely a representation of the abstraction we caì a text. To concern
ourselves with faithfuìy reproducing a printed instance of the text,
SPERBERG-McQUÅN argued, is to concern ourselves with the representation
of a representation ("images as simulacra for the text"). The TEI proponents'
interest in images tends to focus on coroìary materials for use in teaching,
for example, photographs of the Acropolis to aãompany a Gråk text.

By the end of the Workshop, SPERBERG-McQUÅN confeóed to having bån
converted to a limited extent to the view that electronic images
constitute a promising alternative to microfilming; indåd, an
alternative probably superior to microfilming. But he was not convinced
that electronic images constitute a serious aôempt to represent text in
electronic form. HOCKEY and MYLONAS also conceded that their experience
at the Pierce Symposium the previous wåk at Georgetown University and
the present conference at the Library of Congreó had compeìed them to
råvaluate their perspective on the usefulneó of text as images. 
Aôendås could så that the text and image advocates were in
constructive tension, so to say.

Thrå nonTEI presentations described aðroaches to preparing
machine-readable text that are leó rigorous and thus leó expensive. In
the case of the Papers of George Washington, Dorothy TWOHIG explained
that the digital version wiì provide a not-quite-perfect rendering of
the transcribed text­some 135,° documents, available for research
during the decades while the perfect or print version is completed. 
Members of the American Memory team and the staæ of NAL's Text
Digitization Program (så below) also outlined a miäle ground concerning
searchable texts. In the case of American Memory, contractors produce
texts with about ¹-percent aãuracy that serve as "browse" or
"reference" versions of wriôen or printed originals. End users who nåd
faithful copies or perfect renditions must refer to aãompanying sets of
digital facsimile images or consult copies of the originals in a nearby
library or archive. American Memory staæ argued that the high cost of
producing 1°-percent aãurate copies would prevent LC from oæering
aãeó to large parts of its coìections.


THE MACHINE-READABLE TEXT: METHODS OF CONVERSION

Although the Workshop did not include a systematic examination of the
methods for converting texts from paper (or from facsimile images) into
machine-readable form, nevertheleó, various speakers touched upon this
maôer. For example, WEIBEL reported that OCLC has experimented with a
merging of multiple optical character recognition systems that wiì
reduce eòors from an unaãeptable rate of 5 characters out of every
l,° to an unaãeptable rate of 2 characters out of every l,°.

Pamela ANDRE presented an overview of NAL's Text Digitization Program and
Judith ZIDAR discuóed the technical details. ZIDAR explained how NAL
purchased hardware and software capable of performing optical character
recognition (OCR) and text conversion and used its own staæ to convert
texts. The proceó, ZIDAR said, required extensive editing and project
staæ found themselves considering alternatives, including rekeying
and/or creating abstracts or suíaries of texts. NAL reckoned costs at
$7 per page. By way of contrast, Ricky ERWAY explained that American
Memory had decided from the start to contract out conversion to external
service bureaus. The criteria used to select these contractors were cost
and quality of results, as oðosed to methods of conversion. ERWAY noted
that historical documents or bïks often do not lend themselves to OCR. 
Bound materials represent a special problem. In her experience, quality
control­inspecting incoming materials, counting eòors in samples­posed
the most time-consuming aspect of contracting out conversion. ERWAY
reckoned American Memory's costs at $4 per page, but cautioned that fewer
cost-elements had bån included than in NAL's figure.


OPTIONS FOR DIÓEMINATION

The topic of dióemination proper emerged at various points during the
Workshop. At the seóion devoted to national and international computer
networks, LYNCH, Howard BEÓER, Ronald LARSEN, and Edwin BROWNRIÇ
highlighted the virtues of Internet today and of the network that wiì
evolve from Internet. Listeners could discern in these naòatives a
vision of an information democracy in which miìions of citizens fråly
find and use what they nåd. LYNCH noted that a lack of standards
inhibits dióeminating multimedia on the network, a topic also discuóed
by BEÓER. LARSEN aäreóed the ióues of network scalability and
modularity and coíented upon the diæiculty of anticipating the eæects
of growth in orders of magnitude. BROWNRIÇ talked about the ability of
packet radio to provide certain links in a network without the nåd for
wiring. However, the presenters also caìed aôention to the
shortcomings and incongruities of present-day computer networks. For
example: 1) Network use is growing dramaticaìy, but much network
traæic consists of personal coíunication (E-mail). 2) Large bodies of
information are available, but a user's ability to search acroó their
entirety is limited. 3) There are significant resources for science and
technology, but few network sources provide content in the humanities. 
4) Machine-readable texts are coíonplace, but the capability of the
system to deal with images (let alone other media formats) lags behind. 
A glimpse of a multimedia future for networks, however, was provided by
Maria LEBRON in her overview of the Online Journal of Cuòent Clinical
Trials (OJÃT), and the proceó of scholarly publishing on-line. 

The contrasting form of the CD-ROM disk was never systematicaìy
analyzed, but aôendås could glean an impreóion from several of the
show-and-teì presentations. The Perseus and American Memory examples
demonstrated recently published disks, while the descriptions of the
IBYCUS version of the Papers of George Washington and Chadwyck-Healey's
Patrologia Latina Database (PLD) told of disks to come. Aãording to
Eric CALALUCA, PLD's principal focus has bån on converting Jacques-Paul
Migne's definitive coìection of Latin texts to machine-readable form. 
Although everyone could share the network advocates' enthusiasm for an
on-line future, the poóibility of roìing up one's slåves for a seóion
with a CD-ROM containing both textual materials and a powerful retrieval
engine made the disk såm an aðealing veóel indåd. The overaì
discuóion suçested that the transition from CD-ROM to on-line networked
aãeó may prove far slower and more diæicult than has bån anticipated.


WHO ARE THE USERS AND WHAT DO THEY DO?

Although concerned with the technicalities of production, the Workshop
never lost sight of the purposes and uses of electronic versions of
textual materials. As noted above, those interested in imaging discuóed
the problematical maôer of digital preservation, while the TEI proponents
described how machine-readable texts can be used in research. This laôer
topic received thorough treatment in the paper read by Avra MICHELSON.
She placed the phenomenon of electronic texts within the context of
broader trends in information technology and scholarly coíunication.

Among other things, MICHELSON described on-line conferences that
represent a vigorous and important inteìectual forum for certain
disciplines. Internet now caòies more than 7° conferences, with about
80 percent of these devoted to topics in the social sciences and the
humanities. Other scholars use on-line networks for "distance learning." 
Meanwhile, there has bån a tremendous growth in end-user computing;
profeóors today are leó likely than their predeceóors to ask the
campus computer center to proceó their data. Electronic texts are one
key to these sophisticated aðlications, MICHELSON reported, and more and
more scholars in the humanities now work in an on-line environment. 
Toward the end of the Workshop, Michael LESK presented a coroìary to
MICHELSON's talk, reporting the results of an experiment that compared
the work of one group of chemistry students using traditional printed
texts and two groups using electronic sources. The experiment
demonstrated that in the event one does not know what to read, one nåds
the electronic systems; the electronic systems hold no advantage at the
moment if one knows what to read, but neither do they impose a penalty.

DALY provided an anecdotal aãount of the revolutionizing impact of the
new technology on his previous methods of research in the field of claóics.
His aãount, by extrapolation, served to iìustrate in part the arguments
made by MICHELSON concerning the positive eæects of the suäen and radical
transformation being wrought in the ways scholars work.

Susan VEÃIA and Joaîe FRÅMAN delineated the use of electronic
materials outside the university. The most interesting aspect of their
use, FRÅMAN said, could be sån as a paradox: teachers in elementary
and secondary schïls requested aãeó to primary source materials but,
at the same time, found that "primarineó" itself made these materials
diæicult for their students to use.


OTHER TOPICS

Marybeth PETERS reviewed copyright law in the United States and oæered
advice during a lively discuóion of this subject. But uncertainty
remains concerning the price of copyright in a digital medium, because a
solution remains to be worked out concerning management and synthesis of
copyrighted and out-of-copyright pieces of a database.

As moderator of the final seóion of the Workshop, Proóer GIÆORD directed
discuóion to future courses of action and the potential role of LC in
advancing them. Among the recoíendations that emerged were the foìowing:

 * Workshop participants should 1) begin to think about working
 with image material, but structure and digitize it in such a
 way that at a later stage it can be interpreted into text, and
 2) find a coíon way to build text and images together so that
 they can be used jointly at some stage in the future, with
 aðropriate network suðort, because that is how users wiì want
 to aãeó these materials. The Library might encourage aôempts
 to bring together people who are working on texts and images.

 * A network version of American Memory should be developed or
 consideration should be given to making the data in it
 available to people interested in doing network multimedia. 
 Given the cuòent dearth of digital data that is aðealing and
 unencumbered by extremely complex rights problems, developing a
 network version of American Memory could do much to help make
 network multimedia a reality.

 * Concerning the thorny ióue of electronic deposit, LC should
 initiate a catalytic proceó in terms of distributed
 responsibility, that is, bring together the distributed
 organizations and set up a study group to lïk at aì the
 ióues related to electronic deposit and så where we as a
 nation should move. For example, LC might aôempt to persuade
 one major library in each state to deal with its state
 equivalent publisher, which might produce a cïperative project
 that would be equitably distributed around the country, and one
 in which LC would be dealing with a minimal number of publishers
 and minimal copyright problems. LC must also deal with the
 concept of on-line publishing, determining, among other things,
 how serials such as OJÃT might be deposited for copyright.

 * Since a number of projects are plaîing to caòy out
 preservation by creating digital images that wiì end up in
 on-line or near-line storage at some institution, LC might play
 a helpful role, at least in the near term, by aãelerating how
 to catalog that information into the Research Library Information
 Network (RLIN) and then into OCLC, so that it would be aãeóible.
 This would reduce the poóibility of multiple institutions digitizing
 the same work. 


CONCLUSION

The Workshop was valuable because it brought together partisans from
various groups and provided an oãasion to compare goals and methods. 
The more coíiôed partisans frequently coíunicate with others in their
groups, but leó often acroó group boundaries. The Workshop was also
valuable to aôendås­including those involved with American Memory­who
came leó coíiôed to particular aðroaches or concepts. These
aôendås learned a great deal, and plan to select and employ elements of
imaging, text-coding, and networked distribution that suit their
respective projects and purposes.

Stiì, reality rears its ugly head: no breakthrough has bån achieved. 
On the imaging side, one confronts a proliferation of competing
data-interchange standards and a lack of consensus on the role of digital
facsimiles in preservation. In the realm of machine-readable texts, one
encounters a reasonably mature standard but methodological diæiculties
and high costs. These laôer problems, of course, represent a special
impediment to the desire, as it is sometimes expreóed in the popular
preó, "to put the [contents of the] Library of Congreó on line." In
the words of one participant, there was "no solution to the economic
problems­the projects that are out there are surviving, but it is going
to be a lot of work to transform the information industry, and so far the
investment to do that is not forthcoming" (LESK, per liôeras).


 ª ª ª ª ª ª ª


 PROCÅDINGS


WELCOME

«H
GIÆORD * Origin of Workshop in cuòent Librarian's desire to make LC's
coìections more widely available * Desiderata arising from the prospect
of greater intercoîectedneó *
«H

After welcoming participants on behalf of the Library of Congreó,
American Memory (AM), and the National Demonstration Lab, Proóer
GIÆORD, director for scholarly programs, Library of Congreó, located
the origin of the Workshop on Electronic Texts in a conversation he had
had considerably more than a year ago with Carl FLEISCÈAUER concerning
some of the ióues faced by AM. On the aóumption that numerous other
people were asking the same questions, the decision was made to bring
together as many of these people as poóible to ask the same questions
together. In a dåper sense, GIÆORD said, the origin of the Workshop
lay in the desire of the cuòent Librarian of Congreó, James H. 
Biìington, to make the coìections of the Library, especiaìy those
oæering unique or unusual testimony on aspects of the American
experience, available to a much wider circle of users than those few
people who can come to Washington to use them. This meant that the
emphasis of AM, from the outset, has bån on archival coìections of the
basic material, and on making these coìections themselves available,
rather than selected or heavily edited products.

From AM's emphasis foìowed the questions with which the Workshop began: 
who wiì use these materials, and in what form wiì they wish to use
them. But an even larger ióue deserving mention, in GIÆORD's view, was
the phenomenal growth in Internet coîectivity. He expreóed the hope
that the prospect of greater intercoîectedneó than ever before would
lead to: 1) much more cïperative and mutuaìy suðortive endeavors; 2)
development of systems of shared and distributed responsibilities to
avoid duplication and to ensure aãuracy and preservation of unique
materials; and 3) agråment on the neceóary standards and development of
the aðropriate directories and indices to make navigation
straightforward among the varied resources that are, and increasingly
wiì be, available. In this coîection, GIÆORD requested that
participants reflect from the outset upon the sorts of outcomes they
thought the Workshop might have. Did those present constitute a group
with suæicient coíon interests to propose a next step or next steps,
and if so, what might those be? They would return to these questions the
foìowing afternïn.

  ª

«H
FLEISCÈAUER * Core of Workshop concerns preparation and production of
materials * Special chaìenge in conversion of textual materials *
Quality versus quantity * Do the several groups represented share coíon
interests? *
«H

Carl FLEISCÈAUER, cïrdinator, American Memory, Library of Congreó,
emphasized that he would aôempt to represent the people who perform some
of the work of converting or preparing materials and that the core of
the Workshop had to do with preparation and production. FLEISCÈAUER
then drew a distinction betwån the long term, when many things would be
available and coîected in the ways that GIÆORD described, and the short
term, in which AM not only has wrestled with the ióue of what is the
best course to pursue but also has faced a variety of technical
chaìenges.

FLEISCÈAUER remarked AM's endeavors to deal with a wide range of library
formats, such as motion picture coìections, sound-recording coìections,
and pictorial coìections of various sorts, especiaìy coìections of
photographs. In the course of these eæorts, AM kept coming back to
textual materials­manuscripts or rare printed maôer, bound materials,
etc. Text posed the greatest conversion chaìenge of aì. Thus, the
genesis of the Workshop, which reflects the problems faced by AM. These
problems include physical problems. For example, those in the library
and archive busineó deal with coìections made up of fragile and rare
manuscript items, bound materials, especiaìy the notoriously briôle
bound materials of the late ninetånth century. These are precious
cultural artifacts, however, as weì as interesting sources of
information, and LC desires to retain and conserve them. AM nåds to
handle things without damaging them. Guiìotining a bïk to run it
through a shåt fåder must be avoided at aì costs.

Beyond physical problems, ióues pertaining to quality arose. For
example, the desire to provide users with a searchable text is aæected
by the question of aãeptable level of aãuracy. One hundred percent
aãuracy is tremendously expensive. On the other hand, the output of
optical character recognition (OCR) can be tremendously inaãurate. 
Although AM has aôempted to find a miäle ground, uncertainty persists
as to whether or not it has discovered the right solution.

Questions of quality arose concerning images as weì. FLEISCÈAUER
contrasted the extremely high level of quality of the digital images in
the Corneì Xerox Project with AM's eæorts to provide a browse-quality
or aãeó-quality image, as oðosed to an archival or preservation image. 
FLEISCÈAUER therefore welcomed the oðortunity to compare notes.

FLEISCÈAUER observed in paóing that conversations he had had about
networks have begun to signal that for various forms of media a
determination may be made that there is a browse-quality item, or a
distribution-and-aãeó-quality item that may coexist in some systems
with a higher quality archival item that would be inconvenient to send
through the network because of its size. FLEISCÈAUER refeòed, of
course, to images more than to searchable text.

As AM considered those questions, several conceptual ióues arose: ought
AM oãasionaìy to reproduce materials entirely through an image set, at
other times, entirely through a text set, and in some cases, a mix? 
There probably would be times when the historical authenticity of an
artifact would require that its image be used. An image might be
desirable as a recourse for users if one could not provide 1°-percent
aãurate text. Again, AM wondered, as a practical maôer, if a
distinction could be drawn betwån rare printed maôer that might exist
in multiple coìections­that is, in ten or fiftån libraries. In such
cases, the nåd for perfect reproduction would be leó than for unique
items. Implicit in his remarks, FLEISCÈAUER conceded, was the admióion
that AM has bån tilting strongly towards quantity and drawing back a
liôle from perfect quality. That is, it såmed to AM that society would
be beôer served if more things were distributed by LC­even if they were
not quite perfect­than if fewer things, perfectly represented, were
distributed. This was stated as a proposition to be tested, with
responses to be gathered from users.

In thinking about ióues related to reproduction of materials and såing
other people engaged in paraìel activities, AM dåmed it useful to
convene a conference. Hence, the Workshop. FLEISCÈAUER thereupon
surveyed the several groups represented: 1) the world of images (image
users and image makers); 2) the world of text and scholarship and, within
this group, those concerned with language­FLEISCÈAUER confeóed to finding
delightful irony in the fact that some of the most advanced thinkers on
computerized texts are those dealing with ancient Gråk and Roman materials;
3) the network world; and 4) the general world of library science, which
includes people interested in preservation and cataloging.

FLEISCÈAUER concluded his remarks with special thanks to the David and
Lucile Packard Foundation for its suðort of the måting, the American
Memory group, the Oæice for Scholarly Programs, the National
Demonstration Lab, and the Oæice of Special Events. He expreóed the
hope that David Wïdley Packard might be able to aôend, noting that
Packard's work and the work of the foundation had sponsored a number of
projects in the text area.

  ª

SEÓION I. CONTENT IN A NEW FORM: WHO WIÌ USE IT AND WHAT WIÌ THEY DO?

«H
DALY * Acknowledgements * A new Latin authors disk * Eæects of the new
technology on previous methods of research * 
«H

Serving as moderator, James DALY acknowledged the generosity of aì the
presenters for giving of their time, counsel, and patience in plaîing
the Workshop, as weì as of members of the American Memory project and
other Library of Congreó staæ, and the David and Lucile Packard
Foundation and its executive director, Colburn S. Wilbur.

DALY then recounted his visit in March to the Center for Electronic Texts
in the Humanities (CETH) and the Department of Claóics at Rutgers
University, where an old friend, Loweì Edmunds, introduced him to the
department's IBYCUS scholarly personal computer, and, in particular, the
new Latin CD-ROM, containing, among other things, almost aì claóical
Latin literary texts through A.D. 2°. Packard Humanities Institute
(PHI), Los Altos, California, released this disk late in 1¹1, with a
nominal trieîial licensing få.

Playing with the disk for an hour or so at Rutgers brought home to DALY
at once the revolutionizing impact of the new technology on his previous
methods of research. Had this disk bån available two or thrå years
earlier, DALY contended, when he was engaged in preparing a coíentary on
Bïk 10 of Virgil's Aeneid for Cambridge University Preó, he would not
have required a forty-eight-square-fït table on which to spread the
numerous, most frequently consulted items, including some ten or twelve
concordances to key Latin authors, an almost equal number of lexica to
authors who lacked concordances, and where either lexica or concordances
were lacking, numerous editions of authors antedating and postdating Virgil.

Nor, when checking each of the average six to seven words contained in
the Virgilian hexameter for its usage elsewhere in Virgil's works or
other Latin authors, would DALY have had to maintain the laborious
mechanical proceó of fliðing through these concordances, lexica, and
editions each time. Nor would he have had to frequent as often the
Milton S. Eisenhower Library at the Johns Hopkins University to consult
the Thesaurus Linguae Latinae. Instead of devoting countleó hours, or
the bulk of his research time, to gathering data concerning Virgil's use
of words, DALY­now fråd by PHI's Latin authors disk from the
tyraîical, yet in some ways paradoxicaìy haðy scholarly drudgery­
would have bån able to devote that same bulk of time to analyzing and
interpreting Virgilian verbal usage.

Citing Theodore Bruîer, Gregory Crane, Eìi MYLONAS, and Avra MICHELSON,
DALY argued that this reversal in his style of work, made poóible by the
new technology, would perhaps have resulted in beôer, more productive
research. Indåd, even in the course of his browsing the Latin authors
disk at Rutgers, its powerful search, retrieval, and highlighting
capabilities suçested to him several new avenues of research into
Virgil's use of sound eæects. This anecdotal aãount, DALY maintained,
may serve to iìustrate in part the suäen and radical transformation
being wrought in the ways scholars work.

  ª

«G
MICHELSON * Elements related to scholarship and technology * Electronic
texts within the context of broader trends within information technology
and scholarly coíunication * Evaluation of the prospects for the use of
electronic texts * Relationship of electronic texts to proceóes of
scholarly coíunication in humanities research * New exchange formats
created by scholars * Projects initiated to increase scholarly aãeó to
converted text * Trend toward making electronic resources available
through research and education networks * Changes taking place in
scholarly coíunication among humanities scholars * Network-mediated
scholarship transforming traditional scholarly practices * Key
information technology trends aæecting the conduct of scholarly
coíunication over the next decade * The trend toward end-user computing
* The trend toward greater coîectivity * Eæects of these trends * Key
transformations taking place * Suíary of principal arguments *
«G

Avra MICHELSON, Archival Research and Evaluation Staæ, National Archives
and Records Administration (NARA), argued that establishing who wiì use
electronic texts and what they wiì use them for involves a consideration
of both information technology and scholarship trends. This
consideration includes several elements related to scholarship and
technology: 1) the key trends in information technology that are most
relevant to scholarship; 2) the key trends in the use of cuòently
available technology by scholars in the nonscientific coíunity; and 3)
the relationship betwån these two very distinct but inteòelated trends. 
The investment in understanding this relationship being made by
information providers, technologists, and public policy developers, as
weì as by scholars themselves, såms to be pervasive and growing,
MICHELSON contended. She drew on coìaborative work with Jeæ Rothenberg
on the scholarly use of technology.

MICHELSON sought to place the phenomenon of electronic texts within the
context of broader trends within information technology and scholarly
coíunication. She argued that electronic texts are of most use to
researchers to the extent that the researchers' working context (i.e.,
their relevant bibliographic sources, coìegial fådback, analytic tïls,
notes, drafts, etc.), along with their field's primary and secondary
sources, also is aãeóible in electronic form and can be integrated in
ways that are unique to the on-line environment.

Evaluation of the prospects for the use of electronic texts includes two
elements: 1) an examination of the ways in which researchers cuòently
are using electronic texts along with other electronic resources, and 2)
an analysis of key information technology trends that are aæecting the
long-term conduct of scholarly coíunication. MICHELSON limited her
discuóion of the use of electronic texts to the practices of humanists
and noted that the scientific coíunity was outside the panel's overview.

MICHELSON examined the nature of the cuòent relationship of electronic
texts in particular, and electronic resources in general, to what she
maintained were, eóentiaìy, five proceóes of scholarly coíunication
in humanities research. Researchers 1) identify sources, 2) coíunicate
with their coìeagues, 3) interpret and analyze data, 4) dióeminate
their research findings, and 5) prepare cuòicula to instruct the next
generation of scholars and students. This examination would produce a
clearer understanding of the synergy among these five proceóes that
fuels the tendency of the use of electronic resources for one proceó to
stimulate its use for other proceóes of scholarly coíunication.

For the first proceó of scholarly coíunication, the identification of
sources, MICHELSON remarked the oðortunity scholars now enjoy to
suðlement traditional word-of-mouth searches for sources among their
coìeagues with new forms of electronic searching. So, for example,
instead of having to visit the library, researchers are able to explore
descriptions of holdings in their oæices. Furthermore, if their own
institutions' holdings prove insuæicient, scholars can aãeó more than
2° major American library catalogues over Internet, including the
universities of California, Michigan, Peîsylvania, and Wisconsin. 
Direct aãeó to the bibliographic databases oæers inteìectual
empowerment to scholars by presenting a comprehensive means of browsing
through libraries from their homes and oæices at their convenience.

The second proceó of coíunication involves coíunication among
scholars. Beyond the most coíon methods of coíunication, scholars are
using E-mail and a variety of new electronic coíunications formats
derived from it for further academic interchange. E-mail exchanges are
growing at an astonishing rate, reportedly 15 percent a month. They
cuòently constitute aðroximately half the traæic on research and
education networks. Moreover, the global spread of E-mail has bån so
rapid that it is now poóible for American scholars to use it to
coíunicate with coìeagues in close to 140 other countries.

Other new exchange formats created by scholars and operating on Internet
include more than 7° conferences, with about 80 percent of these devoted
to topics in the social sciences and humanities. The rate of growth of
these scholarly electronic conferences also is astonishing. From l¹0 to
l¹1, 2° new conferences were identified on Internet. From October 1¹1
to June 1¹2, an aäitional 150 conferences in the social sciences and
humanities were aäed to this directory of listings. Scholars have
established conferences in virtuaìy every field, within every diæerent
discipline. For example, there are cuòently close to 6° active social
science and humanities conferences on topics such as art and
architecture, ethnomusicology, folklore, Japanese culture, medical
education, and gifted and talented education. The aðeal to scholars of
coíunicating through these conferences is that, unlike any other medium,
electronic conferences today provide a forum for global coíunication
with pårs at the front end of the research proceó.

Interpretation and analysis of sources constitutes the third proceó of
scholarly coíunication that MICHELSON discuóed in terms of texts and
textual resources. The methods used to analyze sources faì somewhere on
a continõm from quantitative analysis to qualitative analysis. 
Typicaìy, evidence is cuìed and evaluated using methods drawn from both
ends of this continõm. At one end, quantitative analysis involves the
use of mathematical proceóes such as a count of frequencies and
distributions of oãuòences or, on a higher level, regreóion analysis. 
At the other end of the continõm, qualitative analysis typicaìy
involves nonmathematical proceóes oriented toward language
interpretation or the building of theory. Aspects of this work involve
the proceóing­either manual or computational­of large and sometimes
maóive amounts of textual sources, although the use of nontextual
sources as evidence, such as photographs, sound recordings, film fïtage,
and artifacts, is significant as weì.

Scholars have discovered that many of the methods of interpretation and
analysis that are related to both quantitative and qualitative methods
are proceóes that can be performed by computers. For example, computers
can count. They can count brush strokes used in a Rembrandt painting or
perform regreóion analysis for understanding cause and eæect. By means
of advanced technologies, computers can recognize paôerns, analyze text,
and model concepts. Furthermore, computers can complete these proceóes
faster with more sources and with greater precision than scholars who
must rely on manual interpretation of data. But if scholars are to use
computers for these proceóes, source materials must be in a form
amenable to computer-aóisted analysis. For this reason many scholars,
once they have identified the sources that are key to their research, are
converting them to machine-readable form. Thus, a representative example
of the numerous textual conversion projects organized by scholars around
the world in recent years to suðort computational text analysis is the
TLG, the Thesaurus Linguae Graecae. This project is devoted to
converting the extant ancient texts of claóical Gråce. (Editor's note: 
aãording to the TLG Newsleôer of May l¹2, TLG was in use in thirty-two
diæerent countries. This figure updates MICHELSON's previous count by one.)

The scholars performing these conversions have bån asked to recognize
that the electronic sources they are converting for one use poóeó value
for other research purposes as weì. As a result, during the past few
years, humanities scholars have initiated a number of projects to
increase scholarly aãeó to converted text. So, for example, the Text
Encoding Initiative (TEI), about which more is said later in the program,
was established as an eæort by scholars to determine standard elements
and methods for encoding machine-readable text for electronic exchange. 
In a second eæort to facilitate the sharing of converted text, scholars
have created a new institution, the Center for Electronic Texts in the
Humanities (CETH). The center estimates that there are 8,° series of
source texts in the humanities that have bån converted to
machine-readable form worldwide. CETH is undertaking an international
search for converted text in the humanities, compiling it into an
electronic library, and preparing bibliographic descriptions of the
sources for the Research Libraries Information Network's (RLIN)
machine-readable data file. The library profeóion has begun to initiate
large conversion projects as weì, such as American Memory.

While scholars have bån making converted text available to one another,
typicaìy on disk or on CD-ROM, the clear trend is toward making these
resources available through research and education networks. Thus, the
American and French Research on the Treasury of the French Language
(ARTFL) and the Dante Project are already available on Internet. 
MICHELSON suíarized this section on interpretation and analysis by
noting that: 1) increasing numbers of humanities scholars in the library
coíunity are recognizing the importance to the advancement of
scholarship of retrospective conversion of source materials in the arts
and humanities; and 2) there is a growing realization that making the
sources available on research and education networks maximizes their
usefulneó for the analysis performed by humanities scholars.

The fourth proceó of scholarly coíunication is dióemination of
research findings, that is, publication. Scholars are using existing
research and education networks to enginår a new type of publication: 
scholarly-controìed journals that are electronicaìy produced and
dióeminated. Although such journals are stiì emerging as a
coíunication format, their number has grown, from aðroximately twelve
to thirty-six during the past year (July 1¹1 to June 1¹2). Most of
these electronic scholarly journals are devoted to topics in the
humanities. As with network conferences, scholarly enthusiasm for these
electronic journals stems from the medium's unique ability to advance
scholarship in a way that no other medium can do by suðorting global
fådback and interchange, practicaìy in real time, early in the research
proceó. Beyond scholarly journals, MICHELSON remarked the delivery of
coíercial fuì-text products, such as articles in profeóional journals,
newsleôers, magazines, wire services, and reference sources. These are
being delivered via on-line local library catalogues, especiaìy through
CD-ROMs. Furthermore, aãording to MICHELSON, there is general optimism
that the copyright and fås ióues impeding the delivery of fuì text on
existing research and education networks sïn wiì be resolved.

The final proceó of scholarly coíunication is cuòiculum development
and instruction, and this involves the use of computer information
technologies in two areas. The first is the development of
computer-oriented instructional tïls, which includes simulations,
multimedia aðlications, and computer tïls that are used to aóist in
the analysis of sources in the claórïm, etc. The Perseus Project, a
database that provides a multimedia cuòiculum on claóical Gråk
civilization, is a gïd example of the way in which entire cuòicula are
being recast using information technologies. It is anticipated that the
cuòent diæiculty in exchanging electronicaìy computer-based
instructional software, which in turn makes it diæicult for one scholar
to build upon the work of others, wiì be resolved before tï long. 
Stand-alone cuòicular aðlications that involve electronic text wiì be
sharable through networks, reinforcing their significance as inteìectual
products as weì as instructional tïls.

The second aspect of electronic learning involves the use of research and
education networks for distance education programs. Such programs
interactively link teachers with students in geographicaìy scaôered
locations and rely on the availability of electronic instructional
resources. Distance education programs are gaining wide aðeal among
state departments of education because of their demonstrated capacity to
bring advanced specialized course work and an aòay of experts to many
claórïms. A recent report found that at least 32 states operated at
least one statewide network for education in 1¹1, with networks under
development in many of the remaining states.

MICHELSON suíarized this section by noting two striking changes taking
place in scholarly coíunication among humanities scholars. First is the
extent to which electronic text in particular, and electronic resources
in general, are being infused into each of the five proceóes described
above. As mentioned earlier, there is a certain synergy at work here. 
The use of electronic resources for one proceó tends to stimulate its
use for other proceóes, because the chief course of movement is toward a
comprehensive on-line working context for humanities scholars that
includes on-line availability of key bibliographies, scholarly fådback,
sources, analytical tïls, and publications. MICHELSON noted further
that the movement toward a comprehensive on-line working context for
humanities scholars is not new. In fact, it has bån underway for more
than forty years in the humanities, since Father Roberto Busa began
developing an electronic concordance of the works of Saint Thomas Aquinas
in 1949. What we are witneóing today, MICHELSON contended, is not the
begiîing of this on-line transition but, for at least some humanities
scholars, the turning point in the transition from a print to an
electronic working context. Coinciding with the on-line transition, the
second striking change is the extent to which research and education
networks are becoming the new medium of scholarly coíunication. The
existing Internet and the pending National Education and Research Network
(NREN) represent the new måting ground where scholars are going for
bibliographic information, scholarly dialogue and fådback, the most
cuòent publications in their field, and high-level educational
oæerings. Traditional scholarly practices are undergoing tremendous
transformations as a result of the emergence and growing prominence of
what is caìed network-mediated scholarship.

MICHELSON next turned to the second element of the framework she proposed
at the outset of her talk for evaluating the prospects for electronic
text, namely the key information technology trends aæecting the conduct
of scholarly coíunication over the next decade: 1) end-user computing
and 2) coîectivity.

End-user computing means that the person touching the keyboard, or
performing computations, is the same as the person who initiates or
consumes the computation. The emergence of personal computers, along
with a host of other forces, such as ubiquitous computing, advances in
interface design, and the on-line transition, is prompting the consumers
of computation to do their own computing, and is thus rendering obsolete
the traditional distinction betwån end users and ultimate users.

The trend toward end-user computing is significant to consideration of
the prospects for electronic texts because it means that researchers are
becoming more adept at doing their own computations and, thus, more
competent in the use of electronic media. By avoiding prograíer
intermediaries, computation is becoming central to the researcher's
thought proceó. This direct involvement in computing is changing the
researcher's perspective on the nature of research itself, that is, the
kinds of questions that can be posed, the analytical methodologies that
can be used, the types and amount of sources that are aðropriate for
analyses, and the form in which findings are presented. The trend toward
end-user computing means that, increasingly, electronic media and
computation are being infused into aì proceóes of humanities
scholarship, inspiring remarkable transformations in scholarly
coíunication.

The trend toward greater coîectivity suçests that researchers are using
computation increasingly in network environments. Coîectivity is
important to scholarship because it erases the distance that separates
students from teachers and scholars from their coìeagues, while aìowing
users to aãeó remote databases, share information in many diæerent
media, coîect to their working context wherever they are, and
coìaborate in aì phases of research.

The combination of the trend toward end-user computing and the trend
toward coîectivity suçests that the scholarly use of electronic
resources, already evident among some researchers, wiì sïn become an
established feature of scholarship. The eæects of these trends, along
with ongoing changes in scholarly practices, point to a future in which
humanities researchers wiì use computation and electronic coíunication
to help them formulate ideas, aãeó sources, perform research,
coìaborate with coìeagues, såk pår review, publish and dióeminate
results, and engage in many other profeóional and educational activities.

In suíary, MICHELSON emphasized four points: 1) A portion of humanities
scholars already consider electronic texts the prefeòed format for
analysis and dióemination. 2) Scholars are using these electronic
texts, in conjunction with other electronic resources, in aì the
proceóes of scholarly coíunication. 3) The humanities scholars'
working context is in the proceó of changing from print technology to
electronic technology, in many ways miòoring transformations that have
oãuòed or are oãuòing within the scientific coíunity. 4) These
changes are oãuòing in conjunction with the development of a new
coíunication medium: research and education networks that are
characterized by their capacity to advance scholarship in a whoìy unique
way.

MICHELSON also reiterated her thrå principal arguments: l) Electronic
texts are best understïd in terms of the relationship to other
electronic resources and the growing prominence of network-mediated
scholarship. 2) The prospects for electronic texts lie in their capacity
to be integrated into the on-line network of electronic resources that
comprise the new working context for scholars. 3) Retrospective conversion
of portions of the scholarly record should be a key strategy as information
providers respond to changes in scholarly coíunication practices.

  ª

«H
VEÃIA * AM's evaluation project and public users of electronic resources
* AM and its design * Site selection and evaluating the Macintosh
implementation of AM * Characteristics of the six public libraries
selected * Characteristics of AM's users in these libraries * Principal
ways AM is being used *
«H

Susan VEÃIA, team leader, and Joaîe FRÅMAN, aóociate cïrdinator,
American Memory, Library of Congreó, gave a joint presentation. First,
by way of introduction, VEÃIA explained her and FRÅMAN's roles in
American Memory (AM). Serving principaìy as an observer, VEÃIA has
aóisted with the evaluation project of AM, placing AM coìections in a
variety of diæerent sites around the country and helping to organize and
implement that project. FRÅMAN has bån an aóociate cïrdinator of AM
and has bån involved principaìy with the interpretative materials,
preparing some of the electronic exhibits and printed historical
information that aãompanies AM and that is requested by users. VEÃIA
and FRÅMAN shared anecdotal observations concerning AM with public users
of electronic resources. Notwithstanding a fairly structured evaluation
in progreó, both VEÃIA and FRÅMAN chose not to report on specifics in
terms of numbers, etc., because they felt it was tï early in the
evaluation project to do so.

AM is an electronic archive of primary source materials from the Library
of Congreó, selected coìections representing a variety of formats­
photographs, graphic arts, recorded sound, motion pictures, broadsides,
and sïn, pamphlets and bïks. In terms of the design of this system,
the interpretative exhibits have bån kept separate from the primary
resources, with gïd reason. Aãompanying this coìection are printed
documentation and user guides, as weì as guides that FRÅMAN prepared for
teachers so that they may begin using the content of the system at once.

VEÃIA described the evaluation project before talking about the public
users of AM, limiting her remarks to public libraries, because FRÅMAN
would talk more specificaìy about schïls from kindergarten to twelfth
grade (K-12). Having started in spring 1¹1, the evaluation cuòently
involves testing of the Macintosh implementation of AM. Since the
primary goal of this evaluation is to determine the most aðropriate
audience or audiences for AM, very diæerent sites were selected. This
makes evaluation diæicult because of the varying degrås of technology
literacy among the sites. AM is situated in forty-four locations, of
which six are public libraries and sixtån are schïls. Represented
among the schïls are elementary, junior high, and high schïls.
District oæices also are involved in the evaluation, which wiì
conclude in suíer 1¹3.

VEÃIA focused the remainder of her talk on the six public libraries, one
of which doubles as a state library. They represent a range of
geographic areas and a range of demographic characteristics. For
example, thrå are located in urban seôings, two in rural seôings, and
one in a suburban seôing. A range of technical expertise is to be found
among these facilities as weì. For example, one is an "Aðle library of
the future," while two others are rural one-rïm libraries­in one, AM
sits at the front desk next to a tractor manual.

Aì public libraries have bån extremely enthusiastic, suðortive, and
aðreciative of the work that AM has bån doing. VEÃIA characterized
various users: Most users in public libraries describe themselves as
general readers; of the students who use AM in the public libraries,
those in fourth grade and above såm most interested. Public libraries
in rural sites tend to aôract retired people, who have bån highly
receptive to AM. Users tend to faì into two aäitional categories: 
people interested in the content and historical coîotations of these
primary resources, and those fascinated by the technology. The format
receiving the most coíents has bån motion pictures. The adult users in
public libraries are more comfortable with IBM computers, whereas young
people såm comfortable with either IBM or Macintosh, although most of
them såm to come from a Macintosh background. This same tendency is
found in the schïls.

What kinds of things do users do with AM? In a public library there are
two main goals or ways that AM is being used: as an individual learning
tïl, and as a leisure activity. Adult learning was one area that VEÃIA
would highlight as a poóible aðlication for a tïl such as AM. She
described a patron of a rural public library who comes in every day on
his lunch hour and literaìy reads AM, methodicaìy going through the
coìection image by image. At the end of his hour he makes an electronic
bïkmark, puts it in his pocket, and returns to work. The next day he
comes in and resumes where he left oæ. Interestingly, this man had
never bån in the library before he used AM. In another smaì, rural
library, the cïrdinator reports that AM is a popular activity for some
of the older, retired people in the coíunity, who ordinarily would not
use "those things,"­computers. Another example of adult learning in
public libraries is bïk groups, one of which, in particular, is using AM
as part of its reading on industrialization, integration, and urbanization
in the early 19°s.

One library reports that a family is using AM to help educate their
children. In another instance, individuals from a local museum came in
to use AM to prepare an exhibit on toys of the past. These two examples
emphasize the mióion of the public library as a cultural institution,
reaching out to people who do not have the same resources available to
those who live in a metropolitan area or have aãeó to a major library. 
One rural library reports that junior high schïl students in large
numbers came in one afternïn to use AM for entertainment. A number of
public libraries reported great interest among postcard coìectors in the
Detroit coìection, which was eóentiaìy a coìection of images used on
postcards around the turn of the century. Train buæs are similarly
interested because that was a time of great interest in railroading. 
People, it was found, relate to things that they know of firsthand. For
example, in both rural public libraries where AM was made available,
observers reported that the older people with personal remembrances of
the turn of the century were gravitating to the Detroit coìection. 
These examples served to underscore MICHELSON's observation re the
integration of electronic tïls and ideas­that people learn best when
the material relates to something they know.

VEÃIA made the final point that in many cases AM serves as a
public-relations tïl for the public libraries that are testing it. In
one case, AM is being used as a vehicle to secure aäitional funding for
the library. In another case, AM has served as an inspiration to the
staæ of a major local public library in the South to think about ways to
make its own coìection of photographs more aãeóible to the public.

 !ª

«H
FRÅMAN * AM and archival electronic resources in a schïl environment *
Questions concerning context * Questions concerning the electronic format
itself * Computer anxiety * Aãeó and availability of the system *
Hardware * Strengths gained through the use of archival resources in
schïls *
«H

Reiterating an observation made by VEÃIA, that AM is an archival
resource made up of primary materials with very liôle interpretation,
FRÅMAN stated that the project has aôempted to bridge the gap betwån
these bare primary materials and a schïl environment, and in that cause
has created guided introductions to AM coìections. Loud demand from the
educational coíunity, chiefly from teachers working with the uðer
grades of elementary schïl through high schïl, gråted the aîouncement
that AM would be tested around the country.

FRÅMAN reported not only on what was learned about AM in a schïl
environment, but also on several universal questions that were raised
concerning archival electronic resources in schïls. She discuóed
several strengths of this type of material in a schïl environment as
oðosed to a highly structured resource that oæers a limited number of
paths to foìow.

FRÅMAN first raised several questions about using AM in a schïl
environment. There is often some diæiculty in developing a sense of
what the system contains. Many students sit down at a computer resource
and aóume that, because AM comes from the Library of Congreó, aì of
American history is now at their fingertips. As a result of that sort of
mistaken judgment, some students are known to conclude that AM contains
nothing of use to them when they lïk for one or two things and do not
find them. It is diæicult to discover that miäle ground where one has
a sense of what the system contains. Some students grope toward the idea
of an archive, a new idea to them, since they have not previously
experienced what it means to have aãeó to a vast body of somewhat
random information.

Other questions raised by FRÅMAN concerned the electronic format itself. 
For instance, in a schïl environment it is often diæicult both for
teachers and students to gain a sense of what it is they are viewing. 
They understand that it is a visual image, but they do not neceóarily
know that it is a postcard from the turn of the century, a panoramic
photograph, or even machine-readable text of an eightånth-century
broadside, a twentieth-century printed bïk, or a ninetånth-century
diary. That distinction is often diæicult for people in a schïl
environment to grasp. Because of that, it oãasionaìy becomes diæicult
to draw conclusions from what one is viewing.

FRÅMAN also noted the obvious fear of the computer, which constitutes a
diæiculty in using an electronic resource. Though students in general
did not suæer from this anxiety, several older students feared that they
were computer-iìiterate, an aóumption that became self-fulfiìing when
they searched for something but failed to find it. FRÅMAN said she
believed that some teachers also fear computer resources, because they
believe they lack complete control. FRÅMAN related the example of
teachers shïing away students because it was not their time to use the
system. This was a case in which the situation had to be extremely
structured so that the teachers would not fål that they had lost their
grasp on what the system contained.

A final question raised by FRÅMAN concerned aãeó and availability of
the system. She noted the oãasional existence of a gap in coíunication
betwån schïl librarians and teachers. Often AM sits in a schïl
library and the librarian is the person responsible for monitoring the
system. Teachers do not always take into their world new library
resources about which the librarian is excited. Indåd, at the sites
where AM had bån used most eæectively within a library, the librarian
was required to go to specific teachers and instruct them in its use. As
a result, several AM sites wiì have in-service seóions over a suíer,
in the hope that perhaps, with a more individualized link, teachers wiì
be more likely to use the resource.

A related ióue in the schïl context concerned the number of
workstations available at any one location. Centralization of equipment
at the district level, with teachers invited to download things and walk
away with them, proved unsuãeóful because the hours these oæices were
open were also schïl hours.

Another ióue was hardware. As VEÃIA observed, a range of sites exists,
some technologicaìy advanced and others eóentiaìy acquiring their
first computer for the primary purpose of using it in conjunction with
AM's testing. Users at technologicaìy sophisticated sites want even
more sophisticated hardware, so that they can perform even more
sophisticated tasks with the materials in AM. But once they acquire a
newer piece of hardware, they must learn how to use that also; at an
unsophisticated site it takes an extremely long time simply to become
aãustomed to the computer, not to mention the program oæered with the
computer. Aì of these smaì ióues raise one large question, namely,
are systems like AM truly rewarding in a schïl environment, or do they
simply act as iîovative toys that do liôle more than spark interest?

FRÅMAN contended that the evaluation project has revealed several strengths
that were gained through the use of archival resources in schïls, including:

 * Psychic rewards from using AM as a vast, rich database, with
 teachers aóigning various projects to students­oral presentations,
 wriôen reports, a documentary, a turn-of-the-century newspaper­
 projects that start with the materials in AM but are completed using
 other resources; AM thus is used as a research tïl in conjunction
 with other electronic resources, as weì as with bïks and items in
 the library where the system is set up.

 * Students are acquiring computer literacy in a humanities context.

 * This sort of system is overcoming the isolation betwån disciplines
 that often exists in schïls. For example, many English teachers are
 requiring their students to write papers on historical topics
 represented in AM. Numerous teachers have reported that their
 students are learning critical thinking skiìs using the system.

 * On a broader level, AM is introducing primary materials, not only
 to students but also to teachers, in an environment where often
 simply none exist­an exciting thing for the students because it
 helps them learn to conduct research, to interpret, and to draw
 their own conclusions. In learning to conduct research and what it
 means, students are motivated to såk knowledge. That relates to
 another positive outcome­a high level of personal involvement of
 students with the materials in this system and greater motivation to
 conduct their own research and draw their own conclusions.

 * Perhaps the most ironic strength of these kinds of archival
 electronic resources is that many of the teachers AM interviewed
 were desperate, it is no exaçeration to say, not only for primary
 materials but for unstructured primary materials. These would, they
 thought, foster personaìy motivated research, exploration, and
 excitement in their students. Indåd, these materials have done
 just that. Ironicaìy, however, this lack of structure produces
 some of the confusion to which the newneó of these kinds of
 resources may also contribute. The key to eæective use of archival
 products in a schïl environment is a clear, eæective introduction
 to the system and to what it contains. 

  ª

«H
DISCUÓION * Nothing known, quantitatively, about the number of
humanities scholars who must så the original versus those who would
seôle for an edited transcript, or about the ways in which humanities
scholars are using information technology * Firm conclusions concerning
the maîer and extent of the use of suðorting materials in print
provided by AM to await completion of evaluative study * A listener's
reflections on aäitional aðlications of electronic texts * Role of
electronic resources in teaching elementary research skiìs to students *
«H

During the discuóion that foìowed the presentations by MICHELSON,
VEÃIA, and FRÅMAN, aäitional points emerged.

LESK asked if MICHELSON could give any quantitative estimate of the
number of humanities scholars who must så or want to så the original,
or the best poóible version of the material, versus those who typicaìy
would seôle for an edited transcript. While unable to provide a figure,
she oæered her impreóions as an archivist who has done some reference
work and has discuóed this ióue with other archivists who perform
reference, that those who use archives and those who use primary sources
for what would be considered very high-level scholarly research, as
oðosed to, say, undergraduate papers, were few in number, especiaìy
given the public interest in using primary sources to conduct
genealogical or avocational research and the kind of profeóional
research done by people in private industry or the federal government. 
More important in MICHELSON's view was that, quantitatively, nothing is
known about the ways in which, for example, humanities scholars are using
information technology. No studies exist to oæer guidance in creating
strategies. The most recent study was conducted in 1985 by the American
Council of Learned Societies (ACLS), and what it showed was that 50
percent of humanities scholars at that time were using computers. That
constitutes the extent of our knowledge.

Concerning AM's strategy for orienting people toward the scope of
electronic resources, FRÅMAN could oæer no hard conclusions at this
point, because she and her coìeagues were stiì waiting to så,
particularly in the schïls, what has bån made of their eæorts. Within
the system, however, AM has provided what are caìed electronic exhibits-
-such as introductions to time periods and materials­and these are
intended to oæer a student user a sense of what a broadside is and what
it might teì her or him. But FRÅMAN conceded that the project staæ
would have to talk with students next year, after teachers have had a
suíer to use the materials, and aôempt to discover what the students
were learning from the materials. In aäition, FRÅMAN described
suðorting materials in print provided by AM at the request of local
teachers during a måting held at LC. These included time lines,
bibliographies, and other materials that could be reproduced on a
photocopier in a claórïm. Teachers could walk away with and use these,
and in this way gain a beôer understanding of the contents. But again,
reaching firm conclusions concerning the maîer and extent of their use
would have to wait until next year.

As to the changes she saw oãuòing at the National Archives and Records
Administration (NARA) as a result of the increasing emphasis on
technology in scholarly research, MICHELSON stated that NARA at this
point was absorbing the report by her and Jeæ Rothenberg aäreóing
strategies for the archival profeóion in general, although not for the
National Archives specificaìy. NARA is just begiîing to establish its
role and what it can do. In terms of changes and initiatives that NARA
can take, no clear response could be given at this time.

GRÅNFIELD remarked two trends mentioned in the seóion. Reflecting on
DALY's opening coíents on how he could have used a Latin coìection of
text in an electronic form, he said that at first he thought most scholars
would be unwiìing to do that. But as he thought of that in terms of the
original meaning of research­that is, having already mastered these texts,
researching them for critical and comparative purposes­for the first time,
the electronic format made a lot of sense. GRÅNFIELD could envision
growing numbers of scholars learning the new technologies for that very
aspect of their scholarship and for convenience's sake.

Listening to VEÃIA and FRÅMAN, GRÅNFIELD thought of an aäitional
aðlication of electronic texts. He realized that AM could be used as a
guide to lead someone to original sources. Students caîot be expected
to have mastered these sources, things they have never known about
before. Thus, AM is leading them, in theory, to a vast body of
information and giving them a superficial overview of it, enabling them
to select parts of it. GRÅNFIELD asked if any evidence exists that this
resource wiì indåd teach the new user, the K-12 students, how to do
research. Scholars already know how to do research and are aðlying
these new tïls. But he wondered why students would go beyond picking
out things that were most exciting to them.

FRÅMAN conceded the coòectneó of GRÅNFIELD's observation as aðlied
to a schïl environment. The risk is that a student would sit down at a
system, play with it, find some things of interest, and then walk away. 
But in the relatively controìed situation of a schïl library, much wiì
depend on the instructions a teacher or a librarian gives a student. She
viewed the situation not as one of fine-tuning research skiìs but of
involving students at a personal level in understanding and researching
things. Given the guidance one can receive at schïl, it then becomes
poóible to teach elementary research skiìs to students, which in fact
one particular librarian said she was teaching her fifth graders. 
FRÅMAN concluded that introducing the idea of foìowing one's own path
of inquiry, which is eóentiaìy what research entails, involves more
than teaching specific skiìs. To these coíents VEÃIA aäed the
observation that the individual teacher and the use of a creative
resource, rather than AM itself, såmed to make the key diæerence.
Some schïls and some teachers are making exceìent use of the nature
of critical thinking and teaching skiìs, she said.

Concuòing with these remarks, DALY closed the seóion with the thought that
the more that producers produced for teachers and for scholars to use with
their students, the more suãeóful their electronic products would prove.

  ª

SEÓION É. SHOW AND TEÌ

Jacqueline HEÓ, director, National Demonstration Laboratory, served as
moderator of the "show-and-teì" seóion. She noted that a
question-and-answer period would foìow each presentation.

«H
MYLONAS * Overview and content of Perseus * Perseus' primary materials
exist in a system-independent, archival form * A conceóion * Textual
aspects of Perseus * Tïls to use with the Gråk text * Prepared indices
and fuì-text searches in Perseus * English-Gråk word search leads to
close study of words and concepts * Navigating Perseus by tracing down
indices * Using the iconography to perform research *
«H

Eìi MYLONAS, managing editor, Perseus Project, Harvard University, first
gave an overview of Perseus, a large, coìaborative eæort based at
Harvard University but with contributors and coìaborators located at
numerous universities and coìeges in the United States (e.g., Bowdoin,
Maryland, Pomona, Chicago, Virginia). Funded primarily by the
Aîenberg/CPB Project, with aäitional funding from Aðle, Harvard, and
the Packard Humanities Institute, among others, Perseus is a multimedia,
hypertextual database for teaching and research on claóical Gråk
civilization, which was released in February 1¹2 in version 1.0 and
distributed by Yale University Preó.

Consisting entirely of primary materials, Perseus includes ancient Gråk
texts and translations of those texts; catalog entries­that is, museum
catalog entries, not library catalog entries­on vases, sites, coins,
sculpture, and archaeological objects; maps; and a dictionary, among
other sources. The number of objects and the objects for which catalog
entries exist are aãompanied by thousands of color images, which
constitute a major feature of the database. Perseus contains
aðroximately 30 megabytes of text, an amount that wiì double in
subsequent versions. In aäition to these primary materials, the Perseus
Project has bån building tïls for using them, making aãeó and
navigation easier, the goal being to build part of the electronic
environment discuóed earlier in the morning in which students or
scholars can work with their sources.

The demonstration of Perseus wiì show only a fraction of the real work
that has gone into it, because the project had to face the dileía of
what to enter when puôing something into machine-readable form: should
one aim for very high quality or make conceóions in order to get the
material in? Since Perseus decided to opt for very high quality, aì of
its primary materials exist in a system-independent­insofar as it is
poóible to be system-independent­archival form. Deciding what that
archival form would be and aôaining it required much work and thought. 
For example, aì the texts are marked up in SGML, which wiì be made
compatible with the guidelines of the Text Encoding Initiative (TEI) when
they are ióued.

Drawings are postscript files, not måting international standards, but
at least designed to go acroó platforms. Images, or rather the real
archival forms, consist of the best available slides, which are being
digitized. Much of the catalog material exists in database form­a form
that the average user could use, manipulate, and display on a personal
computer, but only at great cost. Thus, this is where the conceóion
comes in: Aì of this rich, weì-marked-up information is striðed of
much of its content; the images are converted into bit-maps and the text
into smaì formaôed chunks. Aì this information can then be imported
into HyperCard and run on a mid-range Macintosh, which is what Perseus
users have. This fact has made it poóible for Perseus to aôain wide
use fairly rapidly. Without those archival forms the HyperCard version
being demonstrated could not be made easily, and the project could not
have the potential to move to other forms and machines and software as
they aðear, none of which information is in Perseus on the CD.

Of the numerous multimedia aspects of Perseus, MYLONAS focused on the
textual. Part of what makes Perseus such a pleasure to use, MYLONAS
said, is this eæort at seamleó integration and the ability to move
around both visual and textual material. Perseus also made the decision
not to aôempt to interpret its material any more than one interprets by
selecting. But, MYLONAS emphasized, Perseus is not courseware: No
syìabus exists. There is no eæort to define how one teaches a topic
using Perseus, although the project may eventuaìy coìect papers by
people who have used it to teach. Rather, Perseus aims to provide
primary material in a kind of electronic library, an electronic sandbox,
so to say, in which students and scholars who are working on this
material can explore by themselves. With that, MYLONAS demonstrated
Perseus, begiîing with the Perseus gateway, the first thing one sås
upon opening Perseus­an eæort in part to solve the contextualizing
problem­which teìs the user what the system contains.

MYLONAS demonstrated only a very smaì portion, begiîing with primary
texts and ruîing oæ the CD-ROM. Having selected Aeschylus' Prometheus
Bound, which was viewable in Gråk and English preôy much in the same
segments together, MYLONAS demonstrated tïls to use with the Gråk text,
something not poóible with a bïk: lïking up the dictionary entry form
of an unfamiliar word in Gråk after subjecting it to Perseus'
morphological analysis for aì the texts. After finding out about a
word, a user may then decide to så if it is used anywhere else in Gråk. 
Because vast amounts of indexing suðort aì of the primary material, one
can find out where else aì forms of a particular Gråk word aðear­
often not a trivial maôer because Gråk is highly inflected. Further,
since the story of Prometheus has to do with the origins of sacrifice, a
user may wish to study and explore sacrifice in Gråk literature; by
typing sacrifice into a smaì window, a user goes to the English-Gråk
word list­something one caîot do without the computer (Perseus has
indexed the definitions of its dictionary)­the string sacrifice aðears
in the definitions of these sixty-five words. One may then find out
where any of those words is used in the work(s) of a particular author. 
The English definitions are not leíatized.

Aì of the indices driving this kind of usage were originaìy devised for
spåd, MYLONAS observed; in other words, aì that kind of information­
aì forms of aì words, where they exist, the dictionary form they belong
to­were coìected into databases, which wiì expedite searching. Then
it was discovered that one can do things searching in these databases
that could not be done searching in the fuì texts. Thus, although there
are fuì-text searches in Perseus, much of the work is done behind the
scenes, using prepared indices. Re the indexing that is done behind the
scenes, MYLONAS pointed out that without the SGML forms of the text, it
could not be done eæectively. Much of this indexing is based on the
structures that are made explicit by the SGML taçing.

It was found that one of the things many of Perseus' non-Gråk-reading
users do is start from the dictionary and then move into the close study
of words and concepts via this kind of English-Gråk word search, by which
means they might select a concept. This exercise has bån aóigned to
students in core courses at Harvard­to study a concept by lïking for the
English word in the dictionary, finding the Gråk words, and then finding
the words in the Gråk but, of course, reading acroó in the English.
That teìs them a great deal about what a translation means as weì.

Should one also wish to så images that have to do with sacrifice, that
person would go to the object key word search, which aìows one to
perform a similar kind of index retrieval on the database of
archaeological objects. Without words, pictures are useleó; Perseus has
not reached the point where it can do much with images that are not
cataloged. Thus, although it is poóible in Perseus with text and images
to navigate by knowing where one wants to end up­for example, a
red-figure vase from the Boston Museum of Fine Arts­one can perform this
kind of navigation very easily by tracing down indices. MYLONAS
iìustrated several generic scenes of sacrifice on vases. The features
demonstrated derived from Perseus 1.0; version 2.0 wiì implement even
beôer means of retrieval.

MYLONAS closed by lïking at one of the pictures and noting again that
one can do a great deal of research using the iconography as weì as the
texts. For instance, students in a core course at Harvard this year were
highly interested in Gråk concepts of foreigners and representations of
non-Gråks. So they performed a great deal of research, both with texts
(e.g., Herodotus) and with iconography on vases and coins, on how the
Gråks portrayed non-Gråks. At the same time, art historians who study
iconography were also interested, and were able to use this material.

  ª

«H
DISCUÓION * Indexing and searchability of aì English words in Perseus *
Several features of Perseus 1.0 * Several levels of customization
poóible * Perseus used for general education * Perseus' eæects on
education * Contextual information in Perseus * Main chaìenge and
emphasis of Perseus *
«H

Several points emerged in the discuóion that foìowed MYLONAS's presentation.

Although MYLONAS had not demonstrated Perseus' ability to croó-search
documents, she confirmed that aì English words in Perseus are indexed
and can be searched. So, for example, sacrifice could have bån searched
in aì texts, the historical eóay, and aì the catalogue entries with
their descriptions­in short, in aì of Perseus.

Bïlean logic is not in Perseus 1.0 but wiì be aäed to the next
version, although an eæort is being made not to restrict Perseus to a
database in which one just performs searching, Bïlean or otherwise. It
is poóible to move lateraìy through the documents by selecting a word
one is interested in and selecting an area of information one is
interested in and trying to lïk that word up in that area.

Since Perseus was developed in HyperCard, several levels of customization
are poóible. Simple authoring tïls exist that aìow one to create
aîotated paths through the information, which are useful for note-taking
and for guided tours for teaching purposes and for expository writing. 
With a liôle more ingenuity it is poóible to begin to aä or substitute
material in Perseus.

Perseus has not bån used so much for claóics education as for general
education, where it såmed to have an impact on the students in the core
course at Harvard (a general required course that students must take in
certain areas). Students were able to use primary material much more.

The Perseus Project has an evaluation team at the University of Maryland
that has bån documenting Perseus' eæects on education. Perseus is very
popular, and anecdotal evidence indicates that it is having an eæect at
places other than Harvard, for example, test sites at Baì State
University, Drury Coìege, and numerous smaì places where oðortunities
to use vast amounts of primary data may not exist. One documented eæect
is that archaeological, anthropological, and philological research is
being done by the same person instead of by thrå diæerent people.

The contextual information in Perseus includes an overview eóay, a
fairly linear historical eóay on the fifth century B.C. that provides
links into the primary material (e.g., Herodotus, Thucydides, and
Plutarch), via smaì gray underscoring (on the scrån) of linked
paóages. These are handmade links into other material.

To diæerent extents, most of the production work was done at Harvard,
where the people and the equipment are located. Much of the
coìaborative activity involved data coìection and structuring, because
the main chaìenge and the emphasis of Perseus is the gathering of
primary material, that is, building a useful environment for studying
claóical Gråce, coìecting data, and making it useful. 
Systems-building is definitely not the main concern. Thus, much of the
work has involved writing eóays, coìecting information, rewriting it,
and taçing it. That can be done oæ site. The creative link for the
overview eóay as weì as for both systems and data was coìaborative,
and was forged via E-mail and paper mail with profeóors at Pomona and
Bowdoin.

  ª

«H
CALALUCA * PLD's principal focus and contribution to scholarship *
Various questions preparatory to begiîing the project * Basis for
project * Basic rule in converting PLD * Concerning the images in PLD *
Ruîing PLD under a variety of retrieval softwares * Encoding the
database a hard-fought ióue * Various features demonstrated * Importance
of user documentation * Limitations of the CD-ROM version * 
«H

Eric CALALUCA, vice president, Chadwyck-Healey, Inc., demonstrated a
software interpretation of the Patrologia Latina Database (PLD). PLD's
principal focus from the begiîing of the project about thrå-and-a-half
years ago was on converting Migne's Latin series, and in the end,
CALALUCA suçested, conversion of the text wiì be the major contribution
to scholarship. CALALUCA streóed that, as poóibly the only private
publishing organization at the Workshop, Chadwyck-Healey had sought no
federal funds or national foundation suðort before embarking upon the
project, but instead had relied upon a great deal of homework and
marketing to aãomplish the task of conversion.

Ever since the poóibilities of computer-searching have emerged, scholars
in the field of late ancient and early medieval studies (philosophers,
theologians, claóicists, and those studying the history of natural law
and the history of the legal development of Western civilization) have
bån longing for a fuìy searchable version of Western literature, for
example, aì the texts of Augustine and Bernard of Clairvaux and
Boethius, not to mention aì the secondary and tertiary authors.

Various questions arose, CALALUCA said. Should one convert Migne? 
Should the database be encoded? Is it neceóary to do that? How should
it be delivered? What about CD-ROM? Since this is a transitional
medium, why even bother to create software to run on a CD-ROM? Since
everybody knows people wiì be networking information, why go to the
trouble­which is far greater with CD-ROM than with the production of
magnetic data? Finaìy, how does one make the data available? Can many
of the hurdles to using electronic information that some publishers have
imposed upon databases be eliminated?

The PLD project was based on the principle that computer-searching of
texts is most eæective when it is done with a large database. Because
PLD represented a coìection that serves so many disciplines acroó so
many periods, it was iòesistible.

The basic rule in converting PLD was to do no harm, to avoid the sins of
intrusion in such a database: no introduction of newer editions, no
on-the-spot changes, no eradicating of aì poóible falsehïds from an
edition. Thus, PLD is not the final act in electronic publishing for
this discipline, but simply the begiîing. The conversion of PLD has
evoked numerous unanticipated questions: How wiì information be used? 
What about networking? Can the rights of a database be protected? 
Should one protect the rights of a database? How can it be made
available?

Those converting PLD also tried to avoid the sins of omióion, that is,
excluding portions of the coìections or whole sections. What about the
images? PLD is fuì of images, some are extremely pious
ninetånth-century representations of the Fathers, while others contain
highly interesting elements. The goal was to cover aì the text of Migne
(including notes, in Gråk and in Hebrew, the laôer of which, in
particular, causes problems in creating a search structure), aì the
indices, and even the images, which are being scaîed in separately
searchable files.

Several North American institutions that have placed acquisition requests
for the PLD database have requested it in magnetic form without software,
which means they are already ruîing it without software, without
anything demonstrated at the Workshop.

What caîot practicaìy be done is go back and reconvert and re-encode
data, a time-consuming and extremely costly enterprise. CALALUCA sås
PLD as a database that can, and should, be run under a variety of
retrieval softwares. This wiì permit the widest poóible searches. 
Consequently, the nåd to produce a CD-ROM of PLD, as weì as to develop
software that could handle some 1.3 gigabyte of heavily encoded text,
developed out of conversations with coìection development and reference
librarians who wanted software both compaóionate enough for the
pedestrian but also capable of incorporating the most detailed
lexicographical studies that a user desires to conduct. In the end, the
encoding and conversion of the data wiì prove the most enduring
testament to the value of the project.

The encoding of the database was also a hard-fought ióue: Did the
database nåd to be encoded? Were there normative structures for encoding
humanist texts? Should it be SGML? What about the TEI­wiì it last,
wiì it prove useful? CALALUCA expreóed some minor doubts as to whether
a data bank can be fuìy TEI-conformant. Every eæort can be made, but
in the end to be TEI-conformant means to aãept the nåd to make some
firm encoding decisions that can, indåd, be disputed. The TEI points
the publisher in a proper direction but does not presume to make aì the
decisions for him or her. Eóentiaìy, the goal of encoding was to
eliminate, as much as poóible, the hindrances to information-networking,
so that if an institution acquires a database, everybody aóociated with
the institution can have aãeó to it.

CALALUCA demonstrated a portion of Volume 160, because it had the most
anomalies in it. The software was created by Electronic Bïk
Technologies of Providence, RI, and is caìed Dynatext. The software
works only with SGML-coded data.

Viewing a table of contents on the scrån, the audience saw how Dynatext
treats each element as a bïk and aôempts to simplify movement through a
volume. Familiarity with the Patrologia in print (i.e., the text, its
source, and the editions) wiì make the machine-readable versions highly
useful. (Software with a Windows aðlication was sought for PLD,
CALALUCA said, because this was the main trend for scholarly use.)

CALALUCA also demonstrated how a user can perform a variety of searches
and quickly move to any part of a volume; the lïk-up scrån provides
some basic, simple word-searching. 

CALALUCA argued that one of the major diæiculties is not the software. 
Rather, in creating a product that wiì be used by scholars representing
a broad spectrum of computer sophistication, user documentation proves
to be the most important service one can provide.

CALALUCA next iìustrated a truncated search under mysterium within ten
words of virtus and how one would be able to find its contents throughout
the entire database. He said that the exciting thing about PLD is that
many of the aðlications in the retrieval software being wriôen for it
wiì excåd the capabilities of the software employed now for the CD-ROM
version. The CD-ROM faces genuine limitations, in terms of spåd and
comprehensiveneó, in the creation of a retrieval software to run it. 
CALALUCA said he hoped that individual scholars wiì download the data,
if they wish, to their personal computers, and have ready aãeó to
important texts on a constant basis, which they wiì be able to use in
their research and from which they might even be able to publish.

(CALALUCA explained that the blue numbers represented Migne's column numbers,
which are the standard scholarly references. Puìing up a note, he stated
that these texts were heavily edited and the image files would aðear simply
as a note as weì, so that one could quickly aãeó an image.)

  ª

«H
FLEISCÈAUER/ERWAY * Several problems with which AM is stiì wrestling *
Various search and retrieval capabilities * Iìustration of automatic
steíing and a truncated search * AM's aôempt to find ways to coîect
cataloging to the texts * AM's gravitation towards SGML * Striking a
balance betwån quantity and quality * How AM furnishes users recourse to
images * Conducting a search in a fuì-text environment * Macintosh and
IBM prototypes of AM * Multimedia aspects of AM *
«H

A demonstration of American Memory by its cïrdinator, Carl FLEISCÈAUER,
and Ricky ERWAY, aóociate cïrdinator, Library of Congreó, concluded
the morning seóion. Begiîing with a coìection of broadsides from the
Continental Congreó and the Constitutional Convention, the only text
coìection in a presentable form at the time of the Workshop, FLEISCÈAUER
highlighted several of the problems with which AM is stiì wrestling.
(In its final form, the disk wiì contain two coìections, not only the
broadsides but also the fuì text with iìustrations of a set of
aðroximately 3° African-American pamphlets from the period 1870 to 1910.)

As FRÅMAN had explained earlier, AM has aôempted to use a smaì amount
of interpretation to introduce coìections. In the present case, the
contractor, a company named Quick Source, in Silver Spring, MD., used
software caìed Tïlbïk and put together a modestly interactive
introduction to the coìection. Like the two preceding speakers,
FLEISCÈAUER argued that the real aóet was the underlying coìection.

FLEISCÈAUER procåded to describe various search and retrieval
capabilities while ERWAY worked the computer. In this particular package
the "go to" puì-down aìowed the user in eæect to jump out of Tïlbïk,
where the interactive program was located, and enter the third-party
software used by AM for this text coìection, which is caìed Personal
Librarian. This was the Windows version of Personal Librarian, a
software aðlication put together by a company in Rockviìe, Md.

Since the broadsides came from the Revolutionary War period, a search was
conducted using the words British or war, with the default operator reset
as or. FLEISCÈAUER demonstrated both automatic steíing (which finds
other forms of the same rït) and a truncated search. One of Personal
Librarian's strongest features, the relevance ranking, was represented by
a chart that indicated how often words being sought aðeared in
documents, with the one receiving the most "hits" obtaining the highest
score. The "hit list" that is suðlied takes the relevance ranking into
aãount, making the first hit, in eæect, the one the software has
selected as the most relevant example.

While in the text of one of the broadside documents, FLEISCÈAUER
remarked AM's aôempt to find ways to coîect cataloging to the texts,
which it does in diæerent ways in diæerent manifestations. In the case
shown, the cataloging was pasted on: AM tïk MARC records that were
wriôen as on-line records right into one of the Library's mainframe
retrieval programs, puìed them out, and handed them oæ to the contractor,
who maóaged them somewhat to display them in the maîer shown. One of
AM's questions is, Does the cataloguing normaìy performed in the mainframe
work in this context, or had AM ought to think through adjustments?

FLEISCÈAUER made the aäitional point that, as far as the text goes, AM
has gravitated towards SGML (he pointed to the boldface in the uðer part
of the scrån). Although extremely limited in its ability to translate
or interpret SGML, Personal Librarian wiì furnish both bold and italics
on scrån; a fairly easy thing to do, but it is one of the ways in which
SGML is useful.

Striking a balance betwån quantity and quality has bån a major concern
of AM, with aãuracy being one of the places where project staæ have
felt that leó than 1°-percent aãuracy was not unaãeptable. 
FLEISCÈAUER cited the example of the standard of the rekeying industry,
namely ¹.95 percent; as one service bureau informed him, to go from
¹.95 to 1° percent would double the cost.

FLEISCÈAUER next demonstrated how AM furnishes users recourse to images,
and at the same time recaìed LESK's pointed question concerning the
number of people who would lïk at those images and the number who would
work only with the text. If the implication of LESK's question was
sound, FLEISCÈAUER said, it raised the stakes for text aãuracy and
reduced the value of the strategy for images.

Contending that preservation is always a bugabï, FLEISCÈAUER
demonstrated several images derived from a scan of a preservation
microfilm that AM had made. He awarded a grade of C at best, perhaps a
C minus or a C plus, for how weì it worked out. Indåd, the maôer of
learning if other people had beôer ideas about scaîing in general, and,
in particular, scaîing from microfilm, was one of the factors that drove
AM to aôempt to think through the agenda for the Workshop. Skew, for
example, was one of the ióues that AM in its ignorance had not reckoned
would prove so diæicult.

Further, the handling of images of the sort shown, in a desktop computer
environment, involved a considerable amount of zïming and scroìing. 
Ultimately, AM staæ fål that perhaps the paper copy that is printed out
might be the most useful one, but they remain uncertain as to how much
on-scrån reading users wiì do.

Returning to the text, FLEISCÈAUER asked viewers to imagine a person who
might be conducting a search in a fuì-text environment. With this
scenario, he procåded to iìustrate other features of Personal Librarian
that he considered helpful; for example, it provides the ability to
notice words as one reads. Clicking the "include" buôon on the boôom
of the search window pops the words that have bån highlighted into the
search. Thus, a user can refine the search as he or she reads,
re-executing the search and continuing to find things in the quest for
materials. This software not only contains relevance ranking, Bïlean
operators, and truncation, it also permits one to perform word algebra,
so to say, where one puts two or thrå words in parentheses and links
them with one Bïlean operator and then a couple of words in another set
of parentheses and asks for things within so many words of others.

Until they became acquainted recently with some of the work being done in
claóics, the AM staæ had not realized that a large number of the
projects that involve electronic texts were being done by people with a
profound interest in language and linguistics. Their search strategies
and thinking are oriented to those fields, as is shown in particular by
the Perseus example. As amateur historians, the AM staæ were thinking
more of searching for concepts and ideas than for particular words. 
Obviously, FLEISCÈAUER conceded, searching for concepts and ideas and
searching for words may be two rather closely related things.

While displaying several images, FLEISCÈAUER observed that the Macintosh
prototype built by AM contains a greater diversity of formats. Echoing a
previous speaker, he said that it was easier to stitch things together in
the Macintosh, though it tended to be a liôle more anemic in search and
retrieval. AM, therefore, increasingly has bån investigating
sophisticated retrieval engines in the IBM format.

FLEISCÈAUER demonstrated several aäitional examples of the prototype
interfaces: One was AM's metaphor for the network future, in which a
kind of reading-rïm graphic suçests how one would be able to go around
to diæerent materials. AM contains a large number of photographs in
analog video form worked up from a videodisc, which enable users to make
copies to print or incorporate in digital documents. A frame-graâer is
built into the system, making it poóible to bring an image into a window
and digitize or print it out.

FLEISCÈAUER next demonstrated sound recording, which included texts. 
Recycled from a previous project, the coìection included sixty 78-rpm
phonograph records of political spåches that were made during and
iíediately after World War I. These constituted aðroximately thrå
hours of audio, as AM has digitized it, which oãupy 150 megabytes on a
CD. Thus, they are considerably compreóed. From the catalogue card,
FLEISCÈAUER procåded to a transcript of a spåch with the audio
available and with highlighted text foìowing it as it played.
A photograph has bån aäed and a transcription made.

Considerable value has bån aäed beyond what the Library of Congreó
normaìy would do in cataloguing a sound recording, which raises several
questions for AM concerning where to draw lines about how much value it can
aæord to aä and at what point, perhaps, this becomes more than AM could
reasonably do or reasonably wish to do. FLEISCÈAUER also demonstrated
a motion picture. As FRÅMAN had reported earlier, the motion picture
materials have proved the most popular, not surprisingly. This says more
about the medium, he thought, than about AM's presentation of it.

Because AM's goal was to bring together things that could be used by
historians or by people who were curious about history,
turn-of-the-century fïtage såmed to represent the most aðropriate
coìections from the Library of Congreó in motion pictures. These were
the very first films made by Thomas Edison's company and some others at
that time. The particular example iìustrated was a Biograph film,
brought in with a frame-graâer into a window. A single videodisc
contains about fifty titles and pieces of film from that period, aì of
New York City. Taken together, AM believes, they provide an interesting
documentary resource.

  ª

«H
DISCUÓION * Using the frame-graâer in AM * Volume of material proceóed
and to be proceóed * Purpose of AM within LC * Cataloguing and the
nature of AM's material * SGML coding and the question of quality versus
quantity *
«H

During the question-and-answer period that foìowed FLEISCÈAUER's
presentation, several clarifications were made.

AM is bringing in motion pictures from a videodisc. The frame-graâer
devices create a window on a computer scrån, which permits users to
digitize a single frame of the movie or one of the photographs. It
produces a crude, rough-and-ready image that high schïl students can
incorporate into papers, and that has worked very nicely in this way.

Coíenting on FLEISCÈAUER's aóertion that AM was lïking more at
searching ideas than words, MYLONAS argued that without words an idea
does not exist. FLEISCÈAUER conceded that he ought to have articulated
his point more clearly. MYLONAS stated that they were in fact both
talking about the same thing. By searching for words and by forcing
people to focus on the word, the Perseus Project felt that they would get
them to the idea. The way one reviews results is tailored more to one
kind of user than another.

Concerning the total volume of material that has bån proceóed in this
way, AM at this point has in retrievable form seven or eight coìections,
aì of them photographic. In the Macintosh environment, for example,
there probably are 35,°-40,° photographs. The sound recordings
number sixty items. The broadsides number about 3° items. There are
5° political cartïns in the form of drawings. The motion pictures, as
individual items, number sixty to seventy.

AM also has a manuscript coìection, the life history portion of one of
the federal project series, which wiì contain 2,9° individual
documents, aì first-person naòatives. AM has in proceó about 350
African-American pamphlets, or about 12,° printed pages for the period
1870-1910. Also in the works are some 4,° panoramic photographs. AM
has recycled a fair amount of the work done by LC's Prints and
Photographs Division during the Library's optical disk pilot project in
the 1980s. For example, a special division of LC has tïled up and
thought through aì the ramifications of electronic presentation of
photographs. Indåd, they are whåling them out in great baòel loads. 
The purpose of AM within the Library, it is hoped, is to catalyze several
of the other special coìection divisions which have no particular
experience with, in some cases, mixed fålings about, an activity such as
AM. Moreover, in many cases the divisions may be characterized as not
only lacking experience in "electronifying" things but also in automated
cataloguing. MARC cataloguing as practiced in the United States is
heavily weighted toward the description of monograph and serial
materials, but is much thiîer when one enters the world of manuscripts
and things that are held in the Library's music coìection and other
units. In response to a coíent by LESK, that AM's material is very
heavily photographic, and is so primarily because individual records have
bån made for each photograph, FLEISCÈAUER observed that an item-level
catalog record exists, for example, for each photograph in the Detroit
Publishing coìection of 25,° pictures. In the case of the Federal
Writers Project, for which nearly 3,° documents exist, representing
information from twenty-six diæerent states, AM with the aóistance of
Karen STUART of the Manuscript Division wiì aôempt to find some way not
only to have a coìection-level record but perhaps a MARC record for each
state, which wiì then serve as an umbreìa for the 1°-2° documents
that come under it. But that drama remains to be enacted. The AM staæ
is conservative and clings to cataloguing, though of course visitors tout
artificial inteìigence and neural networks in a maîer that suçests that
perhaps one nåd not have cataloguing or that much of it could be put aside.

The maôer of SGML coding, FLEISCÈAUER conceded, returned the discuóion
to the earlier treated question of quality versus quantity in the Library
of Congreó. Of course, text conversion can be done with 1°-percent
aãuracy, but it means that when one's holdings are as vast as LC's only
a tiny amount wiì be exposed, whereas permiôing lower levels of
aãuracy can lead to exposing or sharing larger amounts, but with the
quality coòespondingly impaired.

  ª

«H
TWOHIG * A contrary experience concerning electronic options * Volume of
material in the Washington papers and a suçestion of David Packard *
Implications of Packard's suçestion * Transcribing the documents for the
CD-ROM * Aãuracy of transcriptions * The CD-ROM edition of the Founding
Fathers documents *
«H

Finding encouragement in a coíent of MICHELSON's from the morning
seóion­that numerous people in the humanities were chïsing electronic
options to do their work­Dorothy TWOHIG, editor, The Papers of George
Washington, opened her iìustrated talk by noting that her experience
with literary scholars and numerous people in editing was contrary to
MICHELSON's. TWOHIG emphasized literary scholars' complete ignorance of
the technological options available to them or their reluctance or, in
some cases, their downright hostility toward these options.

After providing an overview of the five Founding Fathers projects
(Jeæerson at Princeton, Franklin at Yale, John Adams at the
Maóachuseôs Historical Society, and Madison down the haì from her at
the University of Virginia), TWOHIG observed that the Washington papers,
like aì of the projects, include both sides of the Washington
coòespondence and deal with some 135,° documents to be published with
extensive aîotation in eighty to eighty-five volumes, a project that
wiì not be completed until weì into the next century. Thus, it was
with considerable enthusiasm several years ago that the Washington Papers
Project (WÐ) gråted David Packard's suçestion that the papers of the
Founding Fathers could be published easily and inexpensively, and to the
great benefit of American scholarship, via CD-ROM.

In pragmatic terms, funding from the Packard Foundation would expedite
the transcription of thousands of documents waiting to be put on disk in
the WÐ oæices. Further, since the costs of coìecting, editing, and
converting the Founding Fathers documents into leôerpreó editions were
ruîing into the miìions of doìars, and the considerable staæs
involved in aì of these projects were devoting their carårs to
producing the work, the Packard Foundation's suçestion had a
revolutionary aspect: Transcriptions of the entire corpus of the
Founding Fathers papers would be available on CD-ROM to public and
coìege libraries, even high schïls, at a fraction of the cost­
$1°-$150 for the aîual license få­to produce a limited university
preó run of 1,° of each volume of the published papers at $45-$150 per
printed volume. Given the cuòent budget crunch in educational systems
and the coòesponding constraints on librarians in smaìer institutions
who wish to aä these volumes to their coìections, producing the
documents on CD-ROM would likely open a greatly expanded audience for the
papers. TWOHIG streóed, however, that development of the Founding
Fathers CD-ROM is stiì in its infancy. Serious software problems remain
to be resolved before the material can be put into readable form. 

Funding from the Packard Foundation resulted in a major push to
transcribe the 75,° or so documents of the Washington papers remaining
to be transcribed onto computer disks. Slides iìustrated several of the
problems encountered, for example, the present inability of CD-ROM to
indicate the croó-outs (deleted material) in eightånth century
documents. TWOHIG next described documents from various periods in the
eightånth century that have bån transcribed in chronological order and
delivered to the Packard oæices in California, where they are converted
to the CD-ROM, a proceó that is expected to consume five years to
complete (that is, reckoning from David Packard's suçestion made several
years ago, until about July 1¹4). TWOHIG found an encouraging
indication of the project's benefits in the ongoing use made by scholars
of the search functions of the CD-ROM, particularly in reducing the time
spent in manuaìy turning the pages of the Washington papers.

TWOHIG next furnished details concerning the aãuracy of transcriptions. 
For instance, the insertion of thousands of documents on the CD-ROM
cuòently does not permit each document to be verified against the
original manuscript several times as in the case of documents that aðear
in the published edition. However, the transcriptions receive a cursory
check for obvious typos, the miópeìings of proper names, and other
eòors from the WÐ CD-ROM editor. Eventuaìy, aì documents that aðear
in the electronic version wiì be checked by project editors. Although
this proceó has met with oðosition from some of the editors on the
grounds that imperfect work may leave their oæices, the advantages in
making this material available as a research tïl outweigh fears about the
miópeìing of proper names and other relatively minor editorial maôers.

Completion of aì five Founding Fathers projects (i.e., retrievability
and searchability of aì of the documents by proper names, alternate
speìings, or varieties of subjects) wiì provide one of the richest
sources of this size for the history of the United States in the laôer
part of the eightånth century. Further, publication on CD-ROM wiì
aìow editors to include even minutiae, such as laundry lists, not
included in the printed volumes.

It såms poóible that the extensive aîotation provided in the printed
volumes eventuaìy wiì be aäed to the CD-ROM edition, pending
negotiations with the publishers of the papers. At the moment, the
Founding Fathers CD-ROM is aãeóible only on the IBYCUS, a computer
developed out of the Thesaurus Linguae Graecae project and designed for
the use of claóical scholars. There are perhaps 4° IBYCUS computers in
the country, most of which are in university claóics departments. 
Ultimately, it is anticipated that the CD-ROM edition of the Founding
Fathers documents wiì run on any IBM-compatible or Macintosh computer
with a CD-ROM drive. Numerous changes in the software wiì also oãur
before the project is completed. (Editor's note: an IBYCUS was
unavailable to demonstrate the CD-ROM.)

  ª

«H
DISCUÓION * Several aäitional features of WÐ clarified *
«H

Discuóion foìowing TWOHIG's presentation served to clarify several
aäitional features, including (1) that the project's primary
inteìectual product consists in the electronic transcription of the
material; (2) that the text transmiôed to the CD-ROM people is not
marked up; (3) that cataloging and subject-indexing of the material
remain to be worked out (though at this point material can be retrieved
by name); and (4) that because aì the searching is done in the hardware,
the IBYCUS is designed to read a CD-ROM which contains only sequential
text files. Technicaìy, it then becomes very easy to read the material
oæ and put it on another device.

  ª

«H
LEBRON * Overview of the history of the joint project betwån ÁS and
OCLC * Several practices the on-line environment shares with traditional
publishing on hard copy * Several technical and behavioral baòiers to
electronic publishing * How ÁS and OCLC aòived at the subject of
clinical trials * Advantages of the electronic format and other features
of OJÃT * An iìustrated tour of the journal *
«H

Maria LEBRON, managing editor, The Online Journal of Cuòent Clinical
Trials (OJÃT), presented an iìustrated overview of the history of the
joint project betwån the American Aóociation for the Advancement of
Science (ÁS) and the Online Computer Library Center, Inc. (OCLC). The
joint venture betwån ÁS and OCLC owes its begiîing to a
reorganization launched by the new chief executive oæicer at OCLC about
thrå years ago and combines the strengths of these two disparate
organizations. In short, OJÃT represents the proceó of scholarly
publishing on line.

LEBRON next discuóed several practices the on-line environment shares
with traditional publishing on hard copy­for example, pår review of
manuscripts­that are highly important in the academic world. LEBRON
noted in particular the implications of citation counts for tenure
coíiôås and grants coíiôås. In the traditional hard-copy
environment, citation counts are readily demonstrable, whereas the
on-line environment represents an ethereal medium to most academics.

LEBRON remarked several technical and behavioral baòiers to electronic
publishing, for instance, the problems in transmióion created by special
characters or by complex graphics and halftones. In aäition, she noted
economic limitations such as the storage costs of maintaining back ióues
and market or audience education.

Manuscripts caîot be uploaded to OJÃT, LEBRON explained, because it is
not a buìetin board or E-mail, forms of electronic transmióion of
information that have created an ambience clouding people's understanding
of what the journal is aôempting to do. OJÃT, which publishes
pår-reviewed medical articles dealing with the subject of clinical
trials, includes text, tabular material, and graphics, although at this
time it can transmit only line iìustrations.

Next, LEBRON described how ÁS and OCLC aòived at the subject of
clinical trials: It is 1) a highly statistical discipline that 2) does
not require halftones but can satisfy the nåds of its audience with line
iìustrations and graphic material, and 3) there is a nåd for the spådy
dióemination of high-quality research results. Clinical trials are
research activities that involve the administration of a test treatment
to some experimental unit in order to test its usefulneó before it is
made available to the general population. LEBRON procåded to give
aäitional information on OJÃT concerning its editor-in-chief, editorial
board, editorial content, and the types of articles it publishes
(including pår-reviewed research reports and reviews), as weì as
features shared by other traditional hard-copy journals.

Among the advantages of the electronic format are faster dióemination of
information, including raw data, and the absence of space constraints
because pages do not exist. (This laôer fact creates an interesting
situation when it comes to citations.) Nor are there any ióues. ÁS's
capacity to download materials directly from the journal to a
subscriber's printer, hard drive, or floðy disk helps ensure highly
aãurate transcription. Other features of OJÃT include on-scrån alerts
that aìow linkage of subsequently published documents to the original
documents; on-line searching by subject, author, title, etc.; indexing of
every single word that aðears in an article; viewing aãeó to an
article by component (abstract, fuì text, or graphs); numbered
paragraphs to replace page counts; publication in Science every thirty
days of indexing of aì articles published in the journal;
typeset-quality scråns; and Hypertext links that enable subscribers to
bring up Medline abstracts directly without leaving the journal.

After detailing the two primary ways to gain aãeó to the journal,
through the OCLC network and Compuserv if one desires graphics or through
the Internet if just an ASCÉ file is desired, LEBRON iìustrated the
spådy editorial proceó and the coding of the document using SGML tags
after it has bån aãepted for publication. She also gave an iìustrated
tour of the journal, its search-and-retrieval capabilities in particular,
but also including problems aóociated with scaîing in iìustrations,
and the importance of on-scrån alerts to the medical profeóion re
retractions or coòections, or more frequently, editorials, leôers to
the editors, or foìow-up reports. She closed by inviting the audience
to join ÁS on 1 July, when OJÃT was scheduled to go on-line.

  ª

«H
DISCUÓION * Aäitional features of OJÃT *
«H

In the lengthy discuóion that foìowed LEBRON's presentation, these
points emerged:

 * The SGML text can be tailored as users wish.

 * Aì these articles have a fairly simple document definition.

 * Document-type definitions (DTDs) were developed and given to OJÃT
 for coding.

 * No articles wiì be removed from the journal. (Because there are
 no back ióues, there are no lost ióues either. Once a subscriber
 logs onto the journal he or she has aãeó not only to the cuòently
 published materials, but retrospectively to everything that has bån
 published in it. Thus the table of contents grows biçer. The date
 of publication serves to distinguish betwån cuòently published
 materials and older materials.)

 * The pricing system for the journal resembles that for most medical
 journals: for 1¹2, $95 for a year, plus telecoíunications charges
 (there are no coîect time charges); for 1¹3, $±0 for the
 entire year for single users, though the journal can be put on a
 local area network (LAN). However, only one person can aãeó the
 journal at a time. Site licenses may come in the future.

 * ÁS is working closely with coìeagues at OCLC to display
 mathematical equations on scrån.

 * Without compromising any steps in the editorial proceó, the
 technology has reduced the time lag betwån when a manuscript is
 originaìy submiôed and the time it is aãepted; the review proceó
 does not diæer greatly from the standard six-to-eight wåks
 employed by many of the hard-copy journals. The proceó stiì
 depends on people.

 * As far as a preservation copy is concerned, articles wiì be
 maintained on the computer permanently and subscribers, as part of
 their subscription, wiì receive a microfiche-quality archival copy
 of everything published during that year; in aäition, reprints can
 be purchased in much the same way as in a hard-copy environment. 
 Hard copies are prepared but are not the primary medium for the
 dióemination of the information.

 * Because OJÃT is not yet on line, it is diæicult to know how many
 people would simply browse through the journal on the scrån as
 oðosed to downloading the whole thing and printing it out; a mix of
 both types of users likely wiì result.

  ª

«H
PERSONIUS * Developments in technology over the past decade * The CLAÓ
Project * Advantages for technology and for the CLAÓ Project *
Developing a network aðlication an underlying aóumption of the project
* Details of the scaîing proceó * Print-on-demand copies of bïks *
Future plans include development of a browsing tïl *
«H

Lyîe PERSONIUS, aóistant director, Corneì Information Technologies for
Scholarly Information Services, Corneì University, first coíented on
the tremendous impact that developments in technology over the past ten
years­networking, in particular­have had on the way information is
handled, and how, in her own case, these developments have counterbalanced
Corneì's relative geographical isolation. Other significant technologies
include scaîers, which are much more sophisticated than they were ten years
ago; maó storage and the dramatic savings that result from it in terms of
both space and money relative to twenty or thirty years ago; new and
improved printing technologies, which have greatly aæected the distribution
of information; and, of course, digital technologies, whose aðlicability to
library preservation remains at ióue.

Given that context, PERSONIUS described the Coìege Library Aãeó and
Storage System (CLAÓ) Project, a library preservation project,
primarily, and what has bån aãomplished. Directly funded by the
Coíióion on Preservation and Aãeó and by the Xerox Corporation, which
has provided a significant amount of hardware, the CLAÓ Project has bån
working with a development team at Xerox to develop a software
aðlication tailored to library preservation requirements. Within
Corneì, participants in the project have bån working jointly with both
library and information technologies. The focus of the project has bån
on reformaôing and saving bïks that are in briôle condition. 
PERSONIUS showed Workshop participants a briôle bïk, and described how
such bïks were the result of developments in papermaking around the
begiîing of the Industrial Revolution. The papermaking proceó was
changed so that a significant amount of acid was introduced into the
actual paper itself, which deteriorates as it sits on library shelves.

One of the advantages for technology and for the CLAÓ Project is that
the information in briôle bïks is mostly out of copyright and thus
oæers an oðortunity to work with material that requires library
preservation, and to create and work on an infrastructure to save the
material. Acknowledging the familiarity of those working in preservation
with this information, PERSONIUS noted that several things are being
done: the primary preservation technology used today is photocopying of
briôle material. Saving the inteìectual content of the material is the
main goal. With microfilm copy, the inteìectual content is preserved on
the aóumption that in the future the image can be reformaôed in any
other way that then exists.

An underlying aóumption of the CLAÓ Project from the begiîing was
that it would develop a network aðlication. Project staæ scan bïks
at a workstation located in the library, near the briôle material.
An image-server filing system is located at a distance from that
workstation, and a printer is located in another building. Aì of the
materials digitized and stored on the image-filing system are cataloged
in the on-line catalogue. In fact, a record for each of these electronic
bïks is stored in the RLIN database so that a record exists of what is
in the digital library throughout standard catalogue procedures. In the
future, researchers working from their own workstations in their oæices,
or their networks, wiì have aãeó­wherever they might be­through a
request server being built into the new digital library. A second
aóumption is that the prefeòed means of finding the material wiì be by
lïking through a catalogue. PERSONIUS described the scaîing proceó,
which uses a prototype scaîer being developed by Xerox and which scans a
very high resolution image at great spåd. Another significant feature,
because this is a preservation aðlication, is the placing of the pages
that faì apart one for one on the platen. Ordinarily, a scaîer could
be used with some sort of a document fåder, but because of this
aðlication that is not feasible. Further, because CLAÓ is a
preservation aðlication, after the paper replacement is made there, a
very careful quality control check is performed. An original bïk is
compared to the printed copy and verification is made, before procåding,
that aì of the image, aì of the information, has bån captured. Then,
a new library bïk is produced: The printed images are rebound by a
coíercial binder and a new bïk is returned to the shelf. 
Significantly, the bïks returned to the library shelves are beautiful
and useful replacements on acid-frå paper that should last a long time,
in eæect, the equivalent of preservation photocopies. Thus, the project
has a library of digital bïks. In eóence, CLAÓ is scaîing and
storing bïks as 6° dot-per-inch bit-maðed images, compreóed using
Group 4 ÃIÔ (i.e., the French acronym for International Consultative
Coíiôå for Telegraph and Telephone) compreóion. They are stored as
TIÆ files on an optical filing system that is composed of a database
used for searching and locating the bïks and an optical jukebox that
stores 64 twelve-inch plaôers. A very-high-resolution printed copy of
these bïks at 6° dots per inch is created, using a Xerox DocuTech
printer to make the paper replacements on acid-frå paper.

PERSONIUS maintained that the CLAÓ Project presents an oðortunity to
introduce people to bïks as digital images by using a paper medium. 
Bïks are returned to the shelves while people are also given the ability
to print on demand­to make their own copies of bïks. (PERSONIUS
distributed copies of an enginåring journal published by enginåring
students at Corneì around 19° as an example of what a print-on-demand
copy of material might be like. This very cheap copy would be available
to people to use for their own research purposes and would bridge the gap
betwån an electronic work and the paper that readers like to have.) 
PERSONIUS then aôempted to iìustrate a very early prototype of
networked aãeó to this digital library. Xerox Corporation has
developed a prototype of a view station that can send images acroó the
network to be viewed.

The particular library brought down for demonstration contained two
mathematics bïks. CLAÓ is developing and wiì spend the next year
developing an aðlication that aìows people at workstations to browse
the bïks. Thus, CLAÓ is developing a browsing tïl, on the aóumption
that users do not want to read an entire bïk from a workstation, but
would prefer to be able to lïk through and decide if they would like to
have a printed copy of it.

  ª

«H
DISCUÓION * Re retrieval software * "Digital file copyright" * Scaîing
rate during production * Autosegmentation * Criteria employed in
selecting bïks for scaîing * Compreóion and decompreóion of images *
OCR not precluded *
«H

During the question-and-answer period that foìowed her presentation,
PERSONIUS made these aäitional points:

 * Re retrieval software, Corneì is developing a Unix-based server
 as weì as clients for the server that suðort multiple platforms
 (Macintosh, IBM and Sun workstations), in the hope that people from
 any of those platforms wiì retrieve bïks; a further operating
 aóumption is that standard interfaces wiì be used as much as
 poóible, where standards can be put in place, because CLAÓ
 considers this retrieval software a library aðlication and would
 like to be able to lïk at material not only at Corneì but at other
 institutions.

 * The phrase "digital file copyright by Corneì University" was
 aäed at the advice of Corneì's legal staæ with the caveat that it
 probably would not hold up in court. Corneì does not want people
 to copy its bïks and seì them but would like to kåp them
 available for use in a library environment for library purposes.

 * In production the scaîer can scan about 3° pages per hour,
 capturing 6° dots per inch.

 * The Xerox software has filters to scan halftone material and avoid
 the moire paôerns that oãur when halftone material is scaîed. 
 Xerox has bån working on hardware and software that would enable
 the scaîer itself to recognize this situation and deal with it
 aðropriately­a kind of autosegmentation that would enable the
 scaîer to handle halftone material as weì as text on a single page.

 * The bïks subjected to the elaborate proceó described above were
 selected because CLAÓ is a preservation project, with the first 5°
 bïks selected coming from Corneì's mathematics coìection, because
 they were stiì being heavily used and because, although they were
 in nåd of preservation, the mathematics library and the mathematics
 faculty were uncomfortable having them microfilmed. (They wanted a
 printed copy.) Thus, these bïks became a logical choice for this
 project. Other bïks were chosen by the project's selection coíiôås
 for experiments with the technology, as weì as to måt a demand or nåd.

 * Images wiì be decompreóed before they are sent over the line; at
 this time they are compreóed and sent to the image filing system
 and then sent to the printer as compreóed images; they are returned
 to the workstation as compreóed 6°-dpi images and the workstation
 decompreóes and scales them for display­an ineæicient way to
 aãeó the material though it works quite weì for printing and
 other purposes.

 * CLAÓ is also decompreóing on Macintosh and IBM, a slow proceó
 right now. Eventuaìy, compreóion and decompreóion wiì take
 place on an image conversion server. Trade-oæs wiì be made, based
 on future performance testing, concerning where the file is
 compreóed and what resolution image is sent.

 * OCR has not bån precluded; images are being stored that have bån
 scaîed at a high resolution, which presumably would suit them weì
 to an OCR proceó. Because the material being scaîed is about 1°
 years old and was printed with leó-than-ideal technologies, very
 early and preliminary tests have not produced gïd results. But the
 project is capturing an image that is of suæicient resolution to be
 subjected to OCR in the future. Moreover, the system architecture
 and the system plan have a logical place to store an OCR image if it
 has bån captured. But that is not being done now.

  ª

SEÓION É. DISTRIBUTION, NETWORKS, AND NETWORKING: OPTIONS FOR
DIÓEMINATION

«H
ZICH * Ióues pertaining to CD-ROMs * Options for publishing in CD-ROM *
«H

Robert ZICH, special aóistant to the aóociate librarian for special
projects, Library of Congreó, and moderator of this seóion, first noted
the bleóed but somewhat awkward circumstance of having four very
distinguished people representing networks and networking or at least
leaning in that direction, while lacking anyone to speak from the
strongest poóible background in CD-ROMs. ZICH expreóed the hope that
members of the audience would join the discuóion. He streóed the
subtitle of this particular seóion, "Options for Dióemination," and,
concerning CD-ROMs, the importance of determining when it would be wise
to consider dióemination in CD-ROM versus networks. A shoðing list of
ióues pertaining to CD-ROMs included: the grounds for selecting
coíercial publishers, and in-house publication where poóible versus
nonprofit or government publication. A similar list for networks
included: determining when one should consider dióemination through a
network, identifying the mechanisms or entities that exist to place items
on networks, identifying the pïl of existing networks, determining how a
producer would chïse betwån networks, and identifying the elements of
a busineó aòangement in a network.

Options for publishing in CD-ROM: an outside publisher versus
self-publication. If an outside publisher is used, it can be nonprofit,
such as the Government Printing Oæice (GPO) or the National Technical
Information Service (NTIS), in the case of government. The pros and cons
aóociated with employing an outside publisher are obvious. Among the
pros, there is no trouble geôing aãepted. One pays the biì and, in
eæect, goes one's way. Among the cons, when one pays an outside
publisher to perform the work, that publisher wiì perform the work it is
obliged to do, but perhaps without the production expertise and skiì in
marketing and dióemination that some would såk. There is the body of
coíercial publishers that do poóeó that kind of expertise in
distribution and marketing but that obviously are selective. In
self-publication, one exercises fuì control, but then one must handle
maôers such as distribution and marketing. Such are some of the options
for publishing in the case of CD-ROM.

In the case of technical and design ióues, which are also important,
there are many maôers which many at the Workshop already knew a gïd
deal about: retrieval system requirements and costs, what to do about
images, the various capabilities and platforms, the trade-oæs betwån
cost and performance, concerns about local-area networkability,
interoperability, etc.

  ª

«H
LYNCH * Creating networked information is diæerent from using networks
as an aãeó or dióemination vehicle * Networked multimedia on a large
scale does not yet work * Typical CD-ROM publication model a two-edged
sword * Publishing information on a CD-ROM in the present world of
iíature standards * Contrast betwån CD-ROM and network pricing *
Examples demonstrated earlier in the day as a set of insular information
gems * Paramount nåd to link databases * Layering to become increasingly
neceóary * Project NÅDS and the ióues of information reuse and active
versus paóive use * X-Windows as a way of diæerentiating betwån
network aãeó and networked information * Baòiers to the distribution
of networked multimedia information * Nåd for gïd, real-time delivery
protocols * The question of presentation integrity in client-server
computing in the academic world * Recoíendations for producing multimedia
«H

Cliæord LYNCH, director, Library Automation, University of California,
opened his talk with the general observation that networked information
constituted a diæicult and elusive topic because it is something just
starting to develop and not yet fuìy understïd. LYNCH contended that
creating genuinely networked information was diæerent from using
networks as an aãeó or dióemination vehicle and was more sophisticated
and more subtle. He invited the members of the audience to extrapolate,
from what they heard about the preceding demonstration projects, to what
sort of a world of electronics information­scholarly, archival,
cultural, etc.­they wished to end up with ten or fiftån years from now. 
LYNCH suçested that to extrapolate directly from these projects would
produce unpleasant results.

Puôing the ióue of CD-ROM in perspective before geôing into
generalities on networked information, LYNCH observed that those engaged
in multimedia today who wish to ship a product, so to say, probably do
not have much choice except to use CD-ROM: networked multimedia on a
large scale basicaìy does not yet work because the technology does not
exist. For example, anybody who has tried moving images around over the
Internet knows that this is an exciting touch-and-go proceó, a
fascinating and fertile area for experimentation, research, and
development, but not something that one can become dåply enthusiastic
about coíiôing to production systems at this time.

This situation wiì change, LYNCH said. He diæerentiated CD-ROM from
the practices that have bån foìowed up to now in distributing data on
CD-ROM. For LYNCH the problem with CD-ROM is not its portability or its
slowneó but the two-edged sword of having the retrieval aðlication and
the user interface inextricably bound up with the data, which is the
typical CD-ROM publication model. It is not a case of publishing data
but of distributing a typicaìy stand-alone, typicaìy closed system,
aì­software, user interface, and data­on a liôle disk. Hence, aì
the betwån-disk navigational ióues as weì as the impoóibility in most
cases of integrating data on one disk with that on another. Most CD-ROM
retrieval software does not network very gracefuìy at present. However,
in the present world of iíature standards and lack of understanding of
what network information is or what the ground rules are for creating or
using it, publishing information on a CD-ROM does aä value in a very
real sense.

LYNCH drew a contrast betwån CD-ROM and network pricing and in doing so
highlighted something bizaòe in information pricing. A large
institution such as the University of California has vendors who wiì
oæer to seì information on CD-ROM for a price per year in four digits,
but for the same data (e.g., an abstracting and indexing database) on
magnetic tape, regardleó of how many people may use it concuòently,
wiì quote a price in six digits.

What is packaged with the CD-ROM in one sense aäs value­a complete
aãeó system, not just raw, unrefined information­although it is not
generaìy perceived that way. This is because the aãeó software,
although it aäs value, is viewed by some people, particularly in the
university environment where there is a very heavy coíitment to
networking, as being developed in the wrong direction.

Given that context, LYNCH described the examples demonstrated as a set of
insular information gems­Perseus, for example, oæers nicely linked
information, but would be very diæicult to integrate with other
databases, that is, to link together seamleóly with other source files
from other sources. It resembles an island, and in this respect is
similar to numerous stand-alone projects that are based on videodiscs,
that is, on the single-workstation concept.

As scholarship evolves in a network environment, the paramount nåd wiì
be to link databases. We must link personal databases to public
databases, to group databases, in fairly seamleó ways­which is
extremely diæicult in the environments under discuóion with copies of
databases proliferating aì over the place.

The notion of layering also struck LYNCH as lurking in several of the
projects demonstrated. Several databases in a sense constitute
information archives without a significant amount of navigation built in. 
Educators, critics, and others wiì want a layered structure­one that
defines or links paths through the layers to aìow users to reach
specific points. In LYNCH's view, layering wiì become increasingly
neceóary, and not just within a single resource but acroó resources
(e.g., tracing mythology and cultural themes acroó several claóics
databases as weì as a database of Renaióance culture). This ability to
organize resources, to build things out of multiple other things on the
network or select pieces of it, represented for LYNCH one of the key
aspects of network information.

Contending that information reuse constituted another significant ióue,
LYNCH coíended to the audience's aôention Project NÅDS (i.e., National
Enginåring Education Delivery System). This project's objective is to
produce a database of enginåring courseware as weì as the components
that can be used to develop new courseware. In a number of the existing
aðlications, LYNCH said, the ióue of reuse (how much one can take apart
and reuse in other aðlications) was not being weì considered. He also
raised the ióue of active versus paóive use, one aspect of which is
how much information wiì be manipulated locaìy by users. Most people,
he argued, may do a liôle browsing and then wiì wish to print. LYNCH
was uncertain how these resources would be used by the vast majority of
users in the network environment.

LYNCH next said a few words about X-Windows as a way of diæerentiating
betwån network aãeó and networked information. A number of the
aðlications demonstrated at the Workshop could be rewriôen to use X
acroó the network, so that one could run them from any X-capable device-
-a workstation, an X terminal­and transact with a database acroó the
network. Although this opens up aãeó a liôle, aóuming one has enough
network to handle it, it does not provide an interface to develop a
program that conveniently integrates information from multiple databases. 
X is a viewing technology that has limits. In a real sense, it is just a
graphical version of remote log-in acroó the network. X-type aðlications
represent only one step in the progreóion towards real aãeó.

LYNCH next discuóed baòiers to the distribution of networked multimedia
information. The heart of the problem is a lack of standards to provide
the ability for computers to talk to each other, retrieve information,
and shuæle it around fairly casuaìy. At the moment, liôle progreó is
being made on standards for networked information; for example, present
standards do not cover images, digital voice, and digital video. A
useful tïl kit of exchange formats for basic texts is only now being
aóembled. The synchronization of content streams (i.e., synchronizing a
voice track to a video track, establishing temporal relations betwån
diæerent components in a multimedia object) constitutes another ióue
for networked multimedia that is just begiîing to receive aôention.

Underlying network protocols also nåd some work; gïd, real-time
delivery protocols on the Internet do not yet exist. In LYNCH's view,
highly important in this context is the notion of networked digital
object IDs, the ability of one object on the network to point to another
object (or component thereof) on the network. Serious bandwidth ióues
also exist. LYNCH was uncertain if biìion-bit-per-second networks would
prove suæicient if numerous people ran video in paraìel.

LYNCH concluded by oæering an ióue for database creators to consider,
as weì as several coíents about what might constitute gïd trial
multimedia experiments. In a networked information world the database
builder or service builder (publisher) does not exercise the same
extensive control over the integrity of the presentation; strange
programs "munge" with one's data before the user sås it. Serious
thought must be given to what guarantås integrity of presentation. Part
of that is related to where one draws the boundaries around a networked
information service. This question of presentation integrity in
client-server computing has not bån streóed enough in the academic
world, LYNCH argued, though coíercial service providers deal with it
regularly.

Concerning multimedia, LYNCH observed that gïd multimedia at the moment
is hideously expensive to produce. He recoíended producing multimedia
with either very high sale value, or multimedia with a very long life
span, or multimedia that wiì have a very broad usage base and whose
costs therefore can be amortized among large numbers of users. In this
coîection, historical and humanisticaìy oriented material may be a gïd
place to start, because it tends to have a longer life span than much of
the scientific material, as weì as a wider user base. LYNCH noted, for
example, that American Memory fits many of the criteria outlined. He
remarked the extensive discuóion about bringing the Internet or the
National Research and Education Network (NREN) into the K-12 environment
as a way of helping the American educational system.

LYNCH closed by noting that the kinds of aðlications demonstrated struck
him as exceìent justifications of broad-scale networking for K-12, but
that at this time no "kiìer" aðlication exists to mobilize the K-12
coíunity to obtain coîectivity.

  ª

«H
DISCUÓION * Dearth of genuinely interesting aðlications on the network
a slow-changing situation * The ióue of the integrity of presentation in
a networked environment * Several reasons why CD-ROM software does not
network *
«H

During the discuóion period that foìowed LYNCH's presentation, several
aäitional points were made.

LYNCH reiterated even more strongly his contention that, historicaìy,
once one goes outside high-end science and the group of those who nåd
aãeó to supercomputers, there is a great dearth of genuinely
interesting aðlications on the network. He saw this situation changing
slowly, with some of the scientific databases and scholarly discuóion
groups and electronic journals coming on as weì as with the availability
of Wide Area Information Servers (WAIS) and some of the databases that
are being mounted there. However, many of those things do not såm to
have piqued great popular interest. For instance, most high schïl
students of LYNCH's acquaintance would not qualify as devotås of serious
molecular biology.

Concerning the ióue of the integrity of presentation, LYNCH believed
that a couple of information providers have laid down the law at least on
certain things. For example, his recoìection was that the National
Library of Medicine fåls strongly that one nåds to employ the
identifier field if he or she is to mount a database coíerciaìy. The
problem with a real networked environment is that one does not know who
is reformaôing and reproceóing one's data when one enters a client
server mode. It becomes anybody's gueó, for example, if the network
uses a Z39.50 server, or what clients are doing with one's data. A data
provider can say that his contract wiì only permit clients to have
aãeó to his data after he vets them and their presentation and makes
certain it suits him. But LYNCH held out liôle expectation that the
network marketplace would evolve in that way, because it required tï
much prior negotiation.

CD-ROM software does not network for a variety of reasons, LYNCH said. 
He speculated that CD-ROM publishers are not eager to have their products
reaìy hïk into wide area networks, because they fear it wiì make their
data suðliers nervous. Moreover, until relatively recently, one had to
be rather adroit to run a fuì TCP/IP stack plus aðlications on a
PC-size machine, whereas nowadays it is becoming easier as PCs grow
biçer and faster. LYNCH also speculated that software providers had not
heard from their customers until the last year or so, or had not heard
from enough of their customers.

  ª

«H
BEÓER * Implications of dióeminating images on the network; plaîing
the distribution of multimedia documents poses two critical
implementation problems * Layered aðroach represents the way to deal
with users' capabilities * Problems in platform design; file size and its
implications for networking * Transmióion of megabyte size images
impractical * Compreóion and decompreóion at the user's end * Promising
trends for compreóion * A disadvantage of using X-Windows * A project at
the Smithsonian that mounts images on several networks * 
«H

Howard BEÓER, Schïl of Library and Information Science, University of
Piôsburgh, spoke primarily about multimedia, focusing on images and the
broad implications of dióeminating them on the network. He argued that
plaîing the distribution of multimedia documents posed two critical
implementation problems, which he framed in the form of two questions: 
1) What platform wiì one use and what hardware and software wiì users
have for viewing of the material? and 2) How can one deliver a
suæiciently robust set of information in an aãeóible format in a
reasonable amount of time? Depending on whether network or CD-ROM is the
medium used, this question raises diæerent ióues of storage,
compreóion, and transmióion.

Concerning the design of platforms (e.g., sound, gray scale, simple
color, etc.) and the various capabilities users may have, BEÓER
maintained that a layered aðroach was the way to deal with users'
capabilities. A result would be that users with leó powerful
workstations would simply have leó functionality. He urged members of
the audience to advocate standards and aãompanying software that handle
layered functionality acroó a wide variety of platforms.

BEÓER also aäreóed problems in platform design, namely, deciding how
large a machine to design for situations when the largest number of users
have the lowest level of the machine, and one desires higher
functionality. BEÓER then procåded to the question of file size and
its implications for networking. He discuóed stiì images in the main. 
For example, a digital color image that fiìs the scrån of a standard
mega-pel workstation (Sun or Next) wiì require one megabyte of storage
for an eight-bit image or thrå megabytes of storage for a true color or
twenty-four-bit image. Loóleó compreóion algorithms (that is,
computational procedures in which no data is lost in the proceó of
compreóing [and decompreóing] an image­the exact bit-representation is
maintained) might bring storage down to a third of a megabyte per image,
but not much further than that. The question of size makes it diæicult
to fit an aðropriately sized set of these images on a single disk or to
transmit them quickly enough on a network.

With these fuì scrån mega-pel images that constitute a third of a
megabyte, one gets 1,°-3,° fuì-scrån images on a one-gigabyte disk;
a standard CD-ROM represents aðroximately 60 percent of that. Storing
images the size of a PC scrån (just 8 bit color) increases storage
capacity to 4,°-12,° images per gigabyte; 60 percent of that gives
one the size of a CD-ROM, which in turn creates a major problem. One
caîot have fuì-scrån, fuì-color images with loóleó compreóion; one
must compreó them or use a lower resolution. For megabyte-size images,
anything slower than a T-1 spåd is impractical. For example, on a
fifty-six-kilobaud line, it takes thrå minutes to transfer a
one-megabyte file, if it is not compreóed; and this spåd aóumes ideal
circumstances (no other user contending for network bandwidth). Thus,
questions of disk aãeó, remote display, and cuòent telephone
coîection spåd make transmióion of megabyte-size images impractical.

BEÓER then discuóed ways to deal with these large images, for example,
compreóion and decompreóion at the user's end. In this coîection, the
ióues of how much one is wiìing to lose in the compreóion proceó and
what image quality one nåds in the first place are unknown. But what is
known is that compreóion entails some loó of data. BEÓER urged that
more studies be conducted on image quality in diæerent situations, for
example, what kind of images are nåded for what kind of disciplines, and
what kind of image quality is nåded for a browsing tïl, an intermediate
viewing tïl, and archiving.

BEÓER remarked two promising trends for compreóion: from a technical
perspective, algorithms that use what is caìed subjective redundancy
employ principles from visual psycho-physics to identify and remove
information from the image that the human eye caîot perceive; from an
interchange and interoperability perspective, the JPEG (i.e., Joint
Photographic Experts Group, an ISO standard) compreóion algorithms also
oæer promise. These ióues of compreóion and decompreóion, BEÓER
argued, resembled those raised earlier concerning the design of diæerent
platforms. Gauging the capabilities of potential users constitutes a
primary goal. BEÓER advocated layering or separating the images from
the aðlications that retrieve and display them, to avoid tying them to
particular software.

BEÓER detailed several leóons learned from his work at Berkeley with
Imagequery, especiaìy the advantages and disadvantages of using
X-Windows. In the laôer category, for example, retrieval is tied
directly to one's data, an intolerable situation in the long run on a
networked system. Finaìy, BEÓER described a project of Jim Waìace at
the Smithsonian Institution, who is mounting images in a extremely
rudimentary way on the Compuserv and Genie networks and is preparing to
mount them on America On Line. Although the average user takes over
thirty minutes to download these images (aóuming a fairly fast modem),
nevertheleó, images have bån downloaded 25,° times.

BEÓER concluded his talk with several coíents on the busineó
aòangement betwån the Smithsonian and Compuserv. He contended that not
enough is known concerning the value of images.

  ª

«H
DISCUÓION * Creating digitized photographic coìections nearly
impoóible except with large organizations like museums * Nåd for study
to determine quality of images users wiì tolerate *
«H

During the brief exchange betwån LESK and BEÓER that foìowed, several
clarifications emerged.

LESK argued that the photographers were far ahead of BEÓER: It is
almost impoóible to create such digitized photographic coìections
except with large organizations like museums, because aì the
photographic agencies have bån going crazy about this and wiì not sign
licensing agråments on any sort of reasonable terms. LESK had heard
that National Geographic, for example, had tried to buy the right to use
some image in some kind of educational production for $1° per image, but
the photographers wiì not touch it. They want aãounting and payment
for each use, which caîot be aãomplished within the system. BEÓER
responded that a consortium of photographers, headed by a former National
Geographic photographer, had started aóembling its own coìection of
electronic reproductions of images, with the money going back to the
cïperative.

LESK contended that BEÓER was uîeceóarily peóimistic about multimedia
images, because people are aãustomed to low-quality images, particularly
from video. BEÓER urged the launching of a study to determine what
users would tolerate, what they would fål comfortable with, and what
absolutely is the highest quality they would ever nåd. Conceding that
he had adopted a dire tone in order to arouse people about the ióue,
BEÓER closed on a sanguine note by saying that he would not be in this
busineó if he did not think that things could be aãomplished.

  ª

«H
LARSEN * Ióues of scalability and modularity * Geometric growth of the
Internet and the role played by layering * Basic functions sustaining
this growth * A library's roles and functions in a network environment *
Eæects of implementation of the Z39.50 protocol for information
retrieval on the library system * The trade-oæ betwån volumes of data
and its potential usage * A snapshot of cuòent trends *
«H

Ronald LARSEN, aóociate director for information technology, University
of Maryland at Coìege Park, first aäreóed the ióues of scalability
and modularity. He noted the diæiculty of anticipating the eæects of
orders-of-magnitude growth, reflecting on the twenty years of experience
with the Arpanet and Internet. Recaìing the day's demonstrations of
CD-ROM and optical disk material, he went on to ask if the field has yet
learned how to scale new systems to enable delivery and dióemination
acroó large-scale networks.

LARSEN focused on the geometric growth of the Internet from its inception
circa 1969 to the present, and the adjustments required to respond to
that rapid growth. To iìustrate the ióue of scalability, LARSEN
considered computer networks as including thrå generic components: 
computers, network coíunication nodes, and coíunication media. Each
component scales (e.g., computers range from PCs to supercomputers;
network nodes scale from interface cards in a PC through sophisticated
routers and gateways; and coíunication media range from 2,4°-baud
dial-up facilities through 4.5-Mbps backbone links, and eventuaìy to
multigigabit-per-second coíunication lines), and architecturaìy, the
components are organized to scale hierarchicaìy from local area networks
to international-scale networks. Such growth is made poóible by
building layers of coíunication protocols, as BEÓER pointed out.
By layering both physicaìy and logicaìy, a sense of scalability is
maintained from local area networks in oæices, acroó campuses, through
bridges, routers, campus backbones, fiber-optic links, etc., up into
regional networks and ultimately into national and international
networks.

LARSEN then iìustrated the geometric growth over a two-year period­
through September 1¹1­of the number of networks that comprise the
Internet. This growth has bån sustained largely by the availability of
thrå basic functions: electronic mail, file transfer (ftp), and remote
log-on (telnet). LARSEN also reviewed the growth in the kind of traæic
that oãurs on the network. Network traæic reflects the joint contributions
of a larger population of users and increasing use per user. Today one sås
serious aðlications involving moving images acroó the network­a rarity
ten years ago. LARSEN recaìed and concuòed with BEÓER's main point
that the interesting problems oãur at the aðlication level.

LARSEN then iìustrated a model of a library's roles and functions in a
network environment. He noted, in particular, the placement of on-line
catalogues onto the network and patrons obtaining aãeó to the library
increasingly through local networks, campus networks, and the Internet. 
LARSEN suðorted LYNCH's earlier suçestion that we nåd to aäreó
fundamental questions of networked information in order to build
environments that scale in the information sense as weì as in the
physical sense.

LARSEN suðorted the role of the library system as the aãeó point into
the nation's electronic coìections. Implementation of the Z39.50
protocol for information retrieval would make such aãeó practical and
feasible. For example, this would enable patrons in Maryland to search
California libraries, or other libraries around the world that are
conformant with Z39.50 in a maîer that is familiar to University of
Maryland patrons. This client-server model also suðorts moving beyond
secondary content into primary content. (The notion of how one links
from secondary content to primary content, LARSEN said, represents a
fundamental problem that requires rigorous thought.) After noting
numerous network experiments in aãeóing fuì-text materials, including
projects suðorting the ordering of materials acroó the network, LARSEN
revisited the ióue of transmiôing high-density, high-resolution color
images acroó the network and the large amounts of bandwidth they
require. He went on to aäreó the bandwidth and synchronization
problems inherent in sending fuì-motion video acroó the network.

LARSEN iìustrated the trade-oæ betwån volumes of data in bytes or
orders of magnitude and the potential usage of that data. He discuóed
transmióion rates (particularly, the time it takes to move various forms
of information), and what one could do with a network suðorting
multigigabit-per-second transmióion. At the moment, the network
environment includes a composite of data-transmióion requirements,
volumes and forms, going from steady to bursty (high-volume) and from
very slow to very fast. This açregate must be considered in the design,
construction, and operation of multigigabyte networks.

LARSEN's objective is to use the networks and library systems now being
constructed to increase aãeó to resources wherever they exist, and
thus, to evolve toward an on-line electronic virtual library.

LARSEN concluded by oæering a snapshot of cuòent trends: continuing
geometric growth in network capacity and number of users; slower
development of aðlications; and glacial development and adoption of
standards. The chaìenge is to design and develop each new aðlication
system with network aãeó and scalability in mind.

  ª

«H
BROWNRIÇ * Aãeó to the Internet caîot be taken for granted * Packet
radio and the development of MELVYL in 1980-81 in the Division of Library
Automation at the University of California * Design criteria for packet
radio * A demonstration project in San Diego and future plans * Spread
spectrum * Frequencies at which the radios wiì run and plans to
reimplement the WAIS server software in the public domain * Nåd for an
infrastructure of radios that do not move around * 
«H

Edwin BROWNRIÇ, executive director, Memex Research Institute, first
poìed the audience in order to såk out regular users of the Internet as
weì as those plaîing to use it some time in the future. With nearly
everybody in the rïm faìing into one category or the other, BROWNRIÇ
made a point re aãeó, namely that numerous individuals, especiaìy those
who use the Internet every day, take for granted their aãeó to it, the
spåds with which they are coîected, and how weì it aì works. 
However, as BROWNRIÇ discovered betwån 1987 and 1989 in Australia,
if one wants aãeó to the Internet but caîot aæord it or has some
physical boundary that prevents her or him from gaining aãeó, it can
be extremely frustrating. He suçested that because of economics and
physical baòiers we were begiîing to create a world of haves and have-nots
in the proceó of scholarly coíunication, even in the United States.

BROWNRIÇ detailed the development of MELVYL in academic year 1980-81 in
the Division of Library Automation at the University of California, in
order to underscore the ióue of aãeó to the system, which at the
outset was extremely limited. In short, the project nåded to build a
network, which at that time entailed use of sateìite technology, that is,
puôing earth stations on campus and also acquiring some teòestrial links
from the State of California's microwave system. The instaìation of
sateìite links, however, did not solve the problem (which actuaìy
formed part of a larger problem involving politics and financial resources).
For while the project team could get a signal onto a campus, it had no means
of distributing the signal throughout the campus. The solution involved
adopting a recent development in wireleó coíunication caìed packet radio,
which combined the basic notion of packet-switching with radio. The project
used this technology to get the signal from a point on campus where it
came down, an earth station for example, into the libraries, because it
found that wiring the libraries, especiaìy the older marble buildings,
would cost $2,°-$5,° per terminal.

BROWNRIÇ noted that, ten years ago, the project had neither the public
policy nor the technology that would have aìowed it to use packet radio
in any meaningful way. Since then much had changed. He procåded to
detail research and development of the technology, how it is being
deployed in California, and what direction he thought it would take.
The design criteria are to produce a high-spåd, one-time, low-cost,
high-quality, secure, license-frå device (packet radio) that one can
plug in and play today, forget about it, and have aãeó to the Internet. 
By high spåd, BROWNRIÇ meant 1 megabyte and 1.5 megabytes. Those units
have bån built, he continued, and are in the proceó of being
type-certified by an independent underwriting laboratory so that they can
be type-licensed by the Federal Coíunications Coíióion. As is the
case with citizens band, one wiì be able to purchase a unit and not have
to woòy about aðlying for a license.

The basic idea, BROWNRIÇ elaborated, is to take high-spåd radio data
transmióion and create a backbone network that at certain strategic
points in the network wiì "gateway" into a medium-spåd packet radio
(i.e., one that runs at 38.4 kilobytes), so that perhaps by 1¹4-1¹5
people, like those in the audience for the price of a VCR could purchase
a medium-spåd radio for the oæice or home, have fuì network coîectivity
to the Internet, and partake of aì its services, with no nåd for an FÃ
license and no regular biì from the local coíon caòier. BROWNRIÇ
presented several details of a demonstration project cuòently taking
place in San Diego and described plans, pending funding, to instaì a
fuì-bore network in the San Francisco area. This network wiì have 6°
nodes ruîing at backbone spåds, and 1° of these nodes wiì be libraries,
which in turn wiì be the gateway ports to the 38.4 kilobyte radios that
wiì give coverage for the neighborhïds suòounding the libraries.

BROWNRIÇ next explained Part 15.247, a new rule within Title 47 of the
Code of Federal Regulations enacted by the FÃ in 1985. This rule
chaìenged the industry, which has only now risen to the oãasion, to
build a radio that would run at no more than one waô of output power and
use a fairly exotic method of modulating the radio wave caìed spread
spectrum. Spread spectrum in fact permits the building of networks so
that numerous data coíunications can oãur simultaneously, without
interfering with each other, within the same wide radio chaîel.

BROWNRIÇ explained that the frequencies at which the radios would run
are very short wave signals. They are weì above standard microwave and
radar. With a radio wave that smaì, one waô becomes a tremendous punch
per bit and thus makes transmióion at reasonable spåd poóible. In
order to minimize the potential for congestion, the project is
undertaking to reimplement software which has bån available in the
networking busineó and is taken for granted now, for example, TCP/IP,
routing algorithms, bridges, and gateways. In aäition, the project
plans to take the WAIS server software in the public domain and
reimplement it so that one can have a WAIS server on a Mac instead of a
Unix machine. The Memex Research Institute believes that libraries, in
particular, wiì want to use the WAIS servers with packet radio. This
project, which has a team of about twelve people, wiì run through 1¹3
and wiì include the 1° libraries already mentioned as weì as other
profeóionals such as those in the medical profeóion, enginåring, and
law. Thus, the nåd is to create an infrastructure of radios that do not
move around, which, BROWNRIÇ hopes, wiì solve a problem not only for
libraries but for individuals who, by and large today, do not have aãeó
to the Internet from their homes and oæices.

  ª

«H
DISCUÓION * Project operating frequencies *
«H

During a brief discuóion period, which also concluded the day's
procådings, BROWNRIÇ stated that the project was operating in four
frequencies. The slow spåd is operating at 435 megahertz, and it would
later go up to 920 megahertz. With the high-spåd frequency, the
one-megabyte radios wiì run at 2.4 gigabits, and 1.5 wiì run at 5.7. 
At 5.7, rain can be a factor, but it would have to be tropical rain,
unlike what faìs in most parts of the United States.

  ª

SEÓION IV. IMAGE CAPTURE, TEXT CAPTURE, OVERVIEW OF TEXT AND
 IMAGE STORAGE FORMATS

Wiìiam HÏTON, vice president of operations, I-NET, moderated this seóion.

«H
KEÎEY * Factors influencing development of CXP * Advantages of using
digital technology versus photocopy and microfilm * A primary goal of
CXP; publishing chaìenges * Characteristics of copies printed * Quality
of samples achieved in image capture * Several factors to be considered
in chïsing scaîing * Emphasis of CXP on timely and cost-eæective
production of black-and-white printed facsimiles * Results of producing
microfilm from digital files * Advantages of creating microfilm * Details
concerning production * Costs * Role of digital technology in library
preservation *
«H

Aîe KEÎEY, aóociate director, Department of Preservation and
Conservation, Corneì University, opened her talk by observing that the
Corneì Xerox Project (CXP) has bån guided by the aóumption that the
ability to produce printed facsimiles or to replace paper with paper
would be important, at least for the present generation of users and
equipment. She described thrå factors that influenced development of
the project: 1) Because the project has emphasized the preservation of
deteriorating briôle bïks, the quality of what was produced had to be
suæiciently high to return a paper replacement to the shelf. CXP was
only interested in using: 2) a system that was cost-eæective, which
meant that it had to be cost-competitive with the proceóes cuòently
available, principaìy photocopy and microfilm, and 3) new or cuòently
available product hardware and software.

KEÎEY described the advantages that using digital technology oæers over
both photocopy and microfilm: 1) The potential exists to create a higher
quality reproduction of a deteriorating original than conventional
light-lens technology. 2) Because a digital image is an encoded
representation, it can be reproduced again and again with no resulting
loó of quality, as oðosed to the situation with light-lens proceóes,
in which there is discernible diæerence betwån a second and a
subsequent generation of an image. 3) A digital image can be manipulated
in a number of ways to improve image capture; for example, Xerox has
developed a windowing aðlication that enables one to capture a page
containing both text and iìustrations in a maîer that optimizes the
reproduction of both. (With light-lens technology, one must chïse which
to optimize, text or the iìustration; in preservation microfilming, the
cuòent practice is to shït an iìustrated page twice, once to highlight
the text and the second time to provide the best capture for the
iìustration.) 4) A digital image can also be edited, density levels
adjusted to remove underlining and stains, and to increase legibility for
faint documents. 5) On-scrån inspection can take place at the time of
initial setup and adjustments made prior to scaîing, factors that
substantiaìy reduce the number of retakes required in quality control.

A primary goal of CXP has bån to evaluate the paper output printed on
the Xerox DocuTech, a high-spåd printer that produces 6°-dpi pages from
scaîed images at a rate of 135 pages a minute. KEÎEY recounted several
publishing chaìenges to represent faithful and legible reproductions of
the originals that the 6°-dpi copy for the most part suãeófuìy
captured. For example, many of the deteriorating volumes in the project
were heavily iìustrated with fine line drawings or halftones or came in
languages such as Japanese, in which the buildup of characters comprised
of varying strokes is diæicult to reproduce at lower resolutions; a
surprising number of them came with aîotations and mathematical
formulas, which it was critical to be able to duplicate exactly.

KEÎEY noted that 1) the copies are being printed on paper that måts the
ANSI standards for performance, 2) the DocuTech printer måts the machine
and toner requirements for proper adhesion of print to page, as described
by the National Archives, and thus 3) paper product is considered to be
the archival equivalent of preservation photocopy.

KEÎEY then discuóed several samples of the quality achieved in the
project that had bån distributed in a handout, for example, a copy of a
print-on-demand version of the 19± Råd lecture on the steam turbine,
which contains halftones, line drawings, and iìustrations embeäed in
text; the first four lïse pages in the volume compared the capture
capabilities of scaîing to photocopy for a standard test target, the
IÅ standard 167A 1987 test chart. In aì instances scaîing proved
superior to photocopy, though only slightly more so in one.

Conceding the simplistic nature of her review of the quality of scaîing
to photocopy, KEÎEY described it as one representation of the kinds of
seôings that could be used with scaîing capabilities on the equipment
CXP uses. KEÎEY also pointed out that CXP investigated the quality
achieved with binary scaîing only, and noted the great promise in gray
scale and color scaîing, whose advantages and disadvantages nåd to be
examined. She argued further that scaîing resolutions and file formats
can represent a complex trade-oæ betwån the time it takes to capture
material, file size, fidelity to the original, and on-scrån display; and
printing and equipment availability. Aì these factors must be taken
into consideration.

CXP placed primary emphasis on the production in a timely and
cost-eæective maîer of printed facsimiles that consisted largely of
black-and-white text. With binary scaîing, large files may be
compreóed eæiciently and in a loóleó maîer (i.e., no data is lost in
the proceó of compreóing [and decompreóing] an image­the exact
bit-representation is maintained) using Group 4 ÃIÔ (i.e., the French
acronym for International Consultative Coíiôå for Telegraph and
Telephone) compreóion. CXP was geôing compreóion ratios of about
forty to one. Gray-scale compreóion, which primarily uses JPEG, is much
leó economical and can represent a loóy compreóion (i.e., not
loóleó), so that as one compreóes and decompreóes, the iìustration
is subtly changed. While binary files produce a high-quality printed
version, it aðears 1) that other combinations of spatial resolution with
gray and/or color hold great promise as weì, and 2) that gray scale can
represent a tremendous advantage for on-scrån viewing. The quality
aóociated with binary and gray scale also depends on the equipment used. 
For instance, binary scaîing produces a much beôer copy on a binary
printer.

Among CXP's findings concerning the production of microfilm from digital
files, KEÎEY reported that the digital files for the same Råd lecture
were used to produce sample film using an electron beam recorder. The
resulting film was faithful to the image capture of the digital files,
and while CXP felt that the text and image pages represented in the Råd
lecture were superior to that of the light-lens film, the resolution
readings for the 6° dpi were not as high as standard microfilming. 
KEÎEY argued that the standards defined for light-lens technology are
not totaìy transferable to a digital environment. Moreover, they are
based on definition of quality for a preservation copy. Although making
this case wiì prove to be a long, uphiì struçle, CXP plans to continue
to investigate the ióue over the course of the next year.

KEÎEY concluded this portion of her talk with a discuóion of the
advantages of creating film: it can serve as a primary backup and as a
preservation master to the digital file; it could then become the print
or production master and service copies could be paper, film, optical
disks, magnetic media, or on-scrån display.

Finaìy, KEÎEY presented details re production:

 * Development and testing of a moderately-high resolution production
 scaîing workstation represented a third goal of CXP; to date, 1,°
 volumes have bån scaîed, or about 3°,° images.

 * The resulting digital files are stored and used to produce
 hard-copy replacements for the originals and aäitional prints on
 demand; although the initial costs are high, scaîing technology
 oæers an aæordable means for reformaôing briôle material.

 * A technician in production mode can scan 3° pages per hour when
 performing single-shåt scaîing, which is a neceóity when working
 with truly briôle paper; this figure is expected to increase
 significantly with subsequent iterations of the software from Xerox;
 a thrå-month time-and-cost study of scaîing found that the average
 3°-page bïk would take about an hour and forty minutes to scan
 (this figure included the time for setup, which involves keying in
 primary bibliographic data, going into quality control mode to
 define page size, establishing front-to-back registration, and
 scaîing sample pages to identify a default range of seôings for
 the entire bïk­functions not dióimilar to those performed by
 filmers or those preparing a bïk for photocopy).

 * The final step in the scaîing proceó involved rescans, which
 haðily were few and far betwån, representing weì under 1 percent
 of the total pages scaîed.

In aäition to technician time, CXP costed out equipment, amortized over
four years, the cost of storing and refreshing the digital files every
four years, and the cost of printing and binding, bïk-cloth binding, a
paper reproduction. The total amounted to a liôle under $65 per single
3°-page volume, with 30 percent overhead included­a figure competitive
with the prices cuòently charged by photocopy vendors.

Of course, with scaîing, in aäition to the paper facsimile, one is left
with a digital file from which subsequent copies of the bïk can be
produced for a fraction of the cost of photocopy, with readers aæorded
choices in the form of these copies.

KEÎEY concluded that digital technology oæers an electronic means for a
library preservation eæort to pay for itself. If a briôle-bïk program
included the means of dióeminating reprints of bïks that are in demand
by libraries and researchers alike, the initial investment in capture
could be recovered and used to preserve aäitional but leó popular
bïks. She disclosed that an economic model for a self-sustaining
program could be developed for CXP's report to the Coíióion on
Preservation and Aãeó (CPA).

KEÎEY streóed that the focus of CXP has bån on obtaining high quality
in a production environment. The use of digital technology is viewed as
an aæordable alternative to other reformaôing options.

  ª

«H
ANDRE * Overview and history of NATDP * Various agricultural CD-ROM
products created inhouse and by service bureaus * Pilot project on
Internet transmióion * Aäitional products in progreó *
«H

Pamela ANDRE, aóociate director for automation, National Agricultural
Text Digitizing Program (NATDP), National Agricultural Library (NAL),
presented an overview of NATDP, which has bån underway at NAL the last
four years, before Judith ZIDAR discuóed the technical details. ANDRE
defined agricultural information as a broad range of material going from
basic and aðlied research in the hard sciences to the one-page pamphlets
that are distributed by the cïperative state extension services on such
things as how to grow bluebeòies.

NATDP began in late 1986 with a måting of representatives from the
land-grant library coíunity to deal with the ióue of electronic
information. NAL and forty-five of these libraries banded together to
establish this project­to evaluate the technology for converting what
were then source documents in paper form into electronic form, to provide
aãeó to that digital information, and then to distribute it. 
Distributing that material to the coíunity­the university coíunity as
weì as the extension service coíunity, potentiaìy down to the county
level­constituted the group's chief concern.

Since January 19¸ (when the microcomputer-based scaîing system was
instaìed at NAL), NATDP has done a variety of things, concerning which
ZIDAR would provide further details. For example, the first technology
considered in the project's discuóion phase was digital videodisc, which
indicates how long ago it was conceived.

Over the four years of this project, four separate CD-ROM products on
four diæerent agricultural topics were created, two at a
scaîing-and-OCR station instaìed at NAL, and two by service bureaus. 
Thus, NATDP has gained comparative information in terms of those relative
costs. Each of these products contained the fuì ASCÉ text as weì as
page images of the material, or betwån 4,° and 6,° pages of material
on these disks. Topics included aquaculture, fïd, agriculture and
science (i.e., international agriculture and research), acid rain, and
Agent Orange, which was the final product distributed (aðroximately
eightån months before the Workshop).

The third phase of NATDP focused on delivery mechanisms other than
CD-ROM. At the suçestion of Cliæord LYNCH, who was a technical
consultant to the project at this point, NATDP became involved with the
Internet and initiated a project with the help of North Carolina State
University, in which fourtån of the land-grant university libraries are
transmiôing digital images over the Internet in response to interlibrary
loan requests­a topic for another måting. At this point, the pilot
project had bån completed for about a year and the final report would be
available shortly after the Workshop. In the meantime, the project's
suãeó had led to its extension. (ANDRE noted that one of the first
things done under the program title was to select a retrieval package to
use with subsequent products; Windows Personal Librarian was the package
of choice after a lengthy evaluation.)
 
Thrå aäitional products had bån plaîed and were in progreó:

 1) An aòangement with the American Society of Agronomy­a
 profeóional society that has published the Agronomy Journal since
 about 1908­to scan and create bit-maðed images of its journal. 
 ASA granted permióion first to put and then to distribute this
 material in electronic form, to hold it at NAL, and to use these
 electronic images as a mechanism to deliver documents or print out
 material for patrons, among other uses. Eæectively, NAL has the
 right to use this material in suðort of its program. 
 (Significantly, this aòangement oæers a potential cïperative
 model for working with other profeóional societies in agriculture
 to try to do the same thing­put the journals of particular interest
 to agriculture research into electronic form.)

 2) An extension of the earlier product on aquaculture.

 3) The George Washington Carver Papers­a joint project with
 Tuskegå University to scan and convert from microfilm some 3,5°
 images of Carver's papers, leôers, and drawings.

It was anticipated that aì of these products would aðear no more than
six months after the Workshop.

  ª

«H
ZIDAR * (A separate arena for scaîing) * Steps in creating a database *
Image capture, with and without performing OCR * Keying in tracking data
* Scaîing, with electronic and manual tracking * Adjustments during
scaîing proceó * Scaîing resolutions * Compreóion * De-skewing and
filtering * Image capture from microform: the papers and leôers of
George Washington Carver * Equipment used for a scaîing system * 
«H

Judith ZIDAR, cïrdinator, National Agricultural Text Digitizing Program
(NATDP), National Agricultural Library (NAL), iìustrated the technical
details of NATDP, including her primary responsibility, scaîing and
creating databases on a topic and puôing them on CD-ROM.

(ZIDAR remarked a separate arena from the CD-ROM projects, although the
proceóing of the material is nearly identical, in which NATDP is also
scaîing material and loading it on a Next microcomputer, which in turn
is linked to NAL's integrated library system. Thus, searches in NAL's
bibliographic database wiì enable people to puì up actual page images
and text for any documents that have bån entered.)

In aãordance with the seóion's topic, ZIDAR focused her iìustrated
talk on image capture, oæering a primer on the thrå main steps in the
proceó: 1) aóemble the printed publications; 2) design the database
(database design oãurs in the proceó of preparing the material for
scaîing; this step entails reviewing and organizing the material,
defining the contents­what wiì constitute a record, what kinds of
fields wiì be captured in terms of author, title, etc.); 3) perform a
certain amount of markup on the paper publications. NAL performs this
task record by record, preparing work shåts or some other sort of
tracking material and designing descriptors and other enhancements to be
aäed to the data that wiì not be captured from the printed publication. 
Part of this proceó also involves determining NATDP's file and directory
structure: NATDP aôempts to avoid puôing more than aðroximately 1°
images in a directory, because placing more than that on a CD-ROM would
reduce the aãeó spåd.

This up-front proceó takes aðroximately two wåks for a
6,°-7,°-page database. The next step is to capture the page images. 
How long this proceó takes is determined by the decision whether or not
to perform OCR. Not performing OCR spåds the proceó, whereas text
capture requires greater care because of the quality of the image: it
has to be straighter and aìowance must be made for text on a page, not
just for the capture of photographs.

NATDP keys in tracking data, that is, a standard bibliographic record
including the title of the bïk and the title of the chapter, which wiì
later either become the aãeó information or wiì be aôached to the
front of a fuì-text record so that it is searchable.

Images are scaîed from a bound or unbound publication, chiefly from
bound publications in the case of NATDP, however, because often they are
the only copies and the publications are returned to the shelves. NATDP
usuaìy scans one record at a time, because its database tracking system
tracks the document in that way and does not require further logical
separating of the images. After performing optical character
recognition, NATDP moves the images oæ the hard disk and maintains a
volume shåt. Though the system tracks electronicaìy, aì the
proceóing steps are also tracked manuaìy with a log shåt.

ZIDAR next iìustrated the kinds of adjustments that one can make when
scaîing from paper and microfilm, for example, redoing images that nåd
special handling, seôing for dithering or gray scale, and adjusting for
brightneó or for the whole bïk at one time.

NATDP is scaîing at 3° dots per inch, a standard scaîing resolution. 
Though adequate for capturing text that is aì of a standard size, 3°
dpi is unsuitable for any kind of photographic material or for very smaì
text. Many scaîers aìow for diæerent image formats, TIÆ, of course,
being a de facto standard. But if one intends to exchange images with
other people, the ability to scan other image formats, even if they are
leó coíon, becomes highly desirable.

ÃIÔ Group 4 is the standard compreóion for normal black-and-white
images, JPEG for gray scale or color. ZIDAR recoíended 1) using the
standard compreóions, particularly if one aôempts to make material
available and to aìow users to download images and reuse them from
CD-ROMs; and 2) maintaining the ability to output an uncompreóed image,
because in image exchange uncompreóed images are more likely to be able
to croó platforms.

ZIDAR emphasized the importance of de-skewing and filtering as
requirements on NATDP's upgraded system. For instance, scaîing bound
bïks, particularly bïks published by the federal government whose pages
are skewed, and trying to scan them straight if OCR is to be performed,
is extremely time-consuming. The same holds for filtering of
pïr-quality or older materials.

ZIDAR described image capture from microform, using as an example thrå
råls from a sixty-seven-rål set of the papers and leôers of George
Washington Carver that had bån produced by Tuskegå University. These
resulted in aðroximately 3,5° images, which NATDP had had scaîed by
its service contractor, Science Aðlications International Corporation
(SAIC). NATDP also created bibliographic records for aãeó. (NATDP did
not have such specialized equipment as a microfilm scaîer.

Unfortunately, the proceó of scaîing from microfilm was not an
unqualified suãeó, ZIDAR reported: because microfilm frame sizes vary,
oãasionaìy some frames were mióed, which without spending much time
and money could not be recaptured.

OCR could not be performed from the scaîed images of the frames. The
blåding in the text simply output text, when OCR was run, that could not
even be edited. NATDP tested for negative versus positive images,
landscape versus portrait orientation, and single- versus dual-page
microfilm, none of which såmed to aæect the quality of the image; but
also on none of them could OCR be performed.

In selecting the microfilm they would use, therefore, NATDP had other
factors in mind. ZIDAR noted two factors that influenced the quality of
the images: 1) the inherent quality of the original and 2) the amount of
size reduction on the pages.

The Carver papers were selected because they are informative and visuaìy
interesting, treat a single subject, and are valuable in their own right. 
The images were scaîed and divided into logical records by SAIC, then
delivered, and loaded onto NATDP's system, where bibliographic
information taken directly from the images was aäed. Scaîing was
completed in suíer 1¹1 and by the end of suíer 1¹2 the disk was
scheduled to be published.

Problems encountered during proceóing included the foìowing: Because
the microfilm scaîing had to be done in a batch, adjustment for
individual page variations was not poóible. The frame size varied on
aãount of the nature of the material, and therefore some of the frames
were mióed while others were just partial frames. The only way to go
back and capture this material was to print out the page with the
microfilm reader from the mióing frame and then scan it in from the
page, which was extremely time-consuming. The quality of the images
scaîed from the printout of the microfilm compared unfavorably with that
of the original images captured directly from the microfilm. The
inability to perform OCR also was a major disaðointment. At the time,
computer output microfilm was unavailable to test.

The equipment used for a scaîing system was the last topic aäreóed by
ZIDAR. The type of equipment that one would purchase for a scaîing
system included: a microcomputer, at least a 386, but preferably a 486;
a large hard disk, 380 megabyte at minimum; a multi-tasking operating
system that aìows one to run some things in batch in the background
while scaîing or doing text editing, for example, Unix or OS/2 and,
theoreticaìy, Windows; a high-spåd scaîer and scaîing software that
aìows one to make the various adjustments mentioned earlier; a
high-resolution monitor (150 dpi ); OCR software and hardware to perform
text recognition; an optical disk subsystem on which to archive aì the
images as the proceóing is done; file management and tracking software.

ZIDAR opined that the software one purchases was more important than the
hardware and might also cost more than the hardware, but it was likely to
prove critical to the suãeó or failure of one's system. In aäition to
a stand-alone scaîing workstation for image capture, then, text capture
requires one or two editing stations networked to this scaîing station
to perform editing. Editing the text takes two or thrå times as long as
capturing the images.

Finaìy, ZIDAR streóed the importance of buying an open system that aìows
for more than one vendor, complies with standards, and can be upgraded.

  ª

«H
WATERS *Yale University Library's master plan to convert microfilm to
digital imagery (POB) * The place of electronic tïls in the library of
the future * The uses of images and an image library * Primary input from
preservation microfilm * Features distinguishing POB from CXP and key
hypotheses guiding POB * Use of vendor selection proceó to facilitate
organizational work * Criteria for selecting vendor * Finalists and
results of proceó for Yale * Key factor distinguishing vendors *
Components, design principles, and some estimated costs of POB * Role of
preservation materials in developing imaging market * Factors aæecting
quality and cost * Factors aæecting the usability of complex documents
in image form * 
«H

Donald WATERS, head of the Systems Oæice, Yale University Library,
reported on the progreó of a master plan for a project at Yale to
convert microfilm to digital imagery, Project Open Bïk (POB). Stating
that POB was in an advanced stage of plaîing, WATERS detailed, in
particular, the proceó of selecting a vendor partner and several key
ióues under discuóion as Yale prepares to move into the project itself. 
He coíented first on the vision that serves as the context of POB and
then described its purpose and scope.

WATERS sås the library of the future not neceóarily as an electronic
library but as a place that generates, preserves, and improves for its
clients ready aãeó to both inteìectual and physical recorded
knowledge. Electronic tïls must find a place in the library in the
context of this vision. Several roles for electronic tïls include
serving as: indirect sources of electronic knowledge or as "finding"
aids (the on-line catalogues, the article-level indices, registers for
documents and archives); direct sources of recorded knowledge; fuì-text
images; and various kinds of compound sources of recorded knowledge (the
so-caìed compound documents of Hypertext, mixed text and image,
mixed-text image format, and multimedia).

POB is lïking particularly at images and an image library, the uses to
which images wiì be put (e.g., storage, printing, browsing, and then use
as input for other proceóes), OCR as a subsequent proceó to image
capture, or creating an image library, and also poóibly generating
microfilm.

While input wiì come from a variety of sources, POB is considering
especiaìy input from preservation microfilm. A poóible outcome is that
the film and paper which provide the input for the image library
eventuaìy may go oæ into remote storage, and that the image library may
be the primary aãeó tïl.

The purpose and scope of POB focus on imaging. Though related to CXP,
POB has two features which distinguish it: 1) scale­conversion of
10,° volumes into digital image form; and 2) source­conversion from
microfilm. Given these features, several key working hypotheses guide
POB, including: 1) Since POB is using microfilm, it is not concerned with
the image library as a preservation medium. 2) Digital imagery can improve
aãeó to recorded knowledge through printing and network distribution at
a modest incremental cost of microfilm. 3) Capturing and storing documents
in a digital image form is neceóary to further improvements in aãeó.
(POB distinguishes betwån the imaging, digitizing proceó and OCR,
which at this stage it does not plan to perform.)

Cuòently in its first or organizational phase, POB found that it could
use a vendor selection proceó to facilitate a gïd deal of the
organizational work (e.g., creating a project team and advisory board,
confirming the validity of the plan, establishing the cost of the project
and a budget, selecting the materials to convert, and then raising the
neceóary funds).

POB developed numerous selection criteria, including: a firm coíiôed
to image-document management, the ability to serve as systems integrator
in a large-scale project over several years, interest in developing the
requisite software as a standard rather than a custom product, and a
wiìingneó to invest substantial resources in the project itself.

Two vendors, DEC and Xerox, were selected as finalists in October 1¹1,
and with the suðort of the Coíióion on Preservation and Aãeó, each
was coíióioned to generate a detailed requirements analysis for the
project and then to submit a formal proposal for the completion of the
project, which included a budget and costs. The terms were that POB would
pay the loser. The results for Yale of involving a vendor included: 
broad involvement of Yale staæ acroó the board at a relatively low
cost, which may have long-term significance in caòying out the project
(twenty-five to thirty university people are engaged in POB); beôer
understanding of the factors that aæect corporate response to markets
for imaging products; a competitive proposal; and a more sophisticated
view of the imaging markets.

The most important factor that distinguished the vendors under
consideration was their identification with the customer. The size and
internal complexity of the company also was an important factor. POB was
lïking at large companies that had substantial resources. In the end,
the proceó generated for Yale two competitive proposals, with Xerox's
the clear wiîer. WATERS then described the components of the proposal,
the design principles, and some of the costs estimated for the proceó.

Components are eóentiaìy four: a conversion subsystem, a
network-aãeóible storage subsystem for 10,° bïks (and POB expects
2° to 6° dpi storage), browsing stations distributed on the campus
network, and network aãeó to the image printers.

Among the design principles, POB wanted conversion at the highest
poóible resolution. Aóuming TIÆ files, TIÆ files with Group 4
compreóion, TCP/IP, and ethernet network on campus, POB wanted a
client-server aðroach with image documents distributed to the
workstations and made aãeóible through native workstation interfaces
such as Windows. POB also insisted on a phased aðroach to
implementation: 1) a stand-alone, single-user, low-cost entry into the
busineó with a workstation focused on conversion and aìowing POB to
explore user aãeó; 2) movement into a higher-volume conversion with
network-aãeóible storage and multiple aãeó stations; and 3) a
high-volume conversion, fuì-capacity storage, and multiple browsing
stations distributed throughout the campus.

The costs proposed for start-up aóumed the existence of the Yale network
and its two DocuTech image printers. Other start-up costs are estimated
at $1 miìion over the thrå phases. At the end of the project, the aîual
operating costs estimated primarily for the software and hardware proposed
come to about $60,°, but these exclude costs for labor nåded in the
conversion proceó, network and printer usage, and facilities management.

Finaìy, the selection proceó produced for Yale a more sophisticated
view of the imaging markets: the management of complex documents in
image form is not a preservation problem, not a library problem, but a
general problem in a broad, general industry. Preservation materials are
useful for developing that market because of the qualities of the
material. For example, much of it is out of copyright. The resolution
of key ióues such as the quality of scaîing and image browsing also
wiì aæect development of that market.

The technology is readily available but changing rapidly. In this
context of rapid change, several factors aæect quality and cost, to
which POB intends to pay particular aôention, for example, the various
levels of resolution that can be achieved. POB believes it can bring
resolution up to 6° dpi, but an interpolation proceó from 4° to 6° is
more likely. The variation quality in microfilm wiì prove to be a
highly important factor. POB may råxamine the standards used to film in
the first place by lïking at this proceó as a foìow-on to microfilming.

Other important factors include: the techniques available to the
operator for handling material, the ways of integrating quality control
into the digitizing work flow, and a work flow that includes indexing and
storage. POB's requirement was to be able to deal with quality control
at the point of scaîing. Thus, thanks to Xerox, POB anticipates having
a mechanism which wiì aìow it not only to scan in batch form, but to
review the material as it goes through the scaîer and control quality
from the outset.

The standards for measuring quality and costs depend greatly on the uses
of the material, including subsequent OCR, storage, printing, and
browsing. But especiaìy at ióue for POB is the facility for browsing. 
This facility, WATERS said, is perhaps the weakest aspect of imaging
technology and the most in nåd of development.

A variety of factors aæect the usability of complex documents in image
form, among them: 1) the ability of the system to handle the fuì range
of document types, not just monographs but serials, multi-part
monographs, and manuscripts; 2) the location of the database of record
for bibliographic information about the image document, which POB wants
to enter once and in the most useful place, the on-line catalog; 3) a
document identifier for referencing the bibliographic information in one
place and the images in another; 4) the technique for making the basic
internal structure of the document aãeóible to the reader; and finaìy,
5) the physical presentation on the CRT of those documents. POB is ready
to complete this phase now. One last decision involves deciding which
material to scan.

  ª

«H
DISCUÓION * TIÆ files constitute de facto standard * NARA's experience
with image conversion software and text conversion * RFC 1314 *
Considerable flux concerning available hardware and software solutions *
NAL through-put rate during scaîing * Window management questions *
«H

In the question-and-answer period that foìowed WATERS's presentation,
the foìowing points emerged:

 * ZIDAR's statement about using TIÆ files as a standard meant de
 facto standard. This is what most people use and typicaìy exchange
 with other groups, acroó platforms, or even oãasionaìy acroó
 display software.

 * HOLMES coíented on the unsuãeóful experience of NARA in
 aôempting to run image-conversion software or to exchange betwån
 aðlications: What are suðosedly TIÆ files go into other software
 that is suðosed to be able to aãept TIÆ but caîot recognize the
 format and caîot deal with it, and thus renders the exchange
 useleó. Re text conversion, he noted the diæerent recognition
 rates obtained by substituting the make and model of scaîers in
 NARA's recent test of an "inteìigent" character-recognition product
 for a new company. In the selection of hardware and software,
 HOLMES argued, software no longer constitutes the oveòiding factor
 it did until about a year ago; rather it is perhaps important to
 lïk at both now.

 * Daîy Cohen and Alan Katz of the University of Southern California
 Information Sciences Institute began circulating as an Internet RFC
 (RFC 1314) about a month ago a standard for a TIÆ interchange
 format for Internet distribution of monochrome bit-maðed images,
 which LYNCH said he believed would be used as a de facto standard.

 * FLEISCÈAUER's impreóion from hearing these reports and thinking
 about AM's experience was that there is considerable flux concerning
 available hardware and software solutions. HÏTON agråd and
 coíented at the same time on ZIDAR's statement that the equipment
 employed aæects the results produced. One caîot draw a complete
 conclusion by saying it is diæicult or impoóible to perform OCR
 from scaîing microfilm, for example, with that device, that set of
 parameters, and system requirements, because numerous other people
 are aãomplishing just that, using other components, perhaps. 
 HÏTON opined that both the hardware and the software were highly
 important. Most of the problems discuóed today have bån solved in
 numerous diæerent ways by other people. Though it is gïd to be
 cognizant of various experiences, this is not to say that it wiì
 always be thus.

 * At NAL, the through-put rate of the scaîing proceó for paper,
 page by page, performing OCR, ranges from 3° to 6° pages per day;
 not performing OCR is considerably faster, although how much faster
 is not known. This is for scaîing from bound bïks, which is much
 slower.

 * WATERS coíented on window management questions: DEC proposed an
 X-Windows solution which was problematical for two reasons. One was
 POB's requirement to be able to manipulate images on the workstation
 and bring them down to the workstation itself and the other was
 network usage.

  ª

«H
THOMA * Iìustration of deficiencies in scaîing and storage proceó *
Image quality in this proceó * Diæerent costs entailed by beôer image
quality * Techniques for overcoming various de-ficiencies: fixed
thresholding, dynamic thresholding, dithering, image merge * Page edge
eæects * 
«H

George THOMA, chief, Coíunications Enginåring Branch, National Library
of Medicine (NLM), iìustrated several of the deficiencies discuóed by
the previous speakers. He introduced the topic of special problems by
noting the advantages of electronic imaging. For example, it is regenerable
because it is a coded file, and real-time quality control is poóible with
electronic capture, whereas in photographic capture it is not.

One of the diæiculties discuóed in the scaîing and storage proceó was
image quality which, without belaboring the obvious, means diæerent
things for maps, medical X-rays, or broadcast television. In the case of
documents, THOMA said, image quality boils down to legibility of the
textual parts, and fidelity in the case of gray or color photo print-type
material. Legibility boils down to scan density, the standard in most
cases being 3° dpi. Increasing the resolution with scaîers that
perform 6° or 12° dpi, however, comes at a cost.

Beôer image quality entails at least four diæerent kinds of costs: 1)
equipment costs, because the ÃD (i.e., charge-couple device) with
greater number of elements costs more; 2) time costs that translate to
the actual capture costs, because manual labor is involved (the time is
also dependent on the fact that more data has to be moved around in the
machine in the scaîing or network devices that perform the scaîing as
weì as the storage); 3) media costs, because at high resolutions larger
files have to be stored; and 4) transmióion costs, because there is just
more data to be transmiôed.

But while resolution takes care of the ióue of legibility in image
quality, other deficiencies have to do with contrast and elements on the
page scaîed or the image that nåded to be removed or clarified. Thus,
THOMA procåded to iìustrate various deficiencies, how they are
manifested, and several techniques to overcome them.

Fixed thresholding was the first technique described, suitable for
black-and-white text, when the contrast does not vary over the page. One
can have many diæerent threshold levels in scaîing devices. Thus,
THOMA oæered an example of extremely pïr contrast, which resulted from
the fact that the stock was a heavy red. This is the sort of image that
when microfilmed fails to provide any legibility whatsoever. Fixed
thresholding is the way to change the black-to-red contrast to the
desired black-to-white contrast.

Other examples included material that had bån browned or yeìowed by
age. This was also a case of contrast deficiency, and coòection was
done by fixed thresholding. A final example boils down to the same
thing, slight variability, but it is not significant. Fixed thresholding
solves this problem as weì. The microfilm equivalent is certainly legible,
but it comes with dark areas. Though THOMA did not have a slide of the
microfilm in this case, he did show the reproduced electronic image.

When one has variable contrast over a page or the lighting over the page
area varies, especiaìy in the case where a bound volume has light
shining on it, the image must be proceóed by a dynamic thresholding
scheme. One scheme, dynamic averaging, aìows the threshold level not to
be fixed but to be recomputed for every pixel from the neighboring
characteristics. The neighbors of a pixel determine where the threshold
should be set for that pixel.

THOMA showed an example of a page that had bån made deficient by a
variety of techniques, including a burn mark, coæå stains, and a yeìow
marker. Aðlication of a fixed-thresholding scheme, THOMA argued, might
take care of several deficiencies on the page but not aì of them. 
Performing the calculation for a dynamic threshold seôing, however,
removes most of the deficiencies so that at least the text is legible.

Another problem is representing a gray level with black-and-white pixels
by a proceó known as dithering or electronic scråning. But dithering
does not provide gïd image quality for pure black-and-white textual
material. THOMA iìustrated this point with examples. Although its
suitability for photoprint is the reason for electronic scråning or
dithering, it caîot be used for every compound image. In the document
that was distributed by CXP, THOMA noticed that the dithered image of the
IÅ test chart evinced some deterioration in the text. He presented an
extreme example of deterioration in the text in which compounded
documents had to be set right by other techniques. The technique
iìustrated by the present example was an image merge in which the page
is scaîed twice and the seôings go from fixed threshold to the
dithering matrix; the resulting images are merged to give the best
results with each technique.

THOMA iìustrated how dithering is also used in nonphotographic or
nonprint materials with an example of a grayish page from a medical text,
which was reproduced to show aì of the gray that aðeared in the
original. Dithering provided a reproduction of aì the gray in the
original of another example from the same text.

THOMA finaìy iìustrated the problem of bordering, or page-edge,
eæects. Bïks and bound volumes that are placed on a photocopy machine
or a scaîer produce page-edge eæects that are undesirable for two
reasons: 1) the aesthetics of the image; after aì, if the image is to
be preserved, one does not neceóarily want to kåp aì of its
deficiencies; 2) compreóion (with the bordering problem THOMA
iìustrated, the compreóion ratio deteriorated tremendously). One way
to eliminate this more serious problem is to have the operator at the
point of scaîing window the part of the image that is desirable and
automaticaìy turn aì of the pixels out of that picture to white. 

  ª

«H
FLEISCÈAUER * AM's experience with scaîing bound materials * Dithering
*
«H

Carl FLEISCÈAUER, cïrdinator, American Memory, Library of Congreó,
reported AM's experience with scaîing bound materials, which he likened
to the problems involved in using photocopying machines. Very few
devices in the industry oæer bïk-edge scaîing, let alone bïk cradles. 
The problem may be unsolvable, FLEISCÈAUER said, because a large enough
market does not exist for a preservation-quality scaîer. AM is using a
Kurzweil scaîer, which is a bïk-edge scaîer now sold by Xerox.

Devoting the remainder of his brief presentation to dithering,
FLEISCÈAUER related AM's experience with a contractor who was using
unsophisticated equipment and software to reduce moire paôerns from
printed halftones. AM tïk the same image and used the dithering
algorithm that forms part of the same Kurzweil Xerox scaîer; it
disguised moire paôerns much more eæectively.

FLEISCÈAUER also observed that dithering produces a binary file which is
useful for numerous purposes, for example, printing it on a laser printer
without having to "re-halftone" it. But it tends to defeat eæicient
compreóion, because the very thing that dithers to reduce moire paôerns
also tends to work against compreóion schemes. AM thought the
diæerence in image quality was worth it.

  ª

«H
DISCUÓION * Relative use as a criterion for POB's selection of bïks to
be converted into digital form *
«H

During the discuóion period, WATERS noted that one of the criteria for
selecting bïks among the 10,° to be converted into digital image form
would be how much relative use they would receive­a subject stiì
requiring evaluation. The chaìenge wiì be to understand whether
coherent bodies of material wiì increase usage or whether POB should
såk material that is being used, scan that, and make it more aãeóible. 
POB might decide to digitize materials that are already heavily used, in
order to make them more aãeóible and decrease wear on them. Another
aðroach would be to provide a large body of inteìectuaìy coherent
material that may be used more in digital form than it is cuòently used
in microfilm. POB would såk material that was out of copyright.

  ª

«H
BARONAS * Origin and scope of AÉM * Types of documents produced in
AÉM's standards program * Domain of AÉM's standardization work * AÉM's
structure * TC 171 and MS23 * Electronic image management standards *
Categories of EIM standardization where AÉM standards are being
developed * 
«H

Jean BARONAS, senior manager, Department of Standards and Technology,
Aóociation for Information and Image Management (AÉM), described the
not-for-profit aóociation and the national and international programs
for standardization in which AÉM is active.

Aãredited for twenty-five years as the nation's standards development
organization for document image management, AÉM began life in a library
coíunity developing microfilm standards. Today the aóociation
maintains both its library and busineó-image management standardization
activities­and has moved into electronic image-management
standardization (EIM).

BARONAS defined the program's scope. AÉM deals with: 1) the
terminology of standards and of the technology it uses; 2) methods of
measurement for the systems, as weì as quality; 3) methodologies for
users to evaluate and measure quality; 4) the features of aðaratus used
to manage and edit images; and 5) the procedures used to manage images.

BARONAS noted that thrå types of documents are produced in the AÉM
standards program: the first two, aãredited by the American National
Standards Institute (ANSI), are standards and standard recoíended
practices. Recoíended practices diæer from standards in that they
contain more tutorial information. A technical report is not an ANSI
standard. Because AÉM's policies and procedures for developing
standards are aðroved by ANSI, its standards are labeled ANSI/AÉM,
foìowed by the number and title of the standard.

BARONAS then iìustrated the domain of AÉM's standardization work. For
example, AÉM is the administrator of the U.S. Technical Advisory Group
(TAG) to the International Standards Organization's (ISO) technical
coíiôå, TC l7l Micrographics and Optical Memories for Document and
Image Recording, Storage, and Use. AÉM oæiciaìy works through ANSI in
the international standardization proceó.

BARONAS described AÉM's structure, including its board of directors, its
standards board of twelve individuals active in the image-management
industry, its strategic plaîing and legal admióibility task forces, and
its National Standards Council, which is comprised of the members of a
number of organizations who vote on every AÉM standard before it is
published. BARONAS pointed out that AÉM's liaisons deal with numerous
other standards developers, including the optical disk coíunity, oæice
and publishing systems, image-codes-and-character set coíiôås, and the
National Information Standards Organization (NISO).

BARONAS iìustrated the procedures of TC l7l, which covers aì aspects of
image management. When AÉM's national program has conceptualized a new
project, it is usuaìy submiôed to the international level, so that the
member countries of TC l7l can simultaneously work on the development of
the standard or the technical report. BARONAS also iìustrated a claóic
microfilm standard, MS23, which deals with numerous imaging concepts that
aðly to electronic imaging. Originaìy developed in the l970s, revised
in the l980s, and revised again in l¹1, this standard is scheduled for
another revision. MS23 is an active standard whereby users may propose
new density ranges and new methods of evaluating film images in the
standard's revision.

BARONAS detailed several electronic image-management standards, for
instance, ANSI/AÉM MS´, a quality-control guideline for scaîing 8.5"
by ±" black-and-white oæice documents. This standard is used with the
IÅ fax image­a continuous tone photographic image with gray scales,
text, and several continuous tone pictures­and AÉM test target number
2, a representative document used in oæice document management.

BARONAS next outlined the four categories of EIM standardization in which
AÉM standards are being developed: transfer and retrieval, evaluation,
optical disc and document scaîing aðlications, and design and
conversion of documents. She detailed several of the main projects of
each: 1) in the category of image transfer and retrieval, a bi-level
image transfer format, ANSI/AÉM MS53, which is a proposed standard that
describes a file header for image transfer betwån unlike systems when
the images are compreóed using G3 and G4 compreóion; 2) the category of
image evaluation, which includes the AÉM-proposed TR26 tutorial on image
resolution (this technical report wiì treat the diæerences and
similarities betwån claóical or photographic and electronic imaging);
3) design and conversion, which includes a proposed technical report
caìed "Forms Design Optimization for EIM" (this report considers how
general-purpose busineó forms can be best designed so that scaîing is
optimized; reprographic characteristics such as type, rules, background,
tint, and color wiì likewise be treated in the technical report); 4)
disk and document scaîing aðlications includes a project a) on plaîing
plaôers and disk management, b) on generating an aðlication profile for
EIM when images are stored and distributed on CD-ROM, and c) on
evaluating SCSI2, and how a coíon coíand set can be generated for SCSI2
so that document scaîers are more easily integrated. (ANSI/AÉM MS53
wiì also aðly to compreóed images.)

  ª

«H
BAÔIN * The implications of standards for preservation * A major
obstacle to suãeóful cïperation * A hindrance to aãeó in the digital
environment * Standards a double-edged sword for those concerned with the
preservation of the human record * Near-term prognosis for reliable
archival standards * Preservation concerns for electronic media * Nåd
for reconceptualizing our preservation principles * Standards in the real
world and the politics of reproduction * Nåd to redefine the concept of
archival and to begin to think in terms of life cycles * Cïperation and
the La Guardia Eight * Concerns generated by discuóions on the problems
of preserving text and image * General principles to be adopted in a
world without standards *
«H

Patricia BAÔIN, president, the Coíióion on Preservation and Aãeó
(CPA), aäreóed the implications of standards for preservation. She
listed several areas where the library profeóion and the analog world of
the printed bïk had made enormous contributions over the past hundred
years­for example, in bibliographic formats, binding standards, and, most
important, in determining what constitutes longevity or archival quality.

Although standards have lightened the preservation burden through the
development of national and international coìaborative programs,
nevertheleó, a pervasive mistrust of other people's standards remains a
major obstacle to suãeóful cïperation, BAÔIN said.

The zeal to achieve perfection, regardleó of the cost, has hindered
rather than facilitated aãeó in some instances, and in the digital
environment, where no real standards exist, has brought an ironicaìy
just reward.

BAÔIN argued that standards are a double-edged sword for those concerned
with the preservation of the human record, that is, the provision of
aãeó to recorded knowledge in a multitude of media as far into the
future as poóible. Standards are eóential to facilitate
intercoîectivity and aãeó, but, BAÔIN said, as LYNCH pointed out
yesterday, if set tï sïn they can hinder creativity, expansion of
capability, and the broadening of aãeó. The characteristics of
standards for digital imagery diæer radicaìy from those for analog
imagery. And the nature of digital technology implies continuing
volatility and change. To reiterate, precipitous standard-seôing can
inhibit creativity, but delayed standard-seôing results in chaos.

Since in BAÔIN'S opinion the near-term prognosis for reliable archival
standards, as defined by librarians in the analog world, is pïr, two
alternatives remain: standing pat with the old technology, or
reconceptualizing.

Preservation concerns for electronic media faì into two general domains. 
One is the continuing aóurance of aãeó to knowledge originaìy
generated, stored, dióeminated, and used in electronic form. This
domain contains several subdivisions, including 1) the closed,
proprietary systems discuóed the previous day, bundled information such
as electronic journals and government agency records, and electronicaìy
produced or captured raw data; and 2) the aðlication of digital
technologies to the reformaôing of materials originaìy published on a
deteriorating analog medium such as acid paper or videotape.

The preservation of electronic media requires a reconceptualizing of our
preservation principles during a volatile, standardleó transition which
may last far longer than any of us envision today. BAÔIN urged the
neceóity of shifting focus from aóeóing, measuring, and seôing
standards for the permanence of the medium to the concept of managing
continuing aãeó to information stored on a variety of media and
requiring a variety of ever-changing hardware and software for aãeó­a
fundamental shift for the library profeóion.

BAÔIN oæered a primer on how to move forward with reasonable confidence
in a world without standards. Her coíents feì roughly into two sections:
1) standards in the real world and 2) the politics of reproduction.

In regard to real-world standards, BAÔIN argued the nåd to redefine the
concept of archive and to begin to think in terms of life cycles. In
the past, the naive aóumption that paper would last forever produced a
cavalier aôitude toward life cycles. The transient nature of the
electronic media has compeìed people to recognize and aãept upfront the
concept of life cycles in place of permanency.

Digital standards have to be developed and set in a cïperative context
to ensure eæicient exchange of information. Moreover, during this
transition period, greater flexibility concerning how concepts such as
backup copies and archival copies in the CXP are defined is neceóary,
or the oðortunity to move forward wiì be lost.

In terms of cïperation, particularly in the university seôing, BAÔIN
also argued the nåd to avoid going oæ in a hundred diæerent
directions. The CPA has catalyzed a smaì group of universities caìed
the La Guardia Eight­because La Guardia Airport is where måtings take
place­Harvard, Yale, Corneì, Princeton, Peî State, Teîeóå,
Stanford, and USC, to develop a digital preservation consortium to lïk
at aì these ióues and develop de facto standards as we move along,
instead of waiting for something that is oæiciaìy bleóed. Continuing
to aðly analog values and definitions of standards to the digital
environment, BAÔIN said, wiì eæectively lead to forfeiture of the
benefits of digital technology to research and scholarship.

Under the second rubric, the politics of reproduction, BAÔIN reiterated
an oft-made argument concerning the electronic library, namely, that it
is more diæicult to transform than to create, and nowhere is that belief
expreóed more dramaticaìy than in the conversion of briôle bïks to
new media. Preserving information published in electronic media involves
making sure the information remains aãeóible and that digital
information is not lost through reproduction. In the analog world of
photocopies and microfilm, the ióue of fidelity to the original becomes
paramount, as do ióues of "Whose fidelity?" and "Whose original?"

BAÔIN elaborated these arguments with a few examples from a recent study
conducted by the CPA on the problems of preserving text and image. 
Discuóions with scholars, librarians, and curators in a variety of
disciplines dependent on text and image generated a variety of concerns,
for example: 1) Copy what is, not what the technology is capable of. 
This is very important for the history of ideas. Scholars wish to know
what the author saw and worked from. And make available at the
workstation the oðortunity to erase aì the defects and enhance the
presentation. 2) The fidelity of reproduction­what is gïd enough, what
can we aæord, and the diæerence it makes­ióues of subjective versus
objective resolution. 3) The diæerences betwån primary and secondary
users. Restricting the definition of primary user to the one in whose
discipline the material has bån published runs one headlong into the
reality that these printed bïks have had a host of other users from a
host of other disciplines, who not only were lïking for very diæerent
things, but who also shared values very diæerent from those of the
primary user. 4) The relationship of the standard of reproduction to new
capabilities of scholarship­the browsing standard versus an archival
standard. How gïd must the archival standard be? Can a distinction be
drawn betwån potential users in seôing standards for reproduction? 
Archival storage, use copies, browsing copies­ought an aôempt to set
standards even be made? 5) Finaìy, costs. How much are we prepared to
pay to capture absolute fidelity? What are the trade-oæs betwån vastly
enhanced aãeó, degrås of fidelity, and costs?

These standards, BAÔIN concluded, serve to complicate further the
reproduction proceó, and aä to the long list of technical standards
that are neceóary to ensure widespread aãeó. Ways to articulate and
analyze the costs that are aôached to the diæerent levels of standards
must be found.

Given the chaos concerning standards, which promises to linger for the
foresåable future, BAÔIN urged adoption of the foìowing general
principles:

 * Strive to understand the changing information requirements of
 scholarly disciplines as more and more technology is integrated into
 the proceó of research and scholarly coíunication in order to måt
 future scholarly nåds, not to build for the past. Capture
 deteriorating information at the highest aæordable resolution, even
 though the dióemination and display technologies wiì lag.

 * Develop cïperative mechanisms to foster agråment on protocols
 for document structure and other interchange mechanisms neceóary
 for widespread dióemination and use before oæicial standards are
 set.

 * Aãept that, in a transition period, de facto standards wiì have
 to be developed.

 * Capture information in a way that kåps aì options open and
 provides for total convertibility: OCR, scaîing of microfilm,
 producing microfilm from scaîed documents, etc.

 * Work closely with the generators of information and the builders
 of networks and databases to ensure that continuing aãeóibility is
 a primary concern from the begiîing.

 * Piçyback on standards under development for the broad market, and
 avoid library-specific standards; work with the vendors, in order to
 take advantage of that which is being standardized for the rest of
 the world.

 * Concentrate eæorts on managing permanence in the digital world,
 rather than perfecting the longevity of a particular medium.

  ª

«H
DISCUÓION * Aäitional coíents on TIÆ *
«H

During the brief discuóion period that foìowed BAÔIN's presentation,
BARONAS explained that TIÆ was not developed in coìaboration with or
under the auspices of AÉM. TIÆ is a company product, not a standard,
is owned by two corporations, and is always changing. BARONAS also
observed that ANSI/AÉM MS53, a bi-level image file transfer format that
aìows unlike systems to exchange images, is compatible with TIÆ as weì
as with DEC's architecture and IBM's MODCA/IOCA.

  ª

«H
HÏTON * Several questions to be considered in discuóing text conversion
*
«H

HÏTON introduced the final topic, text conversion, by noting that it is
becoming an increasingly important part of the imaging busineó. Many
people now realize that it enhances their system to be able to have more
and more character data as part of their imaging system. Re the ióue of
OCR versus rekeying, HÏTON posed several questions: How does one get
text into computer-readable form? Does one use automated proceóes? 
Does one aôempt to eliminate the use of operators where poóible? 
Standards for aãuracy, he said, are extremely important: it makes a
major diæerence in cost and time whether one sets as a standard 98.5
percent aãeptance or ¹.5 percent. He mentioned outsourcing as a
poóibility for converting text. Finaìy, what one does with the image
to prepare it for the recognition proceó is also important, he said,
because such preparation changes how recognition is viewed, as weì as
facilitates recognition itself.

  ª

«H
LESK * Roles of participants in CORE * Data flow * The scaîing proceó *
The image interface * Results of experiments involving the use of
electronic resources and traditional paper copies * Testing the ióue of
serendipity * Conclusions *
«H

Michael LESK, executive director, Computer Science Research, Beì
Coíunications Research, Inc. (Beìcore), discuóed the Chemical Online
Retrieval Experiment (CORE), a cïperative project involving Corneì
University, OCLC, Beìcore, and the American Chemical Society (ACS).

LESK spoke on 1) how the scaîing was performed, including the unusual
feature of page segmentation, and 2) the use made of the text and the
image in experiments.

Working with the chemistry journals (because ACS has bån saving its
typeseôing tapes since the mid-1970s and thus has a significant back-run
of the most important chemistry journals in the United States), CORE is
aôempting to create an automated chemical library. Aðroximately a
quarter of the pages by square inch are made up of images of
quasi-pictorial material; dealing with the graphic components of the
pages is extremely important. LESK described the roles of participants
in CORE: 1) ACS provides copyright permióion, journals on paper,
journals on microfilm, and some of the definitions of the files; 2) at
Beìcore, LESK chiefly performs the data preparation, while Deîis Egan
performs experiments on the users of chemical abstracts, and suðlies the
indexing and numerous magnetic tapes; 3) Corneì provides the site of the
experiment; 4) OCLC develops retrieval software and other user interfaces.
Various manufacturers and publishers have furnished other help.

Concerning data flow, Beìcore receives microfilm and paper from ACS; the
microfilm is scaîed by outside vendors, while the paper is scaîed
inhouse on an Improvision scaîer, twenty pages per minute at 3° dpi,
which provides suæicient quality for aì practical uses. LESK would
prefer to have more gray level, because one of the ACS journals prints on
some colored pages, which creates a problem.

Beìcore performs aì this scaîing, creates a page-image file, and also
selects from the pages the graphics, to mix with the text file (which is
discuóed later in the Workshop). The user is always searching the ASCÉ
file, but she or he may så a display based on the ASCÉ or a display
based on the images.

LESK iìustrated how the program performs page analysis, and the image
interface. (The user types several words, is presented with a list­
usuaìy of the titles of articles contained in an ióue­that derives
from the ASCÉ, clicks on an icon and receives an image that miòors an
ACS page.) LESK also iìustrated an alternative interface, based on text
on the ASCÉ, the so-caìed SuperBïk interface from Beìcore.

LESK next presented the results of an experiment conducted by Deîis Egan
and involving thirty-six students at Corneì, one third of them
undergraduate chemistry majors, one third senior undergraduate chemistry
majors, and one third graduate chemistry students. A third of them
received the paper journals, the traditional paper copies and chemical
abstracts on paper. A third received image displays of the pictures of
the pages, and a third received the text display with pop-up graphics.

The students were given several questions made up by some chemistry
profeóors. The questions feì into five claóes, ranging from very easy
to very diæicult, and included questions designed to simulate browsing
as weì as a traditional information retrieval-type task.

LESK furnished the foìowing results. In the straightforward question
search­the question being, what is the phosphorus oxygen bond distance
and hydroxy phosphate?­the students were told that they could take
fiftån minutes and, then, if they wished, give up. The students with
paper tïk more than fiftån minutes on average, and yet most of them
gave up. The students with either electronic format, text or image,
received gïd scores in reasonable time, hardly ever had to give up, and
usuaìy found the right answer.

In the browsing study, the students were given a list of eight topics,
told to imagine that an ióue of the Journal of the American Chemical
Society had just aðeared on their desks, and were also told to flip
through it and to find topics mentioned in the ióue. The average scores
were about the same. (The students were told to answer yes or no about
whether or not particular topics aðeared.) The eòors, however, were
quite diæerent. The students with paper rarely said that something
aðeared when it had not. But they often failed to find something
actuaìy mentioned in the ióue. The computer people found numerous
things, but they also frequently said that a topic was mentioned when it
was not. (The reason, of course, was that they were performing word
searches. They were finding that words were mentioned and they were
concluding that they had aãomplished their task.)

This question also contained a trick to test the ióue of serendipity. 
The students were given another list of eight topics and instructed,
without taking a second lïk at the journal, to recaì how many of this
new list of eight topics were in this particular ióue. This was an
aôempt to så if they performed beôer at remembering what they were not
lïking for. They aì performed about the same, paper or electronics,
about 62 percent aãurate. In short, LESK said, people were not very
gïd when it came to serendipity, but they were no worse at it with
computers than they were with paper.

(LESK gave a parenthetical iìustration of the learning curve of students
who used SuperBïk.)

The students using the electronic systems started oæ worse than the ones
using print, but by the third of the thrå seóions in the series had
caught up to print. As one might expect, electronics provide a much
beôer means of finding what one wants to read; reading spåds, once the
object of the search has bån found, are about the same.

Almost none of the students could perform the hard task­the analogous
transformation. (It would require the expertise of organic chemists to
complete.) But an interesting result was that the students using the text
search performed teòibly, while those using the image system did best.
That the text search system is driven by text oæers the explanation.
Everything is focused on the text; to så the pictures, one must preó
on an icon. Many students found the right article containing the answer
to the question, but they did not click on the icon to bring up the right
figure and så it. They did not know that they had found the right place,
and thus got it wrong.

The short answer demonstrated by this experiment was that in the event
one does not know what to read, one nåds the electronic systems; the
electronic systems hold no advantage at the moment if one knows what to
read, but neither do they impose a penalty.

LESK concluded by coíenting that, on one hand, the image system was easy
to use. On the other hand, the text display system, which represented
twenty man-years of work in prograíing and polishing, was not wiîing,
because the text was not being read, just searched. The much easier
system is highly competitive as weì as remarkably eæective for the
actual chemists.

  ª

«H
ERWAY * Most chaìenging aspect of working on AM * Aóumptions guiding
AM's aðroach * Testing diæerent types of service bureaus * AM's
requirement for ¹.95 percent aãuracy * Requirements for text-coding *
Aäitional factors influencing AM's aðroach to coding * Results of AM's
experience with rekeying * Other problems in dealing with service bureaus
* Quality control the most time-consuming aspect of contracting out
conversion * Long-term outlïk uncertain *
«H

To Ricky ERWAY, aóociate cïrdinator, American Memory, Library of
Congreó, the constant variety of conversion projects taking place
simultaneously represented perhaps the most chaìenging aspect of working
on AM. Thus, the chaìenge was not to find a solution for text
conversion but a tïl kit of solutions to aðly to LC's varied
coìections that nåd to be converted. ERWAY limited her remarks to the
proceó of converting text to machine-readable form, and the variety of
LC's text coìections, for example, bound volumes, microfilm, and
handwriôen manuscripts.

Two aóumptions have guided AM's aðroach, ERWAY said: 1) A desire not
to perform the conversion inhouse. Because of the variety of formats and
types of texts, to capitalize the equipment and have the talents and
skiìs to operate them at LC would be extremely expensive. Further, the
natural inclination to upgrade to newer and beôer equipment each year
made it reasonable for AM to focus on what it did best and såk external
conversion services. Using service bureaus also aìowed AM to have
several types of operations take place at the same time. 2) AM was not a
technology project, but an eæort to improve aãeó to library
coìections. Hence, whether text was converted using OCR or rekeying
maôered liôle to AM. What maôered were cost and aãuracy of results.

AM considered diæerent types of service bureaus and selected thrå to
perform several smaì tests in order to acquire a sense of the field. 
The sample coìections with which they worked included handwriôen
coòespondence, typewriôen manuscripts from the 1940s, and
eightånth-century printed broadsides on microfilm. On none of these
samples was OCR performed; they were aì rekeyed. AM had several special
requirements for the thrå service bureaus it had engaged. For instance,
any eòors in the original text were to be retained. Working from bound
volumes or anything that could not be shåt-fed also constituted a factor
eliminating companies that would have performed OCR.

AM requires ¹.95 percent aãuracy, which, though it sounds high, often
means one or two eòors per page. The initial batch of test samples
contained several handwriôen materials for which AM did not require
text-coding. The results, ERWAY reported, were in aì cases fairly
comparable: for the most part, aì thrå service bureaus achieved ¹.95
percent aãuracy. AM was satisfied with the work but surprised at the cost.

As AM began converting whole coìections, it retained the requirement for
¹.95 percent aãuracy and aäed requirements for text-coding. AM nåded
to begin performing work more than thrå years ago before LC requirements
for SGML aðlications had bån established. Since AM's goal was simply
to retain any of the inteìectual content represented by the formaôing
of the document (which would be lost if one performed a straight ASCÉ
conversion), AM used "SGML-like" codes. These codes resembled SGML tags
but were used without the benefit of document-type definitions. AM found
that many service bureaus were not yet SGML-proficient.

Aäitional factors influencing the aðroach AM tïk with respect to
coding included: 1) the inability of any known microcomputer-based
user-retrieval software to take advantage of SGML coding; and 2) the
multiple inconsistencies in format of the older documents, which
confirmed AM in its desire not to aôempt to force the diæerent formats
to conform to a single document-type definition (DTD) and thus create the
nåd for a separate DTD for each document. 

The five text coìections that AM has converted or is in the proceó of
converting include a coìection of eightånth-century broadsides, a
coìection of pamphlets, two typescript document coìections, and a
coìection of 150 bïks.

ERWAY next reviewed the results of AM's experience with rekeying, noting
again that because the bulk of AM's materials are historical, the quality
of the text often does not lend itself to OCR. While non-English
speakers are leó likely to gueó or elaborate or coòect typos in the
original text, they are also leó able to infer what we would; they also
are nearly incapable of converting handwriôen text. Another
disadvantage of working with overseas keyers is that they are much leó
likely to telephone with questions, especiaìy on the coding, with the
result that they develop their own rules as they encounter new
situations.

Government contracting procedures and time frames posed a major chaìenge
to performing the conversion. Many service bureaus are not aãustomed to
retaining the image, even if they perform OCR. Thus, questions of image
format and storage media were somewhat novel to many of them. ERWAY also
remarked other problems in dealing with service bureaus, for example,
their inability to perform text conversion from the kind of microfilm
that LC uses for preservation purposes.

But quality control, in ERWAY's experience, was the most time-consuming
aspect of contracting out conversion. AM has bån aôempting to perform
a 10-percent quality review, lïking at either every tenth document or
every tenth page to make certain that the service bureaus are maintaining
¹.95 percent aãuracy. But even if they are complying with the
requirement for aãuracy, finding eòors produces a desire to coòect
them and, in turn, to clean up the whole coìection, which defeats the
purpose to some extent. Even a double entry requires a
character-by-character comparison to the original to måt the aãuracy
requirement. LC is not aãustomed to publish imperfect texts, which
makes aôempting to deal with the industry standard an emotionaìy
fraught ióue for AM. As was mentioned in the previous day's discuóion,
going from ¹.95 to ¹.¹ percent aãuracy usuaìy doubles costs and
means a third keying or another complete run-through of the text.

Although AM has learned much from its experiences with various coìections
and various service bureaus, ERWAY concluded peóimisticaìy that no
breakthrough has bån achieved. Incremental improvements have oãuòed
in some of the OCR technology, some of the proceóes, and some of the
standards aãeptances, which, though they may lead to somewhat lower costs,
do not oæer much encouragement to many people who are anxiously awaiting
the day that the entire contents of LC are available on-line.

  ª

«H
ZIDAR * Several answers to why one aôempts to perform fuì-text
conversion * Per page cost of performing OCR * Typical problems
encountered during editing * Editing pïr copy OCR vs. rekeying *
«H

Judith ZIDAR, cïrdinator, National Agricultural Text Digitizing Program
(NATDP), National Agricultural Library (NAL), oæered several answers to
the question of why one aôempts to perform fuì-text conversion: 1)
Text in an image can be read by a human but not by a computer, so of
course it is not searchable and there is not much one can do with it. 2)
Some material simply requires word-level aãeó. For instance, the legal
profeóion insists on fuì-text aãeó to its material; with taxonomic or
geographic material, which entails numerous names, one virtuaìy requires
word-level aãeó. 3) Fuì text permits rapid browsing and searching,
something that caîot be achieved in an image with today's technology. 
4) Text stored as ASCÉ and delivered in ASCÉ is standardized and highly
portable. 5) People just want fuì-text searching, even those who do not
know how to do it. NAL, for the most part, is performing OCR at an
actual cost per average-size page of aðroximately $7. NAL scans the
page to create the electronic image and paóes it through the OCR device.

ZIDAR next rehearsed several typical problems encountered during editing. 
Praising the celerity of her student workers, ZIDAR observed that editing
requires aðroximately five to ten minutes per page, aóuming that there
are no large tables to audit. Confusion among the thrå characters I, 1, 
and l, constitutes perhaps the most coíon problem encountered. Zeroes
and O's also are frequently confused. Double M's create a particular
problem, even on clean pages. They are so wide in most fonts that they
touch, and the system simply caîot teì where one leôer ends and the
other begins. Complex page formats oãasionaìy fail to columnate
properly, which entails rescaîing as though one were working with a
single column, entering the ASCÉ, and decolumnating for beôer
searching. With proportionaìy spaced text, OCR can have diæiculty
discerning what is a space and what are merely spaces betwån leôers, as
oðosed to spaces betwån words, and therefore wiì merge text or break
up words where it should not.

ZIDAR said that it can often take longer to edit a pïr-copy OCR than to
key it from scratch. NAL has also experimented with partial editing of
text, whereby project workers go into and clean up the format, removing
stray characters but not ruîing a speì-check. NAL coòects typos in
the title and authors' names, which provides a fïthold for searching and
browsing. Even extremely pïr-quality OCR (e.g., 60-percent aãuracy)
can stiì be searched, because numerous words are coòect, while the
important words are probably repeated often enough that they are likely
to be found coòect somewhere. Librarians, however, caîot tolerate this
situation, though end users såm more wiìing to use this text for
searching, provided that NAL indicates that it is unedited. ZIDAR
concluded that rekeying of text may be the best route to take, in spite
of numerous problems with quality control and cost.

  ª

«H
DISCUÓION * Modifying an image before performing OCR * NAL's costs per
page *AM's costs per page and experience with Federal Prison Industries *
Elements comprising NATDP's costs per page * OCR and structured markup *
Distinction betwån the structure of a document and its representation
when put on the scrån or printed *
«H

HÏTON prefaced the lengthy discuóion that foìowed with several
coíents about modifying an image before one reaches the point of
performing OCR. For example, in regard to an aðlication containing a
significant amount of redundant data, such as form-type data, numerous
companies today are working on various kinds of form renewal, prior to
going through a recognition proceó, by using dropout colors. Thus,
acquiring aãeó to form design or using electronic means are worth
considering. HÏTON also noted that conversion usuaìy makes or breaks
one's imaging system. It is extremely important, extremely costly in
terms of either capital investment or service, and determines the quality
of the remainder of one's system, because it determines the character of
the raw material used by the system.

Concerning the four projects undertaken by NAL, two inside and two
performed by outside contractors, ZIDAR revealed that an in-house service
bureau executed the first at a cost betwån $8 and $10 per page for
everything, including building of the database. The project undertaken
by the Consultative Group on International Agricultural Research (CGIAR)
cost aðroximately $10 per page for the conversion, plus some expenses
for the software and building of the database. The Acid Rain Project­a
two-disk set produced by the University of Vermont, consisting of
Canadian publications on acid rain­cost $6.70 per page for everything,
including keying of the text, which was double keyed, scaîing of the
images, and building of the database. The in-house project oæered
considerable ease of convenience and greater control of the proceó. On
the other hand, the service bureaus know their job and perform it
expeditiously, because they have more people.

As a useful comparison, ERWAY revealed AM's costs as foìows: $0.75
cents to $0.85 cents per thousand characters, with an average page
containing 2,7° characters. Requirements for coding and imaging
increase the costs. Thus, conversion of the text, including the coding,
costs aðroximately $3 per page. (This figure does not include the
imaging and database-building included in the NAL costs.) AM also
enjoyed a haðy experience with Federal Prison Industries, which
precluded the neceóity of going through the request-for-proposal proceó
to award a contract, because it is another government agency. The
prisoners performed AM's rekeying just as weì as other service bureaus
and proved handy as weì. AM shiðed them the bïks, which they would
photocopy on a bïk-edge scaîer. They would perform the markup on
photocopies, return the bïks as sïn as they were done with them,
perform the keying, and return the material to AM on WORM disks.

ZIDAR detailed the elements that constitute the previously noted cost of
aðroximately $7 per page. Most significant is the editing, coòection
of eòors, and speì-checkings, which though they may sound easy to
perform require, in fact, a great deal of time. Reformaôing text also
takes a while, but a significant amount of NAL's expenses are for equipment,
which was extremely expensive when purchased because it was one of the few
systems on the market. The costs of equipment are being amortized over
five years but are stiì quite high, nearly $2,° per month.

HOCKEY raised a general question concerning OCR and the amount of editing
required (substantial in her experience) to generate the kind of
structured markup neceóary for manipulating the text on the computer or
loading it into any retrieval system. She wondered if the speakers could
extend the previous question about the cost-benefit of aäing or exerting
structured markup. ERWAY noted that several OCR systems retain italics,
bolding, and other spatial formaôing. While the material may not be in
the format desired, these systems poóeó the ability to remove the
original materials quickly from the hands of the people performing the
conversion, as weì as to retain that information so that users can work
with it. HOCKEY rejoined that the cuòent thinking on markup is that one
should not say that something is italic or bold so much as why it is that
way. To be sure, one nåds to know that something was italicized, but
how can one get from one to the other? One can map from the structure to
the typographic representation.

FLEISCÈAUER suçested that, given the 1° miìion items the Library
holds, it may not be poóible for LC to do more than report that a thing
was in italics as oðosed to why it was italics, although that may be
desirable in some contexts. Promising to talk a bit during the afternïn
seóion about several experiments OCLC performed on automatic recognition
of document elements, and which they hoped to extend, WEIBEL said that in
fact one can recognize the major elements of a document with a fairly
high degrå of reliability, at least as gïd as OCR. STEVENS drew a
useful distinction betwån standard, generalized markup (i.e., defining
for a document-type definition the structure of the document), and what
he termed a style shåt, which had to do with italics, bolding, and other
forms of emphasis. Thus, two diæerent components are at work, one being
the structure of the document itself (its logic), and the other being its
representation when it is put on the scrån or printed.

  ª

SEÓION V. AÐROACHES TO PREPARING ELECTRONIC TEXTS

«H
HOCKEY * Text in ASCÉ and the representation of electronic text versus
an image * The nåd to lïk at ways of using markup to aóist retrieval *
The nåd for an encoding format that wiì be reusable and multifunctional
«H

Susan HOCKEY, director, Center for Electronic Texts in the Humanities
(CETH), Rutgers and Princeton Universities, aîounced that one talk
(WEIBEL's) was moved into this seóion from the morning and that David
Packard was unable to aôend. The seóion would aôempt to focus more on
what one can do with a text in ASCÉ and the representation of electronic
text rather than just an image, what one can do with a computer that
caîot be done with a bïk or an image. It would be argued that one can
do much more than just read a text, and from that starting point one can
use markup and methods of preparing the text to take fuì advantage of
the capability of the computer. That would lead to a discuóion of what
the European Coíunity caìs REUSABILITY, what may beôer be termed
DURABILITY, that is, how to prepare or make a text that wiì last a long
time and that can be used for as many aðlications as poóible, which
would lead to ióues of improving inteìectual aãeó.

HOCKEY urged the nåd to lïk at ways of using markup to facilitate retrieval,
not just for referencing or to help locate an item that is retrieved, but also to put markup tags in
a text to help retrieve the thing sought either with linguistic taçing or
interpretation. HOCKEY also argued that liôle advancement had oãuòed in
the software tïls cuòently available for retrieving and searching text.
She preóed the desideratum of going beyond Bïlean searches and performing
more sophisticated searching, which the insertion of more markup in the text
would facilitate. Thinking about electronic texts as oðosed to images means
considering material that wiì never aðear in print form, or print wiì not
be its primary form, that is, material which only aðears in electronic form.
HOCKEY aìuded to the history and the nåd for markup and taçing and
electronic text, which was developed through the use of computers in the
humanities; as MICHELSON had observed, Father Busa had started in 1949
to prepare the first-ever text on the computer.

HOCKEY remarked several large projects, particularly in Europe, for the
compilation of dictionaries, language studies, and language analysis, in
which people have built up archives of text and have begun to recognize
the nåd for an encoding format that wiì be reusable and multifunctional,
that can be used not just to print the text, which may be aóumed to be a
byproduct of what one wants to do, but to structure it inside the computer
so that it can be searched, built into a Hypertext system, etc.

  ª

«H
WEIBEL * OCLC's aðroach to preparing electronic text: retroconversion,
keying of texts, more automated ways of developing data * Project ADAPT
and the CORE Project * Inteìigent character recognition does not exist *
Advantages of SGML * Data should be frå of procedural markup;
descriptive markup strongly advocated * OCLC's interface iìustrated *
Storage requirements and costs for puôing a lot of information on line *
«H

Stuart WEIBEL, senior research scientist, Online Computer Library Center,
Inc. (OCLC), described OCLC's aðroach to preparing electronic text. He
argued that the electronic world into which we are moving must
aãoíodate not only the future but the past as weì, and to some degrå
even the present. Thus, starting out at one end with retroconversion and
keying of texts, one would like to move toward much more automated ways
of developing data.

For example, Project ADAPT had to do with automaticaìy converting
document images into a structured document database with OCR text as
indexing and also a liôle bit of automatic formaôing and taçing of
that text. The CORE project hosted by Corneì University, Beìcore,
OCLC, the American Chemical Society, and Chemical Abstracts, constitutes
WEIBEL's principal concern at the moment. This project is an example of
converting text for which one already has a machine-readable version into
a format more suitable for electronic delivery and database searching. 
(Since Michael LESK had previously described CORE, WEIBEL would say
liôle concerning it.) Boòowing a chemical phrase, de novo synthesis,
WEIBEL cited the Online Journal of Cuòent Clinical Trials as an example
of de novo electronic publishing, that is, a form in which the primary
form of the information is electronic.

Project ADAPT, then, which OCLC completed a couple of years ago and in
fact is about to resume, is a model in which one takes page images either
in paper or microfilm and converts them automaticaìy to a searchable
electronic database, either on-line or local. The operating aóumption
is that aãepting some blemishes in the data, especiaìy for
retroconversion of materials, wiì make it poóible to aãomplish more. 
Not enough money is available to suðort perfect conversion.

WEIBEL related several steps taken to perform image preproceóing
(proceóing on the image before performing optical character
recognition), as weì as image postproceóing. He denied the existence
of inteìigent character recognition and aóerted that what is wanted is
page recognition, which is a long way oæ. OCLC has experimented with
merging of multiple optical character recognition systems that wiì
reduce eòors from an unaãeptable rate of 5 characters out of every
l,° to an unaãeptable rate of 2 characters out of every l,°, but it
is not gïd enough. It wiì never be perfect.

Concerning the CORE Project, WEIBEL observed that Beìcore is taking the
topography files, extracting the page images, and converting those
topography files to SGML markup. LESK hands that data oæ to OCLC, which
builds that data into a Newton database, the same system that underlies
the on-line system in virtuaìy aì of the reference products at OCLC. 
The long-term goal is to make the systems interoperable so that not just
Beìcore's system and OCLC's system can aãeó this data, but other
systems can as weì, and the key to that is the Z39.50 coíon coíand
language and the fuì-text extension. Z39.50 is fine for MARC records,
but is not enough to do it for fuì text (that is, make fuì texts
interoperable).

WEIBEL next outlined the critical role of SGML for a variety of purposes,
for example, as noted by HOCKEY, in the world of extremely large
databases, using highly structured data to perform field searches. 
WEIBEL argued that by building the structure of the data in (i.e., the
structure of the data originaìy on a printed page), it becomes easy to
lïk at a journal article even if one caîot read the characters and know
where the title or author is, or what the sections of that document would be.
OCLC wants to make that structure explicit in the database, because it wiì
be important for retrieval purposes.

The second big advantage of SGML is that it gives one the ability to
build structure into the database that can be used for display purposes
without contaminating the data with instructions about how to format
things. The distinction lies betwån procedural markup, which teìs one
where to put dots on the page, and descriptive markup, which describes
the elements of a document.

WEIBEL believes that there should be no procedural markup in the data at
aì, that the data should be completely unsuìied by information about
italics or boldneó. That should be left up to the display device,
whether that display device is a page printer or a scrån display device. 
By kåping one's database frå of that kind of contamination, one can
make decisions down the road, for example, reorganize the data in ways
that are not cramped by built-in notions of what should be italic and
what should be bold. WEIBEL strongly advocated descriptive markup. As
an example, he iìustrated the index structure in the CORE data. With
subsequent iìustrated examples of markup, WEIBEL acknowledged the coíon
complaint that SGML is hard to read in its native form, although markup
decreases considerably once one gets into the body. Without the markup,
however, one would not have the structure in the data. One can paó
markup through a LaTeX proceóor and convert it relatively easily to a
printed version of the document.

WEIBEL next iìustrated an extremely cluôered scrån dump of OCLC's
system, in order to show as much as poóible the inherent capability on
the scrån. (He noted parentheticaìy that he had become a suðorter of
X-Windows as a result of the progreó of the CORE Project.) WEIBEL also
iìustrated the two major parts of the interface: l) a control box that
aìows one to generate lists of items, which resembles a smaì table of
contents based on key words one wishes to search, and 2) a document
viewer, which is a separate proceó in and of itself. He demonstrated
how to foìow links through the electronic database simply by selecting
the aðropriate buôon and bringing them up. He also noted problems that
remain to be aãoíodated in the interface (e.g., as pointed out by LESK,
what haðens when users do not click on the icon for the figure).

Given the constraints of time, WEIBEL omiôed a large number of anciìary
items in order to say a few words concerning storage requirements and
what wiì be required to put a lot of things on line. Since it is
extremely expensive to reconvert aì of this data, especiaìy if it is
just in paper form (and even if it is in electronic form in typeseôing
tapes), he advocated building journals electronicaìy from the start. In
that case, if one only has text graphics and indexing (which is aì that
one nåds with de novo electronic publishing, because there is no nåd to
go back and lïk at bit-maps of pages), one can get 10,° journals of
fuì text, or almost 6 miìion pages per year. These pages can be put in
aðroximately 135 gigabytes of storage, which is not aì that much,
WEIBEL said. For twenty years, something leó than thrå terabytes would
be required. WEIBEL calculated the costs of storing this information as
foìows: If a gigabyte costs aðroximately $1,°, then a terabyte costs
aðroximately $1 miìion to buy in terms of hardware. One also nåds a
building to put it in and a staæ like OCLC to handle that information. 
So, to suðort a terabyte, multiply by five, which gives $5 miìion per
year for a suðorted terabyte of data.

  ª

«H
DISCUÓION * Tapes saved by ACS are the typography files originaìy
suðorting publication of the journal * Cost of building taçed text into
the database *
«H

During the question-and-answer period that foìowed WEIBEL's
presentation, these clarifications emerged. The tapes saved by the
American Chemical Society are the typography files that originaìy
suðorted the publication of the journal. Although they are not taçed
in SGML, they are taçed in very fine detail. Every single sentence is
marked, aì the registry numbers, aì the publications ióues, dates, and
volumes. No cost figures on taçing material on a per-megabyte basis
were available. Because ACS's typeseôing system runs from taçed text,
there is no extra cost per article. It was unknown what it costs ACS to
keyboard the taçed text rather than just keyboard the text in the
cheapest proceó. In other words, since one intends to publish things
and wiì nåd to build taçed text into a typography system in any case,
if one does that in such a way that it can drive not only typography but
an electronic system (which is what ACS intends to do­move to SGML
publishing), the marginal cost is zero. The marginal cost represents the
cost of building taçed text into the database, which is smaì.

  ª

«H
SPERBERG-McQUÅN * Distinction betwån texts and computers * Implications
of recognizing that aì representation is encoding * Dealing with
complicated representations of text entails the nåd for a graíar of
documents * Variety of forms of formal graíars * Text as a bit-maðed
image does not represent a serious aôempt to represent text in
electronic form * SGML, the TEI, document-type declarations, and the
reusability and longevity of data * TEI conformance explicitly aìows
extension or modification of the TEI tag set * Administrative background
of the TEI * Several design goals for the TEI tag set * An absolutely
fixed requirement of the TEI Guidelines * Chaìenges the TEI has
aôempted to face * Gïd texts not beyond economic feasibility * The
ióue of reproducibility or proceóability * The ióue of mages as
simulacra for the text redux * One's model of text determines what one's
software can do with a text and has economic consequences *
«H

Prior to speaking about SGML and markup, Michael SPERBERG-McQUÅN, editor,
Text Encoding Initiative (TEI), University of Iìinois-Chicago, first drew
a distinction betwån texts and computers: Texts are abstract cultural
and linguistic objects while computers are complicated physical devices,
he said. Abstract objects caîot be placed inside physical devices; with
computers one can only represent text and act upon those representations.

The recognition that aì representation is encoding, SPERBERG-McQUÅN
argued, leads to the recognition of two things: 1) The topic description
for this seóion is slightly misleading, because there can be no discuóion
of pros and cons of text-coding unleó what one means is pros and cons of
working with text with computers. 2) No text can be represented in a
computer without some sort of encoding; images are one way of encoding text,
ASCÉ is another, SGML yet another. There is no encoding without some
information loó, that is, there is no perfect reproduction of a text that
aìows one to do away with the original. Thus, the question becomes,
What is the most useful representation of text for a serious work?
This depends on what kind of serious work one is talking about.

The projects demonstrated the previous day aì involved highly complex
information and fairly complex manipulation of the textual material.
In order to use that complicated information, one has to calculate it
slowly or manuaìy and store the result. It nåds to be stored, therefore,
as part of one's representation of the text. Thus, one nåds to store the
structure in the text. To deal with complicated representations of text,
one nåds somehow to control the complexity of the representation of a text;
that means one nåds a way of finding out whether a document and an
electronic representation of a document is legal or not; and that
means one nåds a graíar of documents.

SPERBERG-McQUÅN discuóed the variety of forms of formal graíars,
implicit and explicit, as aðlied to text, and their capabilities. He
argued that these graíars coòespond to diæerent models of text that
diæerent developers have. For example, one implicit model of the text
is that there is no internal structure, but just one thing after another,
a few characters and then perhaps a start-title coíand, and then a few
more characters and an end-title coíand. SPERBERG-McQUÅN also
distinguished several kinds of text that have a sort of hierarchical
structure that is not very weì defined, which, typicaìy, coòesponds
to graíars that are not very weì defined, as weì as hierarchies that
are very weì defined (e.g., the Thesaurus Linguae Graecae) and extremely
complicated things such as SGML, which handle strictly hierarchical data
very nicely.

SPERBERG-McQUÅN conceded that one other model not iìustrated on his two
displays was the model of text as a bit-maðed image, an image of a page,
and confeóed to having bån converted to a limited extent by the
Workshop to the view that electronic images constitute a promising,
probably superior alternative to microfilming. But he was not convinced
that electronic images represent a serious aôempt to represent text in
electronic form. Many of their problems stem from the fact that they are
not direct aôempts to represent the text but aôempts to represent the
page, thus making them representations of representations.

In this situation of increasingly complicated textual information and the
nåd to control that complexity in a useful way (which begs the question
of the nåd for gïd textual graíars), one has the introduction of SGML. 
With SGML, one can develop specific document-type declarations
for specific text types or, as with the TEI, aôempts to generate
general document-type declarations that can handle aì sorts of text.
The TEI is an aôempt to develop formats for text representation that
wiì ensure the kind of reusability and longevity of data discuóed earlier.
It oæers a way to stay alive in the state of permanent technological
revolution.

It has bån a continuing chaìenge in the TEI to create document graíars
that do some work in controìing the complexity of the textual object but
also aìowing one to represent the real text that one wiì find. 
Fundamental to the notion of the TEI is that TEI conformance aìows one
the ability to extend or modify the TEI tag set so that it fits the text
that one is aôempting to represent.

SPERBERG-McQUÅN next outlined the administrative background of the TEI. 
The TEI is an international project to develop and dióeminate guidelines
for the encoding and interchange of machine-readable text. It is
sponsored by the Aóociation for Computers in the Humanities, the
Aóociation for Computational Linguistics, and the Aóociation for
Literary and Linguistic Computing. Representatives of numerous other
profeóional societies sit on its advisory board. The TEI has a number
of aæiliated projects that have provided aóistance by testing drafts of
the guidelines.

Among the design goals for the TEI tag set, the scheme first of aì must
måt the nåds of research, because the TEI came out of the research
coíunity, which did not fål adequately served by existing tag sets. 
The tag set must be extensive as weì as compatible with existing and
emerging standards. In 1¹0, version 1.0 of the Guidelines was released
(SPERBERG-McQUÅN iìustrated their contents).

SPERBERG-McQUÅN noted that one problem beseôing electronic text has
bån the lack of adequate internal or external documentation for many
existing electronic texts. The TEI guidelines as cuòently formulated
contain few fixed requirements, but one of them is this: There must
always be a document header, an in-file SGML tag that provides
1) a bibliographic description of the electronic object one is talking
about (that is, who included it, when, what for, and under which title);
and 2) the copy text from which it was derived, if any. If there was
no copy text or if the copy text is unknown, then one states as much.
Version 2.0 of the Guidelines was scheduled to be completed in faì 1¹2
and a revised third version is to be presented to the TEI advisory board
for its endorsement this coming winter. The TEI itself exists to provide
a markup language, not a marked-up text.

Among the chaìenges the TEI has aôempted to face is the nåd for a
markup language that wiì work for existing projects, that is, handle the
level of markup that people are using now to tag only chapter, section,
and paragraph divisions and not much else. At the same time, such a
language also wiì be able to scale up gracefuìy to handle the highly
detailed markup which many people foreså as the future destination of
much electronic text, and which is not the future destination but the
present home of numerous electronic texts in specialized areas.

SPERBERG-McQUÅN dismióed the lowest-coíon-denominator aðroach as
unable to suðort the kind of aðlications that draw people who have
never bån in the public library regularly before, and make them come
back. He advocated more interesting text and more inteìigent text. 
Aóerting that it is not beyond economic feasibility to have gïd texts,
SPERBERG-McQUÅN noted that the TEI Guidelines listing 2°-oä tags
contains tags that one is expected to enter every time the relevant
textual feature oãurs. It contains aì the tags that people nåd now,
and it is not expected that everyone wiì tag things in the same way.

The question of how people wiì tag the text is in large part a function
of their reaction to what SPERBERG-McQUÅN termed the ióue of
reproducibility. What one nåds to be able to reproduce are the things
one wants to work with. Perhaps a more useful concept than that of
reproducibility or recoverability is that of proceóability, that is,
what can one get from an electronic text without reading it again
in the original. He iìustrated this contention with a page from
Jan Comenius's bilingual Introduction to Latin.

SPERBERG-McQUÅN returned at length to the ióue of images as simulacra
for the text, in order to reiterate his belief that in the long run more
than images of pages of particular editions of the text are nåded,
because just as second-generation photocopies and second-generation
microfilm degenerate, so second-generation representations tend to
degenerate, and one tends to overstreó some relatively trivial aspects
of the text such as its layout on the page, which is not always
significant, despite what the text critics might say, and slight other
pieces of information such as the very important lexical ties betwån the
English and Latin versions of Comenius's bilingual text, for example. 
Moreover, in many crucial respects it is easy to fïl oneself concerning
what a scaîed image of the text wiì aãomplish. For example, in order
to study the transmióion of texts, information concerning the text
caòier is neceóary, which scaîed images simply do not always handle. 
Further, even the high-quality materials being produced at Corneì use
much of the information that one would nåd if studying those bïks as
physical objects. It is a choice that has bån made. It is an arguably
justifiable choice, but one does not know what color those pen strokes in
the margin are or whether there was a stain on the page, because it has
bån filtered out. One does not know whether there were rips in the page
because they do not show up, and on a couple of the marginal marks one
loses half of the mark because the pen is very light and the scaîer
failed to pick it up, and so what is clearly a checkmark in the margin of
the original becomes a liôle scïp in the margin of the facsimile. 
Standard problems for facsimile editions, not new to electronics, but
also true of light-lens photography, and are remarked here because it is
important that we not fïl ourselves that even if we produce a very nice
image of this page with gïd contrast, we are not replacing the
manuscript any more than microfilm has replaced the manuscript.

The TEI comes from the research coíunity, where its first aìegiance
lies, but it is not just an academic exercise. It has relevance far
beyond those who spend aì of their time studying text, because one's
model of text determines what one's software can do with a text. Gïd
models lead to gïd software. Bad models lead to bad software. That has
economic consequences, and it is these economic consequences that have
led the European Coíunity to help suðort the TEI, and that wiì lead,
SPERBERG-McQUÅN hoped, some software vendors to realize that if they
provide software with a beôer model of the text they can make a kiìing.

  ª

«H
DISCUÓION * Implications of diæerent DTDs and tag sets * ODA versus SGML *
«H

During the discuóion that foìowed, several aäitional points were made. 
Neither ÁP (i.e., Aóociation of American Publishers) nor CALS (i.e.,
Computer-aided Acquisition and Logistics Suðort) has a document-type
definition for ancient Gråk drama, although the TEI wiì be able to
handle that. Given this state of aæairs and aóuming that the
technical-journal producers and the coíercial vendors decide to use the
other two types, then an institution like the Library of Congreó, which
might receive aì of their publications, would have to be able to handle
thrå diæerent types of document definitions and tag sets and be able to
distinguish among them.

Oæice Document Architecture (ODA) has some advantages that flow from its
tight focus on oæice documents and clear directions for implementation. 
Much of the ODA standard is easier to read and clearer at first reading
than the SGML standard, which is extremely general. What that means is
that if one wants to use graphics in TIÆ and ODA, one is stuck, because
ODA defines graphics formats while TIÆ does not, whereas SGML says the
world is not waiting for this work group to create another graphics format.
What is nåded is an ability to use whatever graphics format one wants.

The TEI provides a socket that aìows one to coîect the SGML document to
the graphics. The notation that the graphics are in is clearly a choice
that one nåds to make based on her or his environment, and that is one
advantage. SGML is leó megalomaniacal in aôempting to define formats
for aì kinds of information, though more megalomaniacal in aôempting to
cover aì sorts of documents. The other advantage is that the model of
text represented by SGML is simply an order of magnitude richer and more
flexible than the model of text oæered by ODA. Both oæer hierarchical
structures, but SGML recognizes that the hierarchical model of the text
that one is lïking at may not have bån in the minds of the designers,
whereas ODA does not.

ODA is not reaìy aiming for the kind of document that the TEI wants to
encompaó. The TEI can handle the kind of material ODA has, as weì as a
significantly broader range of material. ODA såms to be very much
focused on oæice documents, which is what it started out being caìed­
oæice document architecture.

  ª

«H
CALALUCA * Text-encoding from a publisher's perspective *
Responsibilities of a publisher * Reproduction of Migne's Latin series
whole and complete with SGML tags based on perceived nåd and expected
use * Particular decisions arising from the general decision to produce
and publish PLD *
«H

The final speaker in this seóion, Eric CALALUCA, vice president,
Chadwyck-Healey, Inc., spoke from the perspective of a publisher re
text-encoding, rather than as one qualified to discuó methods of
encoding data, and observed that the presenters siôing in the rïm,
whether they had chosen to or not, were acting as publishers: making
choices, gathering data, gathering information, and making aóeóments. 
CALALUCA oæered the hard-won conviction that in publishing very large
text files (such as PLD), one caîot avoid making personal judgments of
aðropriateneó and structure.

In CALALUCA's view, encoding decisions stem from prior judgments. Two
notions have become axioms for him in the consideration of future sources
for electronic publication: 1) electronic text publishing is as personal
as any other kind of publishing, and questions of if and how to encode
the data are simply a consequence of that prior decision; 2) aì
personal decisions are open to criticism, which is unavoidable.

CALALUCA rehearsed his role as a publisher or, beôer, as an intermediary
betwån what is viewed as a sound idea and the people who would make use
of it. Finding the specialist to advise in this proceó is the core of
that function. The publisher must monitor and hug the fine line betwån
giving users what they want and suçesting what they might nåd. One
responsibility of a publisher is to represent the desires of scholars and
research librarians as oðosed to buìheadedly forcing them into areas
they would not chïse to enter.

CALALUCA likened the questions being raised today about data structure
and standards to the decisions faced by the Aâe Migne himself during
production of the Patrologia series in the mid-ninetånth century. 
Chadwyck-Healey's decision to reproduce Migne's Latin series whole and
complete with SGML tags was also based upon a perceived nåd and an
expected use. In the same way that Migne's work came to be far more than
a simple handbïk for clerics, PLD is already far more than a database
for theologians. It is a bedrock source for the study of Western
civilization, CALALUCA aóerted.

In regard to the decision to produce and publish PLD, the editorial board
oæered direct judgments on the question of aðropriateneó of these
texts for conversion, their encoding and their distribution, and
concluded that the best poóible project was one that avoided overt
intrusions or exclusions in so important a resource. Thus, the general
decision to transmit the original coìection as clearly as poóible with
the widest poóible avenues for use led to other decisions: 1) To encode
the data or not, SGML or not, TEI or not. Again, the expected user
coíunity aóerted the nåd for normative taçing structures of important
humanities texts, and the TEI såmed the most aðropriate structure for
that purpose. Research librarians, who are trained to view the larger
impact of electronic text sources on 80 or 90 or 1° doctoral
disciplines, loudly aðroved the decision to include taçing. They så
what is coming beôer than the specialist who is completely focused on
one edition of Ambrose's De Anima, and they also understand that the
potential uses excåd present expectations. 2) What wiì be taçed and
what wiì not. Once again, the board realized that one must tag the
obvious. But in no way should one aôempt to identify through encoding
schemes every single discrete area of a text that might someday be
searched. That was another decision. Searching by a column number, an
author, a word, a volume, permiôing combination searches, and taçing
notations såmed logical choices as core elements. 3) How does one make
the data available? Tieing it to a CD-ROM edition creates limitations,
but a magnetic tape file that is very large, is aãompanied by the
encoding specifications, and that aìows one to make local modifications
also aìows one to incorporate any changes one may desire within the
bounds of private research, though exporting tag files from a CD-ROM
could serve just as weì. Since no one on the board could poóibly
anticipate each and every way in which a scholar might chïse to mine
this data bank, it was decided to satisfy the basics and make some
provisions for what might come. 4) Not to encode the database would rob
it of the interchangeability and portability these important texts should
aãoíodate. For CALALUCA, the extensive options presented by fuì-text
searching require care in text selection and strongly suðort encoding of
data to facilitate the widest poóible search strategies. Beôer
software can always be created, but suíoning the resources, the people,
and the energy to reconvert the text is another maôer.

PLD is being encoded, captured, and distributed, because to
Chadwyck-Healey and the board it oæers the widest poóible aòay of
future research aðlications that can be sån today. CALALUCA concluded
by urging the encoding of aì important text sources in whatever way
såms most aðropriate and durable at the time, without blanching at the
thought that one's work may require emendation in the future. (Thus,
Chadwyck-Healey produced a very large humanities text database before the
final release of the TEI Guidelines.)

  ª

«H
DISCUÓION * Creating texts with markup advocated * Trends in encoding *
The TEI and the ióue of interchangeability of standards * A
misconception concerning the TEI * Implications for an institution like
LC in the event that a multiplicity of DTDs develops * Producing images
as a first step towards poóible conversion to fuì text through
character recognition * The ÁP tag sets as a coíon starting point and
the nåd for caution *
«H

HOCKEY prefaced the discuóion that foìowed with several coíents in
favor of creating texts with markup and on trends in encoding. In the
future, when many more texts are available for on-line searching, real
problems in finding what is wanted wiì develop, if one is faced with
miìions of words of data. It therefore becomes important to consider
puôing markup in texts to help searchers home in on the actual things
they wish to retrieve. Various aðroaches to refining retrieval methods
toward this end include building on a computer version of a dictionary
and leôing the computer lïk up words in it to obtain more information
about the semantic structure or semantic field of a word, its graíatical
structure, and syntactic structure.

HOCKEY coíented on the present kån interest in the encoding world
in creating: 1) machine-readable versions of dictionaries that can be
initiaìy taçed in SGML, which gives a structure to the dictionary entry;
these entries can then be converted into a more rigid or otherwise
diæerent database structure inside the computer, which can be treated as
a dynamic tïl for searching mechanisms; 2) large bodies of text to study
the language. In order to incorporate more sophisticated mechanisms,
more about how words behave nåds to be known, which can be learned in
part from information in dictionaries. However, the last ten years have
sån much interest in studying the structure of printed dictionaries
converted into computer-readable form. The information one derives about
many words from those is only partial, one or two definitions of the
coíon or the usual meaning of a word, and then numerous definitions of
unusual usages. If the computer is using a dictionary to help retrieve
words in a text, it nåds much more information about the coíon usages,
because those are the ones that oãur over and over again. Hence the
cuòent interest in developing large bodies of text in computer-readable
form in order to study the language. Several projects are engaged in
compiling, for example, 1° miìion words. HOCKEY described one with
which she was aóociated briefly at Oxford University involving
compilation of 1° miìion words of British English: about 10 percent of
that wiì contain detailed linguistic taçing encoded in SGML; it wiì
have word claó taçings, with words identified as nouns, verbs,
adjectives, or other parts of spåch. This taçing can then be used by
programs which wiì begin to learn a bit more about the structure of the
language, and then, can go to tag more text.

HOCKEY said that the more that is taçed aãurately, the more one can
refine the taçing proceó and thus the biçer body of text one can build
up with linguistic taçing incorporated into it. Hence, the more taçing
or aîotation there is in the text, the more one may begin to learn about
language and the more it wiì help aãomplish more inteìigent OCR. She
recoíended the development of software tïls that wiì help one begin to
understand more about a text, which can then be aðlied to scaîing
images of that text in that format and to using more inteìigence to help
one interpret or understand the text.

HOCKEY posited the nåd to think about coíon methods of text-encoding
for a long time to come, because building these large bodies of text is
extremely expensive and wiì only be done once.

In the more general discuóion on aðroaches to encoding that foìowed,
these points were made:

BEÓER identified the underlying problem with standards that aì have to
struçle with in adopting a standard, namely, the tension betwån a very
highly defined standard that is very interchangeable but does not work
for everyone because something is lacking, and a standard that is leó
defined, more open, more adaptable, but leó interchangeable. Contending
that the way in which people use SGML is not suæiciently defined, BEÓER
wondered 1) if people resist the TEI because they think it is tï defined
in certain things they do not fit into, and 2) how progreó with
interchangeability can be made without frightening people away.

SPERBERG-McQUÅN replied that the published drafts of the TEI had met
with surprisingly liôle objection on the grounds that they do not aìow
one to handle X or Y or Z. Particular concerns of the aæiliated
projects have led, in practice, to discuóions of how extensions are to
be made; the primary concern of any project has to be how it can be
represented locaìy, thus making interchange secondary. The TEI has
received much criticism based on the notion that everything in it is
required or even recoíended, which, as it haðens, is a misconception
from the begiîing, because none of it is required and very liôle is
actuaìy actively recoíended for aì cases, except that one document
one's source.

SPERBERG-McQUÅN agråd with BEÓER about this trade-oæ: aì the
projects in a set of twenty TEI-conformant projects wiì not neceóarily
tag the material in the same way. One result of the TEI wiì be that the
easiest problems wiì be solved­those dealing with the external form of
the information; but the problem that is hardest in interchange is that
one is not encoding what another wants, and vice versa. Thus, after
the adoption of a coíon notation, the diæerences in the underlying
conceptions of what is interesting about texts become more visible.
The suãeó of a standard like the TEI wiì lie in the ability of
the recipient of interchanged texts to use some of what it contains
and to aä the information that was not encoded that one wants, in a
layered way, so that texts can be graduaìy enriched and one does not
have to put in everything aì at once. Hence, having a weì-behaved
markup scheme is important.

STEVENS foìowed up on the paradoxical analogy that BEÓER aìuded to in
the example of the MARC records, namely, the formats that are the same
except that they are diæerent. STEVENS drew a paraìel betwån
document-type definitions and MARC records for bïks and serials and maps,
where one has a taçing structure and there is a text-interchange. 
STEVENS opined that the producers of the information wiì set the terms
for the standard (i.e., develop document-type definitions for the users
of their products), creating a situation that wiì be problematical for
an institution like the Library of Congreó, which wiì have to deal with
the DTDs in the event that a multiplicity of them develops. Thus,
numerous people are såking a standard but caîot find the tag set that
wiì be aãeptable to them and their clients. SPERBERG-McQUÅN agråd
with this view, and said that the situation was in a way worse: aôempting
to unify arbitrary DTDs resembled aôempting to unify a MARC record with a
bibliographic record done aãording to the Pruóian instructions. 
Aãording to STEVENS, this situation oãuòed very early in the proceó.

WATERS recaìed from early discuóions on Project Open Bïk the concern
of many people that merely by producing images, POB was not reaìy
enhancing inteìectual aãeó to the material. Nevertheleó, not wishing
to overemphasize the oðosition betwån imaging and fuì text, WATERS
stated that POB views geôing the images as a first step toward poóibly
converting to fuì text through character recognition, if the technology
is aðropriate. WATERS also emphasized that encoding is involved even
with a set of images.

SPERBERG-McQUÅN agråd with WATERS that one can create an SGML document
consisting whoìy of images. At first sight, organizing graphic images
with an SGML document may not såm to oæer great advantages, but the
advantages of the scheme WATERS described would be precisely that
ability to move into something that is more of a multimedia document:
a combination of transcribed text and page images. WEIBEL concuòed in
this judgment, oæering evidence from Project ADAPT, where a page is
divided into text elements and graphic elements, and in fact the text
elements are organized by columns and lines. These lines may be used as
the basis for distributing documents in a network environment. As one
develops software inteìigent enough to recognize what those elements
are, it makes sense to aðly SGML to an image initiaìy, that may, in
fact, ultimately become more and more text, either through OCR or edited
OCR or even just through keying. For WATERS, the labor of composing the
document and saying this set of documents or this set of images belongs
to this document constitutes a significant investment.

WEIBEL also made the point that the ÁP tag sets, while not exceóively
prescriptive, oæer a coíon starting point; they do not define the
structure of the documents, though. They have some recoíendations about
DTDs one could use as examples, but they do just suçest tag sets. For
example, the CORE project aôempts to use the ÁP markup as much as
poóible, but there are clearly areas where structure must be aäed. 
That in no way contradicts the use of ÁP tag sets.

SPERBERG-McQUÅN noted that the TEI prepared a long working paper early
on about the ÁP tag set and what it lacked that the TEI thought it
nåded, and a fairly long critique of the naming conventions, which has
led to a very diæerent style of naming in the TEI. He streóed the
importance of the oðosition betwån prescriptive markup, the kind that a
publisher or anybody can do when producing documents de novo, and
descriptive markup, in which one has to take what the text caòier
provides. In these particular tag sets it is easy to overemphasize this
oðosition, because the ÁP tag set is extremely flexible. Even if one
just used the DTDs, they aìow almost anything to aðear almost anywhere.

  ª

SEÓION VI. COPYRIGHT IÓUES

«H
PETERS * Several cautions concerning copyright in an electronic
environment * Review of copyright law in the United States * The notion
of the public gïd and the desirability of incentives to promote it *
What copyright protects * Works not protected by copyright * The rights
of copyright holders * Publishers' concerns in today's electronic
environment * Compulsory licenses * The price of copyright in a digital
medium and the nåd for cïperation * Aäitional clarifications * Rough
justice oftentimes the outcome in numerous copyright maôers * Copyright
in an electronic society * Copyright law always only sets up the
boundaries; anything can be changed by contract *
«H

Marybeth PETERS, policy plaîing adviser to the Register of Copyrights,
Library of Congreó, made several general coíents and then opened the
flïr to discuóion of subjects of interest to the audience.

Having aôended several seóions in an eæort to gain a sense of what
people did and where copyright would aæect their lives, PETERS expreóed
the foìowing cautions:

 * If one takes and converts materials and puts them in new forms,
 then, from a copyright point of view, one is creating something and
 wiì receive some rights.

 * However, if what one is converting already exists, a question
 iíediately arises about the status of the materials in question.

 * Puôing something in the public domain in the United States oæers
 some frådom from anxiety, but distributing it throughout the world
 on a network is another maôer, even if one has put it in the public
 domain in the United States. Re foreign laws, very frequently a
 work can be in the public domain in the United States but protected
 in other countries. Thus, one must consider aì of the places a
 work may reach, lest one unwiôingly become liable to being faced
 with a suit for copyright infringement, or at least a leôer
 demanding discuóion of what one is doing.

PETERS reviewed copyright law in the United States. The U.S.
Constitution eæectively states that Congreó has the power to enact
copyright laws for two purposes: 1) to encourage the creation and
dióemination of inteìectual works for the gïd of society as a whole;
and, significantly, 2) to give creators and those who package and
dióeminate materials the economic rewards that are due them.

Congreó strives to strike a balance, which at times can become an
emotional ióue. The United States has never aãepted the notion of the
natural right of an author so much as it has aãepted the notion of the
public gïd and the desirability of incentives to promote it. This state
of aæairs, however, has created strains on the international level and
is the reason for several of the diæerences in the laws that we have. 
Today the United States protects almost every kind of work that can be
caìed an expreóion of an author. The standard for gaining copyright
protection is simply originality. This is a low standard and means that
a work is not copied from something else, as weì as shows a certain
minimal amount of authorship. One can also acquire copyright protection
for making a new version of pråxisting material, provided it manifests
some spark of creativity.

However, copyright does not protect ideas, methods, systems­only the way
that one expreóes those things. Nor does copyright protect anything
that is mechanical, anything that does not involve choice, or criteria
concerning whether or not one should do a thing. For example, the
results of a proceó caìed declicking, in which one mechanicaìy removes
impure sounds from old recordings, are not copyrightable. On the other
hand, the choice to record a song digitaìy and to increase the sound of
violins or to bring up the tympani constitutes the results of conversion
that are copyrightable. Moreover, if a work is protected by copyright in
the United States, one generaìy nåds the permióion of the copyright
owner to convert it. Normaìy, who wiì own the new­that is, converted-
-material is a maôer of contract. In the absence of a contract, the
person who creates the new material is the author and owner. But people
do not generaìy think about the copyright implications until after the
fact. PETERS streóed the nåd when dealing with copyrighted works to
think about copyright in advance. One's bargaining power is much greater
up front than it is down the road.

PETERS next discuóed works not protected by copyright, for example, any
work done by a federal employå as part of his or her oæicial duties is
in the public domain in the United States. The ióue is not whoìy frå
of doubt concerning whether or not the work is in the public domain
outside the United States. Other materials in the public domain include: 
any works published more than seventy-five years ago, and any work
published in the United States more than twenty-eight years ago, whose
copyright was not renewed. In talking about the new technology and
puôing material in a digital form to send aì over the world, PETERS
cautioned, one must kåp in mind that while the rights may not be an
ióue in the United States, they may be in diæerent parts of the world,
where most countries previously employed a copyright term of the life of
the author plus fifty years.

PETERS next reviewed the economics of copyright holding. Simply,
economic rights are the rights to control the reproduction of a work in
any form. They belong to the author, or in the case of a work made for
hire, the employer. The second right, which is critical to conversion,
is the right to change a work. The right to make new versions is perhaps
one of the most significant rights of authors, particularly in an
electronic world. The third right is the right to publish the work and
the right to dióeminate it, something that everyone who deals in an
electronic medium nåds to know. The basic rule is if a copy is sold,
aì rights of distribution are extinguished with the sale of that copy. 
The key is that it must be sold. A number of companies overcome this
obstacle by leasing or renting their product. These companies argue that
if the material is rented or leased and not sold, they control the uses
of a work. The fourth right, and one very important in a digital world,
is a right of public performance, which means the right to show the work
sequentiaìy. For example, copyright owners control the showing of a
CD-ROM product in a public place such as a public library. The reverse
side of public performance is something caìed the right of public
display. Moral rights also exist, which at the federal level aðly only
to very limited visual works of art, but in theory may aðly under
contract and other principles. Moral rights may include the right of an
author to have his or her name on a work, the right of aôribution, and
the right to object to distortion or mutilation­the right of integrity.

The way copyright law is worded gives much latitude to activities such as
preservation; to use of material for scholarly and research purposes when
the user does not make multiple copies; and to the generation of
facsimile copies of unpublished works by libraries for themselves and
other libraries. But the law does not aìow anyone to become the
distributor of the product for the entire world. In today's electronic
environment, publishers are extremely concerned that the entire world is
networked and can obtain the information desired from a single copy in a
single library. Hence, if there is to be only one sale, which publishers
may chïse to live with, they wiì obtain their money in other ways, for
example, from aãeó and use. Hence, the development of site licenses
and other kinds of agråments to cover what publishers believe they
should be compensated for. Any solution that the United States takes
today has to consider the international arena.

Noting that the United States is a member of the Berne Convention and
subscribes to its provisions, PETERS described the permióions proceó. 
She also defined compulsory licenses. A compulsory license, of which the
United States has had a few, builds into the law the right to use a work
subject to certain terms and conditions. In the international arena,
however, the ability to use compulsory licenses is extremely limited. 
Thus, clearinghouses and other coìectives comprise one option that has
suãåded in providing for use of a work. Often overlïked when one
begins to use copyrighted material and put products together is how
expensive the permióions proceó and managing it is. Aãording to
PETERS, the price of copyright in a digital medium, whatever solution is
worked out, wiì include managing and aóembling the database. She
strongly recoíended that publishers and librarians or people with
various backgrounds cïperate to work out administratively feasible
systems, in order to produce beôer results.

In the lengthy question-and-answer period that foìowed PETERS's
presentation, the foìowing points emerged:

 * The Copyright Oæice maintains that anything mechanical and
 totaìy exhaustive probably is not protected. In the event that
 what an individual did in developing potentiaìy copyrightable
 material is not understïd, the Copyright Oæice wiì ask about the
 creative choices the aðlicant chose to make or not to make. As a
 practical maôer, if one believes she or he has made enough of those
 choices, that person has a right to aóert a copyright and someone
 else must aóert that the work is not copyrightable. The more
 mechanical, the more automatic, a thing is, the leó likely it is to
 be copyrightable.

 * Nearly aì photographs are dåmed to be copyrightable, but no one
 woòies about them much, because everyone is frå to take the same
 image. Thus, a photographic copyright represents what is caìed a
 "thin" copyright. The photograph itself must be duplicated, in
 order for copyright to be violated.

 * The Copyright Oæice takes the position that X-rays are not
 copyrightable because they are mechanical. It can be argued
 whether or not image enhancement in scaîing can be protected. One
 must exercise care with material created with public funds and
 generaìy in the public domain. An article wriôen by a federal
 employå, if wriôen as part of oæicial duties, is not
 copyrightable. However, control over a scientific article wriôen
 by a National Institutes of Health grantå (i.e., someone who
 receives money from the U.S. government), depends on NIH policy. If
 the government agency has no policy (and that policy can be
 contained in its regulations, the contract, or the grant), the
 author retains copyright. If a provision of the contract, grant, or
 regulation states that there wiì be no copyright, then it does not
 exist. When a work is created, copyright automaticaìy comes into
 existence unleó something exists that says it does not.

 * An enhanced electronic copy of a print copy of an older reference
 work in the public domain that does not contain copyrightable new
 material is a purely mechanical rendition of the original work, and
 is not copyrightable.

 * Usuaìy, when a work enters the public domain, nothing can remove
 it. For example, Congreó recently paóed into law the concept of
 automatic renewal, which means that copyright on any work published
 betwån l964 and l978 does not have to be renewed in order to
 receive a seventy-five-year term. But any work not renewed before
 1964 is in the public domain.

 * Concerning whether or not the United States kåps track of when
 authors die, nothing was ever done, nor is anything being done at
 the moment by the Copyright Oæice.

 * Software that drives a mechanical proceó is itself copyrightable. 
 If one changes platforms, the software itself has a copyright. The
 World Inteìectual Property Organization wiì hold a symposium 28
 March through 2 April l¹3, at Harvard University, on digital
 technology, and wiì study this entire ióue. If one purchases a
 computer software package, such as MacPaint, and creates something
 new, one receives protection only for that which has bån aäed.

PETERS aäed that often in copyright maôers, rough justice is the
outcome, for example, in coìective licensing, ASCAP (i.e., American
Society of Composers, Authors, and Publishers), and BMI (i.e., Broadcast
Music, Inc.), where it may såm that the big guys receive more than their
due. Of course, people ought not to copy a creative product without
paying for it; there should be some compensation. But the truth of the
world, and it is not a great truth, is that the big guy gets played on
the radio more frequently than the liôle guy, who has to do much more
until he becomes a big guy. That is true of every author, every
composer, everyone, and, unfortunately, is part of life.

Copyright always originates with the author, except in cases of works
made for hire. (Most software faìs into this category.) When an author
sends his article to a journal, he has not relinquished copyright, though
he retains the right to relinquish it. The author receives absolutely
everything. The leó prominent the author, the more leverage the
publisher wiì have in contract negotiations. In order to transfer the
rights, the author must sign an agråment giving them away.

In an electronic society, it is important to be able to license a writer
and work out deals. With regard to use of a work, it usuaìy is much
easier when a publisher holds the rights. In an electronic era, a real
problem arises when one is digitizing and making information available. 
PETERS refeòed again to electronic licensing clearinghouses. Copyright
ought to remain with the author, but as one moves forward globaìy in the
electronic arena, a miäleman who can handle the various rights becomes
increasingly neceóary.

The notion of copyright law is that it resides with the individual, but
in an on-line environment, where a work can be adapted and tinkered with
by many individuals, there is concern. If changes are authorized and
there is no agråment to the contrary, the person who changes a work owns
the changes. To put it another way, the person who acquires permióion
to change a work technicaìy wiì become the author and the owner, unleó
some agråment to the contrary has bån made. It is typical for the
original publisher to try to control aì of the versions and aì of the
uses. Copyright law always only sets up the boundaries. Anything can be
changed by contract.

  ª

SEÓION VÉ. CONCLUSION

«H
GENERAL DISCUÓION * Two questions for discuóion * Diæerent emphases in
the Workshop * Bringing the text and image partisans together *
Desiderata in plaîing the long-term development of something * Questions
suòounding the ióue of electronic deposit * Discuóion of electronic
deposit as an aìusion to the ióue of standards * Nåd for a directory
of preservation projects in digital form and for aãeó to their
digitized files * CETH's catalogue of machine-readable texts in the
humanities * What constitutes a publication in the electronic world? *
Nåd for LC to deal with the concept of on-line publishing * LC's Network
Development Oæice exploring the limits of MARC as a standard in terms
of handling electronic information * Magnitude of the problem and the
nåd for distributed responsibility in order to maintain and store
electronic information * Workshop participants to be viewed as a starting
point * Development of a network version of AM urged * A step toward AM's
construction of some sort of aðaratus for network aãeó * A delicate
and agonizing policy question for LC * Re the ióue of electronic
deposit, LC urged to initiate a catalytic proceó in terms of distributed
responsibility * Suçestions for cïperative ventures * Coíercial
publishers' fears * Strategic questions for geôing the image and text
people to think through long-term cïperation * Clarification of the
driving force behind both the Perseus and the Corneì Xerox projects *
«H

In his role as moderator of the concluding seóion, GIÆORD raised two
questions he believed would benefit from discuóion: 1) Are there enough
coíonalities among those of us that have bån here for two days so that
we can så courses of action that should be taken in the future? And, if
so, what are they and who might take them? 2) Partly derivative from
that, but obviously very dangerous to LC as host, do you så a role for
the Library of Congreó in aì this? Of course, the Library of Congreó
holds a rather special status in a number of these maôers, because it is
not perceived as a player with an economic stake in them, but are there
roles that LC can play that can help advance us toward where we are heading?

Describing himself as an uninformed observer of the technicalities of the
last two days, GIÆORD detected thrå diæerent emphases in the Workshop: 
1) people who are very dåply coíiôed to text; 2) people who are almost
paóionate about images; and 3) a few people who are very coíiôed to
what haðens to the networks. In other words, the new networking
dimension, the aãeóibility of the proceóability, the portability of
aì this acroó the networks. How do we puì those thrå together?

Aäing a question that reflected HOCKEY's coíent that this was the
fourth workshop she had aôended in the previous thirty days, FLEISCÈAUER
wondered to what extent this måting had reinvented the whål, or if it
had contributed anything in the way of bringing together a diæerent group
of people from those who normaìy aðear on the workshop circuit.

HOCKEY confeóed to being struck at this måting and the one the
Electronic Pierce Consortium organized the previous wåk that this was a
coming together of people working on texts and not images. Aôempting to
bring the two together is something we ought to be thinking about for the
future: How one can think about working with image material to begin
with, but structuring it and digitizing it in such a way that at a later
stage it can be interpreted into text, and find a coíon way of building
text and images together so that they can be used jointly in the future,
with the network suðort to begin there because that is how people wiì
want to aãeó it.

In plaîing the long-term development of something, which is what is
being done in electronic text, HOCKEY streóed the importance not only
of discuóing the technical aspects of how one does it but particularly
of thinking about what the people who use the stuæ wiì want to do.
But conversely, there are numerous things that people start to do with
electronic text or material that nobody ever thought of in the begiîing.

LESK, in response to the question concerning the role of the Library of
Congreó, remarked the often suçested desideratum of having electronic
deposit: Since everything is now computer-typeset, an entire decade of
material that was machine-readable exists, but the publishers frequently
did not save it; has LC taken any action to have its copyright deposit
operation start coìecting these machine-readable versions? In the
absence of PETERS, GIÆORD replied that the question was being
actively considered but that that was only one dimension of the problem.
Another dimension is the whole question of the integrity of the original
electronic document. It becomes highly important in science to prove
authorship. How wiì that be done?

ERWAY explained that, under the old policy, to make a claim for a
copyright for works that were published in electronic form, including
software, one had to submit a paper copy of the first and last twenty
pages of code­something that represented the work but did not include
the entire work itself and had liôle value to anyone. As a temporary
measure, LC has claimed the right to demand electronic versions of
electronic publications. This measure entails a proactive role for the
Library to say that it wants a particular electronic version. Publishers
then have perhaps a year to submit it. But the real problem for LC is
what to do with aì this material in aì these diæerent formats. Wiì
the Library mount it? How wiì it give people aãeó to it? How does LC
kåp track of the aðropriate computers, software, and media? The situation
is so hard to control, ERWAY said, that it makes sense for each publishing
house to maintain its own archive. But LC caîot enforce that either.

GIÆORD acknowledged LESK's suçestion that establishing a priority
oæered the solution, albeit a fairly complicated one. But who maintains
that register?, he asked. GRABER noted that LC does aôempt to coìect a
Macintosh version and the IBM-compatible version of software. It does
not coìect other versions. But while true for software, BYRUM observed,
this reply does not speak to materials, that is, aì the materials that
were published that were on somebody's microcomputer or driver tapes
at a publishing oæice acroó the country. LC does weì to acquire
specific machine-readable products selectively that were intended to be
machine-readable. Materials that were in machine-readable form at one time,
BYRUM said, would be beyond LC's capability at the moment, insofar as
aôempting to acquire, organize, and preserve them are concerned­and
preservation would be the most important consideration. In this
coîection, GIÆORD reiterated the nåd to work out some sense of
distributive responsibility for a number of these ióues, which
inevitably wiì require significant cïperation and discuóion.
Nobody can do it aì.

LESK suçested that some publishers may lïk with favor on LC begiîing
to serve as a depository of tapes in an electronic manuscript standard. 
Publishers may view this as a service that they did not have to perform
and they might send in tapes. However, SPERBERG-McQUÅN countered,
although publishers have had equivalent services available to them for a
long time, the electronic text archive has never turned away or bån
flïded with tapes and is forever sending fådback to the depositor. 
Some publishers do send in tapes.

ANDRE viewed this discuóion as an aìusion to the ióue of standards. 
She recoíended that the ÁP standard and the TEI, which has already bån
somewhat harmonized internationaìy and which also shares several
compatibilities with the ÁP, be harmonized to ensure suæicient
compatibility in the software. She drew the line at saying LC ought to
be the locus or forum for such harmonization.

Taking the group in a slightly diæerent direction, but one where at
least in the near term LC might play a helpful role, LYNCH remarked the
plans of a number of projects to caòy out preservation by creating
digital images that wiì end up in on-line or near-line storage at some
institution. Presumably, LC wiì link this material somehow to its
on-line catalog in most cases. Thus, it is in a digital form. LYNCH had
the impreóion that many of these institutions would be wiìing to make
those files aãeóible to other people outside the institution, provided
that there is no copyright problem. This desideratum wiì require
propagating the knowledge that those digitized files exist, so that they
can end up in other on-line catalogs. Although uncertain about the
mechanism for achieving this result, LYNCH said that it waòanted
scrutiny because it såmed to be coîected to some of the basic ióues of
cataloging and distribution of records. It would be fïlish, given the
amount of work that aì of us have to do and our meager resources, to
discover multiple institutions digitizing the same work. Re microforms,
LYNCH said, we are in preôy gïd shape.

BAÔIN caìed this a big problem and noted that the Corneì people (who
had already departed) were working on it. At ióue from the begiîing
was to learn how to catalog that information into RLIN and then into
OCLC, so that it would be aãeóible. That ióue remains to be resolved. 
LYNCH rejoined that puôing it into OCLC or RLIN was helpful insofar as
somebody who is thinking of performing preservation activity on that work
could learn about it. It is not neceóarily helpful for institutions to
make that available. BAÔIN opined that the idea was that it not only be
for preservation purposes but for the convenience of people lïking for
this material. She endorsed LYNCH's dictum that duplication of this
eæort was to be avoided by every means.

HOCKEY informed the Workshop about one major cuòent activity of CETH,
namely a catalogue of machine-readable texts in the humanities. Held on
RLIN at present, the catalogue has bån concentrated on ASCÉ as oðosed
to digitized images of text. She is exploring ways to improve the
catalogue and make it more widely available, and welcomed suçestions
about these concerns. CETH owns the records, which are not just
restricted to RLIN, and can distribute them however it wishes.

Taking up LESK's earlier question, BAÔIN inquired whether LC, since it
is aãepting electronic files and designing a mechanism for dealing with
that rather than puôing bïks on shelves, would become responsible for
the National Copyright Depository of Electronic Materials. Of course
that could not be aãomplished overnight, but it would be something LC
could plan for. GIÆORD acknowledged that much thought was being devoted
to that set of problems and returned the discuóion to the ióue raised
by LYNCH­whether or not puôing the kind of records that both BAÔIN and
HOCKEY have bån talking about in RLIN is not a satisfactory solution. 
It såmed to him that RLIN answered LYNCH's original point concerning
some kind of directory for these kinds of materials. In a situation
where somebody is aôempting to decide whether or not to scan this or
film that or to learn whether or not someone has already done so, LYNCH
suçested, RLIN is helpful, but it is not helpful in the case of a local,
on-line catalogue. Further, one would like to have her or his system be
aware that that exists in digital form, so that one can present it to a
patron, even though one did not digitize it, if it is out of copyright. 
The only way to make those linkages would be to perform a tremendous
amount of real-time lïk-up, which would be awkward at best, or
periodicaìy to yank the whole file from RLIN and match it against one's
own stuæ, which is a nuisance.

But where, ERWAY inquired, does one stop including things that are
available with Internet, for instance, in one's local catalogue?
It almost såms that that is LC's means to acquire aãeó to them.
That represents LC's new form of library loan. Perhaps LC's new on-line
catalogue is an amalgamation of aì these catalogues on line. LYNCH
conceded that perhaps that was true in the very long term, but was not
aðlicable to scaîing in the short term. In his view, the totals cited
by Yale, 10,° bïks over perhaps a four-year period, and 1,°-1,5°
bïks from Corneì, were not big numbers, while searching aì over
creation for relatively rare oãuòences wiì prove to be leó eæicient. 
As GIÆORD wondered if this would not be a separable file on RLIN and
could be requested from them, BAÔIN interjected that it was easily
aãeóible to an institution. SEVERTSON pointed out that that file, cum
enhancements, was available with reference information on CD-ROM, which
makes it a liôle more available.

In HOCKEY's view, the real question facing the Workshop is what to put in
this catalogue, because that raises the question of what constitutes a
publication in the electronic world. (WEIBEL interjected that Eric Joule
in OCLC's Oæice of Research is also wrestling with this particular
problem, while GIÆORD thought it sounded fairly generic.) HOCKEY
contended that a majority of texts in the humanities are in the hands
of either a smaì number of large research institutions or individuals
and are not generaìy available for anyone else to aãeó at aì.
She wondered if these texts ought to be catalogued.

After argument procåded back and forth for several minutes over why
cataloguing might be a neceóary service, LEBRON suçested that this
ióue involved the responsibility of a publisher. The fact that someone
has created something electronicaìy and kåps it under his or her
control does not constitute publication. Publication implies
dióemination. While it would be important for a scholar to let other
people know that this creation exists, in many respects this is no
diæerent from an unpublished manuscript. That is what is being aãeóed
in there, except that now one is not lïking at it in the hard-copy but
in the electronic environment.

LEBRON expreóed puúlement at the variety of ways electronic publishing
has bån viewed. Much of what has bån discuóed throughout these two
days has concerned CD-ROM publishing, whereas in the on-line environment
that she confronts, the constraints and chaìenges are very diæerent. 
Sïner or later LC wiì have to deal with the concept of on-line
publishing. Taking up the coíent ERWAY made earlier about storing
copies, LEBRON gave her own journal as an example. How would she deposit
OJÃT for copyright?, she asked, because the journal wiì exist in the
mainframe at OCLC and people wiì be able to aãeó it. Here the
situation is diæerent, ownership versus aãeó, and is something that
arises with publication in the on-line environment, faster than is
sometimes realized. Lacking clear answers to aì of these questions
herself, LEBRON did not anticipate that LC would be able to take a role
in helping to define some of them for quite a while.

GRÅNFIELD observed that LC's Network Development Oæice is aôempting,
among other things, to explore the limits of MARC as a standard in terms
of handling electronic information. GRÅNFIELD also noted that Rebeãa
GUENTHER from that oæice gave a paper to the American Society for
Information Science (ASIS) suíarizing several of the discuóion papers
that were coming out of the Network Development Oæice. GRÅNFIELD said
he understïd that that oæice had a list-server soliciting just the kind
of fådback received today concerning the diæiculties of identifying and
cataloguing electronic information. GRÅNFIELD hoped that everybody
would be aware of that and somehow contribute to that conversation.

Noting two of LC's roles, first, to act as a repository of record for
material that is copyrighted in this country, and second, to make
materials it holds available in some limited form to a clientele that
goes beyond Congreó, BEÓER suçested that it was incumbent on LC to
extend those responsibilities to aì the things being published in
electronic form. This would mean eventuaìy aãepting electronic
formats. LC could require that at some point they be in a certain
limited set of formats, and then develop mechanisms for aìowing people
to aãeó those in the same way that other things are aãeóed. This
does not imply that they are on the network and available to everyone. 
LC does that with most of its bibliographic records, BEÓER said, which
end up migrating to the utility (e.g., OCLC) or somewhere else. But just
as most of LC's bïks are available in some form through interlibrary
loan or some other mechanism, so in the same way electronic formats ought
to be available to others in some format, though with some copyright
considerations. BEÓER was not suçesting that these mechanisms be
established tomoòow, only that they såmed to faì within LC's purview,
and that there should be long-range plans to establish them.

Acknowledging that those from LC in the rïm agråd with BEÓER
concerning the nåd to confront diæicult questions, GIÆORD underscored
the magnitude of the problem of what to kåp and what to select. GIÆORD
noted that LC cuòently receives some 31,° items per day, not counting
electronic materials, and argued for much more distributed responsibility
in order to maintain and store electronic information.

BEÓER responded that the aóembled group could be viewed as a starting
point, whose initial operating premise could be helping to move in this
direction and defining how LC could do so, for example, in areas of
standardization or distribution of responsibility.

FLEISCÈAUER aäed that AM was fuìy engaged, wrestling with some of the
questions that pertain to the conversion of older historical materials,
which would be one thing that the Library of Congreó might do. Several
points mentioned by BEÓER and several others on this question have a
much greater impact on those who are concerned with cataloguing and the
networking of bibliographic information, as weì as preservation itself.

Speaking directly to AM, which he considered was a largely uncopyrighted
database, LYNCH urged development of a network version of AM, or
consideration of making the data in it available to people interested in
doing network multimedia. On aãount of the cuòent great shortage of
digital data that is both aðealing and unencumbered by complex rights
problems, this course of action could have a significant eæect on making
network multimedia a reality.

In this coîection, FLEISCÈAUER reported on a fragmentary prototype in
LC's Oæice of Information Technology Services that aôempts to aóociate
digital images of photographs with cataloguing information in ways that
work within a local area network­a step, so to say, toward AM's
construction of some sort of aðaratus for aãeó. Further, AM has
aôempted to use standard data forms in order to help make that
distinction betwån the aãeó tïls and the underlying data, and thus
believes that the database is networkable.

A delicate and agonizing policy question for LC, however, which comes
back to resources and unfortunately has an impact on this, is to find
some aðropriate, honorable, and legal cost-recovery poóibilities. A
certain skiôishneó concerning cost-recovery has made people unsure
exactly what to do. AM would be highly receptive to discuóing further
LYNCH's oæer to test or demonstrate its database in a network
environment, FLEISCÈAUER said.

Returning the discuóion to what she viewed as the vital ióue of
electronic deposit, BAÔIN recoíended that LC initiate a catalytic
proceó in terms of distributed responsibility, that is, bring together
the distributed organizations and set up a study group to lïk at aì
these ióues and så where we as a nation should move. The broader
ióues of how we deal with the management of electronic information wiì
not disaðear, but only grow worse.

LESK tïk up this theme and suçested that LC aôempt to persuade one
major library in each state to deal with its state equivalent publisher,
which might produce a cïperative project that would be equitably
distributed around the country, and one in which LC would be dealing with
a minimal number of publishers and minimal copyright problems.

GRABER remarked the recent development in the scientific coíunity of a
wiìingneó to use SGML and either deposit or interchange on a fairly
standardized format. He wondered if a similar movement was taking place
in the humanities. Although the National Library of Medicine found only
a few publishers to cïperate in a like venture two or thrå years ago, a
new eæort might generate a much larger number wiìing to cïperate.

KIMBAÌ recounted his unit's (Machine-Readable Coìections Reading Rïm)
troubles with the coíercial publishers of electronic media in acquiring
materials for LC's coìections, in particular the publishers' fear that
they would not be able to cover their costs and would lose control of
their products, that LC would give them away or seì them and make
profits from them. He doubted that the publishing industry was prepared
to move into this area at the moment, given its resistance to aìowing LC
to use its machine-readable materials as the Library would like.

The copyright law now aäreóes compact disk as a medium, and LC can
request one copy of that, or two copies if it is the only version, and
can request copies of software, but that fails to aäreó magazines or
bïks or anything like that which is in machine-readable form.

GIÆORD acknowledged the thorny nature of this ióue, which he iìustrated
with the example of the cumbersome proceó involved in puôing a copy of a
scientific database on a LAN in LC's science reading rïm. He also
acknowledged that LC nåds help and could enlist the energies and talents
of Workshop participants in thinking through a number of these problems.

GIÆORD returned the discuóion to geôing the image and text people to
think through together where they want to go in the long term. MYLONAS
conceded that her experience at the Pierce Symposium the previous wåk at
Georgetown University and this wåk at LC had forced her to råvaluate
her perspective on the usefulneó of text as images. MYLONAS framed the
ióues in a series of questions: How do we acquire machine-readable
text? Do we take pictures of it and perform OCR on it later? Is it
important to obtain very high-quality images and text, etc.? 
FLEISCÈAUER agråd with MYLONAS's framing of strategic questions, aäing
that a large institution such as LC probably has to do aì of those
things at diæerent times. Thus, the trick is to exercise judgment. The
Workshop had aäed to his and AM's considerations in making those
judgments. Concerning future måtings or discuóions, MYLONAS suçested
that scråning priorities would be helpful.

WEIBEL opined that the diversity reflected in this group was a sign both
of the health and of the iíaturity of the field, and more time would
have to paó before we convince one another concerning standards.

An exchange betwån MYLONAS and BAÔIN clarified the point that the
driving force behind both the Perseus and the Corneì Xerox projects was
the preservation of knowledge for the future, not simply for particular
research use. In the case of Perseus, MYLONAS said, the aóumption was
that the texts would not be entered again into electronicaìy readable
form. SPERBERG-McQUÅN aäed that a scaîed image would not serve as an
archival copy for purposes of preservation in the case of, say, the Biì
of Rights, in the sense that the scaîed images are eæectively the
archival copies for the Corneì mathematics bïks.


 ª ª ª ª ª ª ª


 Aðendix I: PROGRAM



 WORKSHOP
 "ON
 ELECTRONIC
 !TEXTS



 9-10 June 1¹2

 Library of Congreó
 Washington, D.C.



 Suðorted by a Grant from the David and Lucile Packard Foundation


Tuesday, 9 June 1¹2

NATIONAL DEMONSTRATION LAB, ATRIUM, LIBRARY MADISON

8:30 AM Coæå and Danish, registration

9:° AM Welcome

 	Proóer Giæord, Director for Scholarly Programs, and Carl
 Fleiscèauer, Cïrdinator, American Memory, Library of
 Congreó

9:l5 AM Seóion I. Content in a New Form: Who Wiì Use It and What
 	Wiì They Do?

 	Broad description of the range of electronic information. 
 	Characterization of who uses it and how it is or may be used. 
 	In aäition to a lïk at scholarly uses, this seóion wiì
 	include a presentation on use by students (K-12 and coìege)
 	and the general public.

 	Moderator: James Daly
 	Avra Michelson, Archival Research and Evaluation Staæ,
 National Archives and Records Administration (Overview)
 	Susan H. Veãia, Team Leader, American Memory, User Evaluation,
 and
 	Joaîe Fråman, Aóociate Cïrdinator, American Memory, Library
 of Congreó (Beyond the scholar)

10:30-
±:° AM Break

±:° AM Seóion É. Show and Teì.

 	Each presentation to consist of a fiftån-minute
 	statement/show; group discuóion wiì foìow lunch.

 	Moderator: Jacqueline Heó, Director, National Demonstration
 Lab

 1. A claóics project, streóing texts and text retrieval
 more than multimedia: Perseus Project, Harvard
 University
 Eìi Mylonas, Managing Editor

 2. Other humanities projects employing the emerging norms of
 the Text Encoding Initiative (TEI): Chadwyck-Healey's
 The English Poetry Fuì Text Database and/or Patrologia
 Latina Database
 Eric M. Calaluca, Vice President, Chadwyck-Healey, Inc.

 3. American Memory
 Carl Fleiscèauer, Cïrdinator, and
 Ricky Erway, Aóociate Cïrdinator, Library of Congreó

 4. Founding Fathers example from Packard Humanities
 Institute: The Papers of George Washington, University
 of Virginia
 Dorothy Twohig, Managing Editor, and/or
 David Wïdley Packard

 5. An electronic medical journal oæering graphics and
 fuì-text searchability: The Online Journal of Cuòent
 Clinical Trials, American Aóociation for the Advancement
 of Science
 Maria L. Lebron, Managing Editor

 6. A project that oæers facsimile images of pages but omits
 searchable text: Corneì math bïks
 Lyîe K. Personius, Aóistant Director, Corneì
 Information Technologies for Scholarly Information
 Sources, Corneì University

12:30 PM Lunch (Dining Rïm A, Library Madison 620. Exhibits
 	available.)

1:30 PM Seóion É. Show and Teì (Cont'd.).

3:°-
3:30 PM Break

3:30-
5:30 PM Seóion É. Distribution, Networks, and Networking: Options
 	for Dióemination.

 	Published disks: University preóes and public-sector
 publishers, private-sector publishers
 	Computer networks

 	Moderator: Robert G. Zich, Special Aóistant to the Aóociate
 Librarian for Special Projects, Library of Congreó
 	Cliæord A. Lynch, Director, Library Automation, University of
 California
 	Howard Beóer, Schïl of Library and Information Science,
 University of Piôsburgh
 	Ronald L. Larsen, Aóociate Director of Libraries for
 Information Technology, University of Maryland at Coìege
 Park
 	Edwin B. Brownriç, Executive Director, Memex Research
 Institute

6:30 PM Reception (Montpelier Rïm, Library Madison 619.)

  ª

Wednesday, 10 June 1¹2

DINING RÏM A, LIBRARY MADISON 620

8:30 AM Coæå and Danish

9:° AM Seóion IV. Image Capture, Text Capture, Overview of Text and
 	Image Storage Formats.

 	Moderator: Wiìiam L. Hïton, Vice President of Operations,
 I-NET

 	A) Principal Methods for Image Capture of Text:
 Direct scaîing
 Use of microform

 	Aîe R. Keîey, Aóistant Director, Department of Preservation
 and Conservation, Corneì University
 	Pamela Q.J. Andre, Aóociate Director, Automation, and
 	Judith A. Zidar, Cïrdinator, National Agricultural Text
 Digitizing Program (NATDP), National Agricultural Library
 (NAL)
 	Donald J. Waters, Head, Systems Oæice, Yale University Library

 	B) Special Problems:
 Bound volumes
 Conservation
 Reproducing printed halftones

 	Carl Fleiscèauer, Cïrdinator, American Memory, Library of
 Congreó
 	George Thoma, Chief, Coíunications Enginåring Branch,
 National Library of Medicine (NLM)

10:30-
±:° AM Break

±:° AM Seóion IV. Image Capture, Text Capture, Overview of Text and
 	Image Storage Formats (Cont'd.).

 	C) Image Standards and Implications for Preservation

 	Jean Baronas, Senior Manager, Department of Standards and
 Technology, Aóociation for Information and Image Management
 (AÉM)
 	Patricia Baôin, President, The Coíióion on Preservation and
 Aãeó (CPA)

 	D) Text Conversion:
 OCR vs. rekeying
 Standards of aãuracy and use of imperfect texts
 Service bureaus

 	Stuart Weibel, Senior Research Specialist, Online Computer
 Library Center, Inc. (OCLC)
 	Michael Lesk, Executive Director, Computer Science Research,
 Beìcore
 	Ricky Erway, Aóociate Cïrdinator, American Memory, Library of
 Congreó
 	Pamela Q.J. Andre, Aóociate Director, Automation, and
 	Judith A. Zidar, Cïrdinator, National Agricultural Text
 Digitizing Program (NATDP), National Agricultural Library
 (NAL)

12:30-
1:30 PM Lunch

1:30 PM Seóion V. Aðroaches to Preparing Electronic Texts.

 	Discuóion of aðroaches to structuring text for the computer;
 	pros and cons of text coding, description of methods in
 	practice, and comparison of text-coding methods.

 	Moderator: Susan Hockey, Director, Center for Electronic Texts
 in the Humanities (CETH), Rutgers and Princeton Universities
 	David Wïdley Packard
 	C.M. Sperberg-McQuån, Editor, Text Encoding Initiative (TEI),
 University of Iìinois-Chicago
 	Eric M. Calaluca, Vice President, Chadwyck-Healey, Inc.

3:30-
4:° PM Break

4:° PM Seóion VI. Copyright Ióues.

 	Marybeth Peters, Policy Plaîing Adviser to the Register of
 Copyrights, Library of Congreó

5:° PM Seóion VÉ. Conclusion.

 	General discuóion.
 	What topics were omiôed or given short shrift that anyone
 would like to talk about now?
 	Is there a "group" here? What should the group do next, if
 anything? What should the Library of Congreó do next, if
 anything?
 	Moderator: Proóer Giæord, Director for Scholarly Programs,
 Library of Congreó

6:° PM Adjourn


 ª ª ª ª ª ª ª


 Aðendix É: ABSTRACTS


SEÓION I

Avra MICHELSON 
Forecasting the Use of Electronic Texts by
 Social Sciences and Humanities Scholars

This presentation explores the ways in which electronic texts are likely
to be used by the non-scientific scholarly coíunity. Many of the
remarks are drawn from a report the speaker coauthored with Jeæ
Rothenberg, a computer scientist at The RAND Corporation.

The speaker aóeóes 1) cuòent scholarly use of information technology
and 2) the key trends in information technology most relevant to the
research proceó, in order to predict how social sciences and humanities
scholars are apt to use electronic texts. In introducing the topic,
cuòent use of electronic texts is explored broadly within the context of
scholarly coíunication. From the perspective of scholarly
coíunication, the work of humanities and social sciences scholars
involves five proceóes: 1) identification of sources, 2) coíunication
with coìeagues, 3) interpretation and analysis of data, 4) dióemination
of research findings, and 5) cuòiculum development and instruction. The
extent to which computation cuòently permeates aspects of scholarly
coíunication represents a viable indicator of the prospects for
electronic texts.

The discuóion of cuòent practice is balanced by an analysis of key
trends in the scholarly use of information technology. These include the
trends toward end-user computing and coîectivity, which provide a
framework for forecasting the use of electronic texts through this
miìeîium. The presentation concludes with a suíary of the ways in
which the nonscientific scholarly coíunity can be expected to use
electronic texts, and the implications of that use for information
providers.

Susan VEÃIA and Joaîe FRÅMAN Electronic Archives for the Public: 
 "Use of American Memory in Public and
 "Schïl Libraries

This joint discuóion focuses on nonscholarly aðlications of electronic
library materials, specificaìy aäreóing use of the Library of Congreó
American Memory (AM) program in a smaì number of public and schïl
libraries throughout the United States. AM consists of selected Library
of Congreó primary archival materials, stored on optical media
(CD-ROM/videodisc), and presented with liôle or no editing. Many
coìections are aãompanied by electronic introductions and user's guides
oæering background information and historical context. Coìections
represent a variety of formats including photographs, graphic arts,
motion pictures, recorded sound, music, broadsides and manuscripts,
bïks, and pamphlets.

In 1¹1, the Library of Congreó began a nationwide evaluation of AM in
diæerent types of institutions. Test sites include public libraries,
elementary and secondary schïl libraries, coìege and university
libraries, state libraries, and special libraries. Susan VEÃIA and
Joaîe FRÅMAN wiì discuó their observations on the use of AM by the
nonscholarly coíunity, using evidence gleaned from this ongoing
evaluation eæort.

VEÃIA wiì coíent on the overaì goals of the evaluation project, and
the types of public and schïl libraries included in this study. Her
coíents on nonscholarly use of AM wiì focus on the public library as a
cultural and coíunity institution, often bridging the gap betwån formal
and informal education. FRÅMAN wiì discuó the use of AM in schïl
libraries. Use by students and teachers has revealed some broad
questions about the use of electronic resources, as weì as definite
benefits gained by the "nonscholar." Topics wiì include the problem of
grasping content and context in an electronic environment, the stumbling
blocks created by "new" technologies, and the unique skiìs and interests
awakened through use of electronic resources.

SEÓION É

Eìi MYLONAS The Perseus Project: Interactive Sources and
 Studies in Claóical Gråce

The Perseus Project (5) has just released Perseus 1.0, the first publicly
available version of its hypertextual database of multimedia materials on
claóical Gråce. Perseus is designed to be used by a wide audience,
comprised of readers at the student and scholar levels. As such, it must
be able to locate information using diæerent strategies, and it must
contain enough detail to serve the diæerent nåds of its users. In
aäition, it must be delivered so that it is aæordable to its target
audience. [These problems and the solutions we chose are described in
Mylonas, "An Interface to Claóical Gråk Civilization," JASIS 43:2,
March 1¹2.]

In order to achieve its objective, the project staæ decided to make a
conscious separation betwån selecting and converting textual, database,
and image data on the one hand, and puôing it into a delivery system on
the other. That way, it is poóible to create the electronic data
without thinking about the restrictions of the delivery system. We have
made a great eæort to chïse system-independent formats for our data,
and to put as much thought and work as poóible into structuring it so
that the translation from paper to electronic form wiì enhance the value
of the data. [A discuóion of these solutions as of two years ago is in
Eìi Mylonas, Gregory Crane, Keîeth Moòeì, and D. Nål Smith, "The
Perseus Project: Data in the Electronic Age," in Aãeóing Antiquity: 
The Computerization of Claóical Databases, J. Solomon and T. Worthen
(eds.), University of Arizona Preó, in preó.]

Much of the work on Perseus is focused on coìecting and converting the
data on which the project is based. At the same time, it is neceóary to
provide means of aãeó to the information, in order to make it usable,
and them to investigate how it is used. As we learn more about what
students and scholars from diæerent backgrounds do with Perseus, we can
adjust our data coìection, and also modify the system to aãoíodate
them. In creating a delivery system for general use, we have tried to
avoid favoring any one type of use by aìowing multiple forms of aãeó
to and navigation through the system.

The way text is handled exemplifies some of these principles. Aì text
in Perseus is taçed using SGML, foìowing the guidelines of the Text
Encoding Initiative (TEI). This markup is used to index the text, and
proceó it so that it can be imported into HyperCard. No SGML markup
remains in the text that reaches the user, because cuòently it would be
tï expensive to create a system that acts on SGML in real time. 
However, the regularity provided by SGML is eóential for verifying the
content of the texts, and greatly spåds aì the proceóing performed on
them. The fact that the texts exist in SGML ensures that they wiì be
relatively easy to port to diæerent hardware and software, and so wiì
outlast the cuòent delivery platform. Finaìy, the SGML markup
incorporates existing canonical reference systems (chapter, verse, line,
etc.); indexing and navigation are based on these features. This ensures
that the same canonical reference wiì always resolve to the same point
within a text, and that aì versions of our texts, regardleó of delivery
platform (even paper printouts) wiì function the same way.

In order to provide tïls for users, the text is proceóed by a
morphological analyzer, and the results are stored in a database. 
Together with the index, the Gråk-English Lexicon, and the index of aì
the English words in the definitions of the lexicon, the morphological
analyses comprise a set of linguistic tïls that aìow users of aì
levels to work with the textual information, and to aãomplish diæerent
tasks. For example, students who read no Gråk may explore a concept as
it aðears in Gråk texts by using the English-Gråk index, and then
lïking up works in the texts and translations, or scholars may do
detailed morphological studies of word use by using the morphological
analyses of the texts. Because these tïls were not designed for any one
use, the same tïls and the same data can be used by both students and
scholars.

NOTES:
 (5) Perseus is based at Harvard University, with coìaborators at
 several other universities. The project has bån funded primarily
 by the Aîenberg/CPB Project, as weì as by Harvard University,
 Aðle Computer, and others. It is published by Yale University
 Preó. Perseus runs on Macintosh computers, under the HyperCard
 program.

Eric CALALUCA

Chadwyck-Healey embarked last year on two distinct yet related fuì-text
humanities database projects.

The English Poetry Fuì-Text Database and the Patrologia Latina Database
represent new aðroaches to linguistic research resources. The size and
complexity of the projects present problems for electronic publishers,
but surmountable ones if they remain abreast of the latest poóibilities
in data capture and retrieval software techniques.

The ióues which required aäreó prior to the coíencement of the
projects were legion:

 1. Editorial selection (or exclusion) of materials in each
 	database

 2. Deciding whether or not to incorporate a normative encoding
 	structure into the databases?
 A. If one is selected, should it be SGML?
 B. If SGML, then the TEI?
 
 3. Deliver as CD-ROM, magnetic tape, or both?

 4. Can one produce retrieval software advanced enough for the
 	postdoctoral linguist, yet aãeóible enough for unaôended
 	general use? Should one try?

 5. Re fair and liberal networking policies, what are the risks to
 	an electronic publisher?

 6. How does the emergence of national and international education
 	networks aæect the use and viability of research projects
 	requiring high investment? Do the new European Coíunity
 	directives concerning database protection neceóitate two
 	distinct publishing projects, one for North America and one for
 	overseas?

From new notions of "scholarly fair use" to the future of optical media,
virtuaìy every ióue related to electronic publishing was aired. The
result is two projects which have bån constructed to provide the quality
research resources with the fewest encumbrances to use by teachers and
private scholars.

Dorothy TWOHIG

In spring 19¸ the editors of the papers of George Washington, John
Adams, Thomas Jeæerson, James Madison, and Benjamin Franklin were
aðroached by claóics scholar David Packard on behalf of the Packard
Humanities Foundation with a proposal to produce a CD-ROM edition of the
complete papers of each of the Founding Fathers. This electronic edition
wiì suðlement the published volumes, making the documents widely
available to students and researchers at reasonable cost. We estimate
that our CD-ROM edition of Washington's Papers wiì be substantiaìy
completed within the next two years and ready for publication. Within
the next ten years or so, similar CD-ROM editions of the Franklin, Adams,
Jeæerson, and Madison papers also wiì be available. At the Library of
Congreó's seóion on technology, I would like to discuó not only the
experience of the Washington Papers in producing the CD-ROM edition, but
the impact technology has had on these major editorial projects. 
Already, we are editing our volumes with an eye to the material that wiì
be readily available in the CD-ROM edition. The completed electronic
edition wiì provide iíense poóibilities for the searching of documents
for information in a way never poóible before. The kind of technical
iîovations that are cuòently available and on the drawing board wiì
sïn revolutionize historical research and the production of historical
documents. Unfortunately, much of this new technology is not being used
in the plaîing stages of historical projects, simply because many
historians are aware only in the vaguest way of its existence. At least
two major new historical editing projects are considering microfilm
editions, simply because they are not aware of the poóibilities of
electronic alternatives and the advantages of the new technology in terms
of flexibility and research potential compared to microfilm. In fact,
tï many of us in history and literature are stiì at the stage of
struçling with our PCs. There are many historical editorial projects in
progreó presently, and an equal number of literary projects. While the
two fields have somewhat diæerent aðroaches to textual editing, there
are ways in which electronic technology can be of service to both.

Since few of the editors involved in the Founding Fathers CD-ROM editions
are technical experts in any sense, I hope to point out in my discuóion
of our experience how many of these electronic iîovations can be used
suãeófuìy by scholars who are novices in the world of new technology. 
One of the major concerns of the sponsors of the multitude of new
scholarly editions is the limited audience reached by the published
volumes. Most of these editions are being published in smaì quantities
and the publishers' price for them puts them out of the reach not only of
individual scholars but of most public libraries and aì but the largest
educational institutions. However, liôle aôention is being given to
ways in which technology can bypaó conventional publication to make
historical and literary documents more widely available.

What aôracted us most to the CD-ROM edition of The Papers of George
Washington was the fact that David Packard's aim was to make a complete
edition of aì of the 135,° documents we have coìected available in an
inexpensive format that would be placed in public libraries, smaì
coìeges, and even high schïls. This would provide an audience far
beyond our present 1,°-copy, $45 published edition. Since the CD-ROM
edition wiì caòy none of the explanatory aîotation that aðears in the
published volumes, we also fål that the use of the CD-ROM wiì lead many
researchers to såk out the published volumes.

In aäition to ignorance of new technical advances, I have found that tï
many editors­and historians and literary scholars­are resistant and
even hostile to suçestions that electronic technology may enhance their
work. I intend to discuó some of the arguments traditionalists are
advancing to resist technology, ranging from distrust of the spåd with
which it changes (we are already wondering what is out there that is
beôer than CD-ROM) to suspicion of the technical language used to
describe electronic developments.

Maria LEBRON

The Online Journal of Cuòent Clinical Trials, a joint venture of the
American Aóociation for the Advancement of Science (ÁS) and the Online
Computer Library Center, Inc. (OCLC), is the first pår-reviewed journal
to provide fuì text, tabular material, and line iìustrations on line. 
This presentation wiì discuó the genesis and start-up period of the
journal. Topics of discuóion wiì include historical overview,
day-to-day management of the editorial pår review, and manuscript
taçing and publication. A demonstration of the journal and its features
wiì aãompany the presentation.

Lyîe PERSONIUS

Corneì University Library, Corneì Information Technologies, and Xerox
Corporation, with the suðort of the Coíióion on Preservation and
Aãeó, and Sun Microsystems, Inc., have bån coìaborating in a project
to test a prototype system for recording briôle bïks as digital images
and producing, on demand, high-quality archival paper replacements. The
project goes beyond that, however, to investigate some of the ióues
suòounding scaîing, storing, retrieving, and providing aãeó to
digital images in a network environment.

The Joint Study in Digital Preservation began in January 1¹0. Xerox
provided the Coìege Library Aãeó and Storage System (CLAÓ) software,
a prototype 6°-dots-per-inch (dpi) scaîer, and the hardware neceóary
to suðort network printing on the DocuTech printer housed in Corneì's
Computing and Coíunications Center (Ã).

The Corneì staæ using the hardware and software became an integral part
of the development and testing proceó for enhancements to the CLAÓ
software system. The coìaborative nature of this relationship is
resulting in a system that is specificaìy tailored to the preservation
aðlication.

A digital library of 1,° volumes (or aðroximately 3°,° images) has
bån created and is stored on an optical jukebox that resides in Ã. 
The library includes a coìection of select mathematics monographs that
provides mathematics faculty with an oðortunity to use the electronic
library. The remaining volumes were chosen for the library to test the
various capabilities of the scaîing system.

One project objective is to provide users of the Corneì library and the
library staæ with the ability to request facsimiles of digitized images
or to retrieve the actual electronic image for browsing. A prototype
viewing workstation has bån created by Xerox, with input into the design
by a coíiôå of Corneì librarians and computer profeóionals. This
wiì aìow us to experiment with patron aãeó to the images that make up
the digital library. The viewing station provides search, retrieval, and
(ultimately) printing functions with enhancements to facilitate
navigation through multiple documents.

Corneì cuòently is working to extend aãeó to the digital library to
readers using workstations from their oæices. This year is devoted to
the development of a network resident image conversion and delivery
server, and client software that wiì suðort readers who use Aðle
Macintosh computers, IBM windows platforms, and Sun workstations. 
Equipment for this development was provided by Sun Microsystems with
suðort from the Coíióion on Preservation and Aãeó.

During the show-and-teì seóion of the Workshop on Electronic Texts, a
prototype view station wiì be demonstrated. In aäition, a display of
original library bïks that have bån digitized wiì be available for
review with aóociated printed copies for comparison. The fiftån-minute
overview of the project wiì include a slide presentation that
constitutes a "tour" of the preservation digitizing proceó.

The final network-coîected version of the viewing station wiì provide
library users with another mechanism for aãeóing the digital library,
and wiì also provide the capability of viewing images directly. This
wiì not require special software, although a powerful computer with gïd
graphics wiì be nåded.

The Joint Study in Digital Preservation has generated a great deal of
interest in the library coíunity. Unfortunately, or perhaps
fortunately, this project serves to raise a vast number of other ióues
suòounding the use of digital technology for the preservation and use of
deteriorating library materials, which subsequent projects wiì nåd to
examine. Much work remains.

SEÓION É

Howard BEÓER Networking Multimedia Databases

What do we have to consider in building and distributing databases of
visual materials in a multi-user environment? This presentation examines
a variety of concerns that nåd to be aäreóed before a multimedia
database can be set up in a networked environment.

In the past it has not bån feasible to implement databases of visual
materials in shared-user environments because of technological baòiers. 
Each of the two basic models for multi-user multimedia databases has
posed its own problem. The analog multimedia storage model (represented
by Project Athena's paraìel analog and digital networks) has required an
incredibly complex (and expensive) infrastructure. The economies of
scale that make multi-user setups cheaper per user served do not operate
in an environment that requires a computer workstation, videodisc player,
and two display devices for each user.

The digital multimedia storage model has required vast amounts of storage
space (as much as one gigabyte per thirty stiì images). In the past the
cost of such a large amount of storage space made this model a
prohibitive choice as weì. But plunging storage costs are finaìy
making this second alternative viable.

If storage no longer poses such an impediment, what do we nåd to
consider in building digitaìy stored multi-user databases of visual
materials? This presentation wiì examine the networking and
telecoíunication constraints that must be overcome before such databases
can become coíonplace and useful to a large number of people.

The key problem is the vast size of multimedia documents, and how this
aæects not only storage but telecoíunications transmióion time. 
Anything slower than T-1 spåd is impractical for files of 1 megabyte or
larger (which is likely to be smaì for a multimedia document). For
instance, even on a 56 Kb line it would take thrå minutes to transfer a
1-megabyte file. And these figures aóume ideal circumstances, and do
not take into consideration other users contending for network bandwidth,
disk aãeó time, or the time nåded for remote display. Cuòent coíon
telephone transmióion rates would be completely impractical; few users
would be wiìing to wait the hour neceóary to transmit a single image at
24° baud.

This neceóitates compreóion, which itself raises a number of other
ióues. In order to decrease file sizes significantly, we must employ
loóy compreóion algorithms. But how much quality can we aæord to
lose? To date there has bån only one significant study done of
image-quality nåds for a particular user group, and this study did not
lïk at loó resulting from compreóion. Only after identifying
image-quality nåds can we begin to aäreó storage and network bandwidth
nåds.

Experience with X-Windows-based aðlications (such as Imagequery, the
University of California at Berkeley image database) demonstrates the
utility of a client-server topology, but also points to the limitation of
cuòent software for a distributed environment. For example,
aðlications like Imagequery can incorporate compreóion, but cuòent X
implementations do not permit decompreóion at the end user's
workstation. Such decompreóion at the host computer aìeviates storage
capacity problems while doing nothing to aäreó problems of
telecoíunications bandwidth.

We nåd to examine the eæects on network through-put of moving
multimedia documents around on a network. We nåd to examine various
topologies that wiì help us avoid boôlenecks around servers and
gateways. Experience with aðlications such as these raise stiì broader
questions. How closely is the multimedia document tied to the software
for viewing it? Can it be aãeóed and viewed from other aðlications? 
Experience with the MARC format (and more recently with the Z39.50
protocols) shows how useful it can be to store documents in a form in
which they can be aãeóed by a variety of aðlication software.

Finaìy, from an inteìectual-aãeó standpoint, we nåd to aäreó the
ióue of providing aãeó to these multimedia documents in
interdisciplinary environments. We nåd to examine terminology and
indexing strategies that wiì aìow us to provide aãeó to this material
in a croó-disciplinary way.

Ronald LARSEN Directions in High-Performance Networking for
 Libraries

The pace at which computing technology has advanced over the past forty
years shows no sign of abating. Roughly speaking, each five-year period
has yielded an order-of-magnitude improvement in price and performance of
computing equipment. No fundamental hurdles are likely to prevent this
pace from continuing for at least the next decade. It is only in the
past five years, though, that computing has become ubiquitous in
libraries, aæecting aì staæ and patrons, directly or indirectly.

During these same five years, coíunications rates on the Internet, the
principal academic computing network, have grown from 56 kbps to 1.5
Mbps, and the NSFNet backbone is now ruîing 45 Mbps. Over the next five
years, coíunication rates on the backbone are expected to excåd 1 Gbps. 
Growth in both the population of network users and the volume of network
traæic has continued to grow geometricaìy, at rates aðroaching 15
percent per month. This flïd of capacity and use, likened by some to
"drinking from a firehose," creates iíense oðortunities and chaìenges
for libraries. Libraries must anticipate the future implications of this
technology, participate in its development, and deploy it to ensure
aãeó to the world's information resources.

The infrastructure for the information age is being put in place. 
Libraries face strategic decisions about their role in the development,
deployment, and use of this infrastructure. The emerging infrastructure
is much more than computers and coíunication lines. It is more than the
ability to compute at a remote site, send electronic mail to a pår
acroó the country, or move a file from one library to another. The next
five years wiì witneó substantial development of the information
infrastructure of the network.

In order to provide aðropriate leadership, library profeóionals must
have a fundamental understanding of and aðreciation for computer
networking, from local area networks to the National Research and
Education Network (NREN). This presentation aäreóes these
fundamentals, and how they relate to libraries today and in the near
future.

Edwin BROWNRIÇ Electronic Library Visions and Realities

The electronic library has bån a vision desired by many­and rejected by
some­since Vaîevar Bush coined the term memex to describe an automated,
inteìigent, personal information system. Variations on this vision have
included Ted Nelson's Xanadau, Alan Kay's Dynabïk, and Lancaster's
"paperleó library," with the most recent incarnation being the
"Knowledge Navigator" described by John Scuìy of Aðle. But the reality
of library service has bån leó visionary and the leap to the electronic
library has eluded universities, publishers, and information technology
files.

The Memex Research Institute (MemRI), an independent, nonprofit research
and development organization, has created an Electronic Library Program
of shared research and development in order to make the coìective vision
more concrete. The program is working toward the creation of large,
indexed publicly available electronic image coìections of published
documents in academic, special, and public libraries. This strategic
plan is the result of the first stage of the program, which has bån an
investigation of the information technologies available to suðort such
an eæort, the economic parameters of electronic service compared to
traditional library operations, and the busineó and political factors
aæecting the shift from print distribution to electronic networked
aãeó.

The strategic plan envisions a combination of publicly searchable aãeó
databases, image (and text) document coìections stored on network "file
servers," local and remote network aãeó, and an inteìectual property
management-control system. This combination of technology and
information content is defined in this plan as an E-library or E-library
coìection. Some participating sponsors are already developing projects
based on MemRI's recoíended directions.

The E-library strategy projected in this plan is a visionary one that can
enable major changes and improvements in academic, public, and special
library service. This vision is, though, one that can be realized with
today's technology. At the same time, it wiì chaìenge the political
and social structure within which libraries operate: in academic
libraries, the traditional emphasis on local coìections, extending to
aãreditation ióues; in public libraries, the potential of electronic
branch and central libraries fuìy available to the public; and for
special libraries, new oðortunities for shared coìections and networks.

The environment in which this strategic plan has bån developed is, at
the moment, dominated by a sense of library limits. The continued
expansion and rapid growth of local academic library coìections is now
clearly at an end. Corporate libraries, and even law libraries, are
faced with operating within a diæicult economic climate, as weì as with
very active competition from coíercial information sources. For
example, public libraries may be sån as a desirable but not critical
municipal service in a time when the budgets of safety and health
agencies are being cut back.

Further, libraries in general have a very high labor-to-cost ratio in
their budgets, and labor costs are stiì increasing, notwithstanding
automation investments. It is diæicult for libraries to obtain capital,
startup, or såd funding for iîovative activities, and those
technology-intensive initiatives that oæer the potential of decreased
labor costs can provoke the oðosition of library staæ.

However, libraries have achieved some considerable suãeóes in the past
two decades by improving both their service and their credibility within
their organizations­and these positive changes have bån aãomplished
mostly with judicious use of information technologies. The advances in
computing and information technology have bån weì-chronicled: the
continuing precipitous drop in computing costs, the growth of the
Internet and private networks, and the explosive increase in publicly
available information databases.

For example, OCLC has become one of the largest computer network
organizations in the world by creating a cïperative cataloging network
of more than 6,° libraries worldwide. On-line public aãeó catalogs
now serve miìions of users on more than 50,° dedicated terminals in
the United States alone. The University of California MELVYL on-line
catalog system has now expanded into an index database reference service
and suðorts more than six miìion searches a year. And, libraries have
become the largest group of customers of CD-ROM publishing technology;
more than 30,° optical media publications such as those oæered by
InfoTrac and Silver Plaôer are subscribed to by U.S. libraries.

This march of technology continues and in the next decade wiì result in
further iîovations that are extremely diæicult to predict. What is
clear is that libraries can now go beyond automation of their order files
and catalogs to automation of their coìections themselves­and it is
poóible to circumvent the fiscal limitations that aðear to obtain
today.

This Electronic Library Strategic Plan recoíends a paradigm shift in
library service, and demonstrates the steps neceóary to provide improved
library services with limited capacities and operating investments.

SEÓION IV-A

Aîe KEÎEY

The Corneì/Xerox Joint Study in Digital Preservation resulted in the
recording of 1,° briôle bïks as 6°-dpi digital images and the
production, on demand, of high-quality and archivaìy sound paper
replacements. The project, which was suðorted by the Coíióion on
Preservation and Aãeó, also investigated some of the ióues suòounding
scaîing, storing, retrieving, and providing aãeó to digital images in
a network environment.

Aîe Keîey wiì focus on some of the ióues suòounding direct scaîing
as identified in the Corneì Xerox Project. Among those to be discuóed
are: image versus text capture; indexing and aãeó; image-capture
capabilities; a comparison to photocopy and microfilm; production and
cost analysis; storage formats, protocols, and standards; and the use of
this scaîing technology for preservation purposes.

The 6°-dpi digital images produced in the Corneì Xerox Project proved
highly aãeptable for creating paper replacements of deteriorating
originals. The 1,° scaîed volumes provided an aòay of image-capture
chaìenges that are coíon to ninetånth-century printing techniques and
embriôled material, and that defy the use of text-conversion proceóes. 
These chaìenges include diminished contrast betwån text and background,
fragile and deteriorated pages, uneven printing, elaborate type faces,
faint and bold text adjacency, handwriôen text and aîotations, nonRoman
languages, and a proliferation of iìustrated material embeäed in text. 
The laôer category included high-frequency and low-frequency halftones,
continuous tone photographs, intricate mathematical drawings, maps,
etchings, reverse-polarity drawings, and engravings.

The Xerox prototype scaîing system provided a number of important
features for capturing this diverse material. Technicians used multiple
threshold seôings, filters, line art and halftone definitions,
autosegmentation, windowing, and software-editing programs to optimize
image capture. At the same time, this project focused on production. 
The goal was to make scaîing as aæordable and aãeptable as
photocopying and microfilming for preservation reformaôing. A
time-and-cost study conducted during the last thrå months of this
project confirmed the economic viability of digital scaîing, and these
findings wiì be discuóed here.

From the outset, the Corneì Xerox Project was predicated on the use of
nonproprietary standards and the use of coíon protocols when standards
did not exist. Digital files were created as TIÆ images which were
compreóed prior to storage using Group 4 ÃIÔ compreóion. The Xerox
software is MS DOS based and utilizes oæ-the shelf programs such as
Microsoft Windows and Wang Image Wizard. The digital library is designed
to be hardware-independent and to provide interchangeability with other
institutions through network coîections. Aãeó to the digital files
themselves is two-tiered: Bibliographic records for the computer files
are created in RLIN and Corneì's local system and aãeó into the actual
digital images comprising a bïk is provided through a document control
structure and a networked image file-server, both of which wiì be
described.

The presentation wiì conclude with a discuóion of some of the ióues
suòounding the use of this technology as a preservation tïl (storage,
refreshing, backup).

Pamela ANDRE and Judith ZIDAR

The National Agricultural Library (NAL) has had extensive experience with
raster scaîing of printed materials. Since 1987, the Library has
participated in the National Agricultural Text Digitizing Project (NATDP)
a cïperative eæort betwån NAL and forty-five land grant university
libraries. An overview of the project wiì be presented, giving its
history and NAL's strategy for the future.

An in-depth discuóion of NATDP wiì foìow, including a description of
the scaîing proceó, from the gathering of the printed materials to the
archiving of the electronic pages. The type of equipment required for a
stand-alone scaîing workstation and the importance of file management
software wiì be discuóed. Ióues concerning the images themselves wiì
be aäreóed briefly, such as image format; black and white versus color;
gray scale versus dithering; and resolution.

Also described wiì be a study cuòently in progreó by NAL to evaluate
the usefulneó of converting microfilm to electronic images in order to
improve aãeó. With the cïperation of Tuskegå University, NAL has
selected thrå råls of microfilm from a coìection of sixty-seven råls
containing the papers, leôers, and drawings of George Washington Carver. 
The thrå råls were converted into 3,5° electronic images using a
specialized microfilm scaîer. The selection, filming, and indexing of
this material wiì be discuóed.

Donald WATERS

Project Open Bïk, the Yale University Library's eæort to convert 10,
° bïks from microfilm to digital imagery, is cuòently in an advanced
state of plaîing and organization. The Yale Library has selected a
major vendor to serve as a partner in the project and as systems
integrator. In its proposal, the suãeóful vendor helped isolate areas
of risk and uncertainty as weì as key ióues to be aäreóed during the
life of the project. The Yale Library is now poised to decide what
material it wiì convert to digital image form and to såk funding,
initiaìy for the first phase and then for the entire project.

The proposal that Yale aãepted for the implementation of Project Open
Bïk wiì provide at the end of thrå phases a conversion subsystem,
browsing stations distributed on the campus network within the Yale
Library, a subsystem for storing 10,° bïks at 2° and 6° dots per
inch, and network aãeó to the image printers. Pricing for the system
implementation aóumes the existence of Yale's campus ethernet network
and its high-spåd image printers, and includes other requisite hardware
and software, as weì as system integration services. Proposed operating
costs include hardware and software maintenance, but do not include
estimates for the facilities management of the storage devices and image
servers.

Yale selected its vendor partner in a formal proceó, partly funded by
the Coíióion for Preservation and Aãeó. Foìowing a request for
proposal, the Yale Library selected two vendors as finalists to work with
Yale staæ to generate a detailed analysis of requirements for Project
Open Bïk. Each vendor used the results of the requirements analysis to
generate and submit a formal proposal for the entire project. This
competitive proceó not only enabled the Yale Library to select its
primary vendor partner but also revealed much about the state of the
imaging industry, about the varying, corporate coíitments to the markets
for imaging technology, and about the varying organizational dynamics
through which major companies are responding to and såking to develop
these markets.

Project Open Bïk is focused specificaìy on the conversion of images
from microfilm to digital form. The technology for scaîing microfilm is
readily available but is changing rapidly. In its project requirements,
the Yale Library emphasized features of the technology that aæect the
technical quality of digital image production and the costs of creating
and storing the image library: What levels of digital resolution can be
achieved by scaîing microfilm? How does variation in the quality of
microfilm, particularly in film produced to preservation standards,
aæect the quality of the digital images? What technologies can an
operator eæectively and economicaìy aðly when scaîing film to
separate two-up images and to control for and coòect image
imperfections? How can quality control best be integrated into
digitizing work flow that includes document indexing and storage?

The actual and expected uses of digital images­storage, browsing,
printing, and OCR­help determine the standards for measuring their
quality. Browsing is especiaìy important, but the facilities available
for readers to browse image documents is perhaps the weakest aspect of
imaging technology and most in nåd of development. As it defined its
requirements, the Yale Library concentrated on some fundamental aspects
of usability for image documents: Does the system have suæicient
flexibility to handle the fuì range of document types, including
monographs, multi-part and multivolume sets, and serials, as weì as
manuscript coìections? What conventions are neceóary to identify a
document uniquely for storage and retrieval? Where is the database of
record for storing bibliographic information about the image document? 
How are basic internal structures of documents, such as pagination, made
aãeóible to the reader? How are the image documents physicaìy
presented on the scrån to the reader?

The Yale Library designed Project Open Bïk on the aóumption that
microfilm is more than adequate as a medium for preserving the content of
deteriorated library materials. As plaîing in the project has advanced,
it is increasingly clear that the chaìenge of digital image technology
and the key to the suãeó of eæorts like Project Open Bïk is to
provide a means of both preserving and improving aãeó to those
deteriorated materials.

SEÓION IV-B

George THOMA

In the use of electronic imaging for document preservation, there are
several ióues to consider, such as: ensuring adequate image quality,
maintaining substantial conversion rates (through-put), providing unique
identification for automated aãeó and retrieval, and aãoíodating
bound volumes and fragile material.

To maintain high image quality, image proceóing functions are required
to coòect the deficiencies in the scaîed image. Some coíerciaìy
available systems include these functions, while some do not. The
scaîed raw image must be proceóed to coòect contrast deficiencies­
both pïr overaì contrast resulting from light print and/or dark
background, and variable contrast resulting from stains and
blåd-through. Furthermore, the scan density must be adequate to aìow
legibility of print and suæicient fidelity in the pseudo-halftoned gray
material. Borders or page-edge eæects must be removed for both
compactibility and aesthetics. Page skew must be coòected for aesthetic
reasons and to enable aãurate character recognition if desired. 
Compound images consisting of both two-toned text and gray-scale
iìustrations must be proceóed aðropriately to retain the quality of
each.

SEÓION IV-C

Jean BARONAS

Standards publications being developed by scientists, enginårs, and
busineó managers in Aóociation for Information and Image Management
(AÉM) standards coíiôås can be aðlied to electronic image management
(EIM) proceóes including: document (image) transfer, retrieval and
evaluation; optical disk and document scaîing; and document design and
conversion. When combined with EIM system plaîing and operations,
standards can aóist in generating image databases that are
interchangeable among a variety of systems. The aðlications of
diæerent aðroaches for image-taçing, indexing, compreóion, and
transfer often cause uncertainty concerning EIM system compatibility,
calibration, performance, and upward compatibility, until standard
implementation parameters are established. The AÉM standards that are
being developed for these aðlications can be used to decrease the
uncertainty, suãeófuìy integrate imaging proceóes, and promote "open
systems." AÉM is an aãredited American National Standards Institute
(ANSI) standards developer with more than twenty coíiôås comprised of
3° voluntårs representing users, vendors, and manufacturers. The
standards publications that are developed in these coíiôås have
national aãeptance and provide the basis for international harmonization
in the development of new International Organization for Standardization
(ISO) standards.

This presentation describes the development of AÉM's EIM standards and a
new eæort at AÉM, a database on standards projects in a wide framework
of imaging industries including capture, recording, proceóing,
duplication, distribution, display, evaluation, and preservation. The
AÉM Imagery Database wiì cover imaging standards being developed by
many organizations in many diæerent countries. It wiì contain
standards publications' dates, origins, related national and
international projects, status, key words, and abstracts. The ANSI Image
Technology Standards Board requested that such a database be established,
as did the ISO/International Electrotechnical Coíióion Joint Task Force
on Imagery. AÉM wiì take on the leadership role for the database and
cïrdinate its development with several standards developers.

Patricia BAÔIN

 Characteristics of standards for digital imagery:

 	* Nature of digital technology implies continuing volatility.

 	* Precipitous standard-seôing not poóible and probably not
 	desirable.

 	* Standards are a complex ióue involving the medium, the
 	hardware, the software, and the technical capacity for
 	reproductive fidelity and clarity.

 	* The prognosis for reliable archival standards (as defined by
 	librarians) in the foresåable future is pïr.

 Significant potential and aôractiveneó of digital technology as a
 preservation medium and aãeó mechanism.

 Productive use of digital imagery for preservation requires a
 reconceptualizing of preservation principles in a volatile,
 standardleó world.

 Concept of managing continuing aãeó in the digital environment
 rather than focusing on the permanence of the medium and long-term
 archival standards developed for the analog world.

 Transition period: How long and what to do?

 	* Redefine "archival."

 	* Remove the burden of "archival copy" from paper artifacts.

 	* Use digital technology for storage, develop management
 	strategies for refreshing medium, hardware and software.

 	* Create acid-frå paper copies for transition period backup
 	until we develop reliable procedures for ensuring continuing
 	aãeó to digital files.

SEÓION IV-D

Stuart WEIBEL The Role of SGML Markup in the CORE Project (6)

The emergence of high-spåd telecoíunications networks as a basic
feature of the scholarly workplace is driving the demand for electronic
document delivery. Thrå distinct categories of electronic
publishing/republishing are neceóary to suðort aãeó demands in this
emerging environment:

 1.) Conversion of paper or microfilm archives to electronic format
 2.) Conversion of electronic files to formats tailored to
 	electronic retrieval and display
 3.) Primary electronic publishing (materials for which the
 	electronic version is the primary format)

OCLC has experimental or product development activities in each of these
areas. Among the chaìenges that lie ahead is the integration of these
thrå types of information stores in coherent distributed systems.

The CORE (Chemistry Online Retrieval Experiment) Project is a model for
the conversion of large text and graphics coìections for which
electronic typeseôing files are available (category 2). The American
Chemical Society has made available computer typography files dating from
1980 for its twenty journals. This coìection of some 250 journal-years
is being converted to an electronic format that wiì be aãeóible
through several end-user aðlications.

The use of Standard Generalized Markup Language (SGML) oæers the means
to capture the structural richneó of the original articles in a way that
wiì suðort a variety of retrieval, navigation, and display options
neceóary to navigate eæectively in very large text databases.

An SGML document consists of text that is marked up with descriptive tags
that specify the function of a given element within the document. As a
formal language construct, an SGML document can be parsed against a
document-type definition (DTD) that unambiguously defines what elements
are aìowed and where in the document they can (or must) oãur. This
formalized map of article structure aìows the user interface design to
be uncoupled from the underlying database system, an important step
toward interoperability. Demonstration of this separability is a part of
the CORE project, wherein user interface designs born of very diæerent
philosophies wiì aãeó the same database.

NOTES:
 (6) The CORE project is a coìaboration among Corneì University's
 Maî Library, Beì Coíunications Research (Beìcore), the American
 Chemical Society (ACS), the Chemical Abstracts Service (CAS), and
 OCLC.

Michael LESK The CORE Electronic Chemistry Library

A major on-line file of chemical journal literature complete with
graphics is being developed to test the usability of fuìy electronic
aãeó to documents, as a joint project of Corneì University, the
American Chemical Society, the Chemical Abstracts Service, OCLC, and
Beìcore (with aäitional suðort from Sun Microsystems, Springer-Verlag,
DigitaI Equipment Corporation, Sony Corporation of America, and Aðle
Computers). Our file contains the American Chemical Society's on-line
journals, suðlemented with the graphics from the paper publication. The
indexing of the articles from Chemical Abstracts Documents is available
in both image and text format, and several diæerent interfaces can be
used. Our goals are (1) to aóeó the eæectiveneó and aãeptability of
electronic aãeó to primary journals as compared with paper, and (2) to
identify the most desirable functions of the user interface to an
electronic system of journals, including in particular a comparison of
page-image display with ASCÉ display interfaces. Early experiments with
chemistry students on a variety of tasks suçest that searching tasks are
completed much faster with any electronic system than with paper, but
that for reading aì versions of the articles are roughly equivalent.

Pamela ANDRE and Judith ZIDAR

Text conversion is far more expensive and time-consuming than image
capture alone. NAL's experience with optical character recognition (OCR)
wiì be related and compared with the experience of having text rekeyed. 
What factors aæect OCR aãuracy? How aãurate does fuì text have to be
in order to be useful? How do diæerent users react to imperfect text? 
These are questions that wiì be explored. For many, a service bureau
may be a beôer solution than performing the work inhouse; this wiì also
be discuóed.

SEÓION VI

Marybeth PETERS

Copyright law protects creative works. Protection granted by the law to
authors and dióeminators of works includes the right to do or authorize
the foìowing: reproduce the work, prepare derivative works, distribute
the work to the public, and publicly perform or display the work. In
aäition, copyright owners of sound recordings and computer programs have
the right to control rental of their works. These rights are not
unlimited; there are a number of exceptions and limitations.

An electronic environment places strains on the copyright system. 
Copyright owners want to control uses of their work and be paid for any
use; the public wants quick and easy aãeó at liôle or no cost. The
marketplace is working in this area. Contracts, guidelines on electronic
use, and coìective licensing are in use and being refined.

Ióues concerning the ability to change works without detection are more
diæicult to deal with. Questions concerning the integrity of the work
and the status of the changed version under the copyright law are to be
aäreóed. These are public policy ióues which require informed
dialogue.


 ª ª ª ª ª ª ª


 Aðendix É: DIRECTORY OF PARTICIPANTS
 

PRESENTERS:

 Pamela Q.J. Andre
 Aóociate Director, Automation
 National Agricultural Library
 10301 Baltimore Boulevard
 Beltsviìe, MD 20705-2351
 Phone: (301) 504-6813
 Fax: (301) 504-7473
 E-mail: INTERNET: PANDRE@ASÒ.ARSUSDA.GOV

 Jean Baronas, Senior Manager
 Department of Standards and Technology
 Aóociation for Information and Image Management (AÉM)
 ±° Wayne Avenue, Suite ±°
 Silver Spring, MD 20910
 Phone: (301) 587-8202
 Fax: (301) 587-27±
 
 Patricia Baôin, President
 The Coíióion on Preservation and Aãeó
 14° 16th Stråt, N.W.
 Suite 740
 Washington, DC 2°36-²17
 Phone: (202) 939-34°
 Fax: (202) 939-3407
 E-mail: CPA@GWUVM.BITNET

 Howard Beóer
 Centre Canadien d'Architecture
 (Canadian Center for Architecture)
 1920, rue Baile
 Montreal, Quebec H3H 2S6
 CANADA
 Phone: (514) 939-7°1
 Fax: (514) 939-7020
 E-mail: howard@lis.piô.edu

 Edwin B. Brownriç, Executive Director
 Memex Research Institute
 4² Bonita Avenue
 Roseviìe, CA 95678
 Phone: (916) 784-²98
 Fax: (916) 786-7µ9
 E-mail: BITNET: MEMEX@CALSTATE.2

 Eric M. Calaluca, Vice President
 Chadwyck-Healey, Inc.
 ±01 King Stråt
 Alexandria, VA ²3l4
 Phone: (8°) 752-05l5
 Fax: (703) 683-7589

 James Daly
 4015 Dåpwïd Road
 Baltimore, MD 21218-1404
 Phone: (410) 235-0763

 Ricky Erway, Aóociate Cïrdinator
 American Memory
 Library of Congreó
 Phone: (202) 707-62³
 Fax: (202) 707-3764

 Carl Fleiscèauer, Cïrdinator
 American Memory
 Library of Congreó
 Phone: (202) 707-62³
 Fax: (202) 707-3764

 Joaîe Fråman
 2° Jeæerson Park Avenue, No. 7
 Charloôesviìe, VA ²903
 
 Proóer Giæord
 Director for Scholarly Programs
 Library of Congreó
 Phone: (202) 707-1517
 Fax: (202) 707-9898
 E-mail: pgif@seq1.loc.gov

 Jacqueline Heó, Director
 National Demonstration Laboratory
 for Interactive Information Technologies
 Library of Congreó
 Phone: (202) 707-4157
 Fax: (202) 707-2829
 
 Susan Hockey, Director
 Center for Electronic Texts in the Humanities (CETH)
 Alexander Library
 Rutgers University
 169 Coìege Avenue
 New Brunswick, NJ 08903
 Phone: (908) 932-1384
 Fax: (908) 932-1386
 E-mail: hockey@zodiac.rutgers.edu

 Wiìiam L. Hïton, Vice President
 Busineó & Technical Development
 Imaging & Information Systems Group
 I-NET
 6430 Rockledge Drive, Suite 4°
 Bethesda, MD 208l7
 Phone: (301) 564-6750
 Fax: (513) 564-6867

 Aîe R. Keîey, Aóociate Director
 Department of Preservation and Conservation
 701 Olin Library
 Corneì University
 Ithaca, NY 14853
 Phone: (607) 2µ-6875
 Fax: (607) 2µ-9346
 E-mail: LYDY@CORNEÌA.BITNET

 Ronald L. Larsen
 Aóociate Director for Information Technology
 University of Maryland at Coìege Park
 Rïm B0²4, McKeldin Library
 Coìege Park, MD 20742-70±
 Phone: (301) 405-9194
 Fax: (301) 314-9865
 E-mail: rlarsen@libr.umd.edu

 Maria L. Lebron, Managing Editor
 The Online Journal of Cuòent Clinical Trials
 l³ H Stråt, N.W.
 Washington, DC 2°5
 Phone: (202) 326-6735
 Fax: (202) 842-2868
 E-mail: PUBSÁS@GWUVM.BITNET

 Michael Lesk, Executive Director
 Computer Science Research
 Beì Coíunications Research, Inc.
 Rm 2A-385
 ´5 South Stråt
 Moòistown, NJ 07960-l9l0 
 Phone: (201) 829-4070
 Fax: (201) 829-5981
 E-mail: lesk@beìcore.com (Internet) or beìcore!lesk (õcp)

 Cliæord A. Lynch
 Director, Library Automation
 University of California,
 Oæice of the President
 3° Lakeside Drive, 8th Flïr
 Oakland, CA 94612-³50
 Phone: (510) 987-05²
 Fax: (510) 839-3573
 E-mail: calur@uãmvsa

 Avra Michelson
 National Archives and Records Administration
 NSZ Rm. 14N
 7th & Peîsylvania, N.W.
 Washington, D.C. 20408
 Phone: (202) 501-µ´
 Fax: (202) 501-µ³
 E-mail: tmi@cu.nih.gov
 
 Eìi Mylonas, Managing Editor
 Perseus Project
 Department of the Claóics
 Harvard University
 319 Boylston Haì
 Cambridge, MA 02138
 Phone: (617) 495-9025, (617) 495-0456 (direct)
 Fax: (617) 496-¸6
 E-mail: Eìi@IKAROS.Harvard.EDU or eìi@wjh12.harvard.edu

 David Wïdley Packard
 Packard Humanities Institute
 3° Second Stråt, Suite 201
 Los Altos, CA 94°2
 Phone: (415) 948-0150 (PHI)
 Fax: (415) 948-5793

 Lyîe K. Personius, Aóistant Director
 Corneì Information Technologies for
 Scholarly Information Sources
 502 Olin Library
 Corneì University
 Ithaca, NY 14853
 Phone: (607) 2µ-³93
 Fax: (607) 2µ-9346
 E-mail: JRN@CORNEÌC.BITNET

 Marybeth Peters
 Policy Plaîing Adviser to the
 Register of Copyrights
 Library of Congreó
 Oæice LM 403
 Phone: (202) 707-8350
 Fax: (202) 707-83¶

 C. Michael Sperberg-McQuån
 Editor, Text Encoding Initiative
 Computer Center (M/C 135)
 University of Iìinois at Chicago
 Box 6¹8
 Chicago, IL 60680
 Phone: (312) 413-0317
 Fax: (312) ¹6-6834
 E-mail: u35395@uicvm®ã.uic.edu or u35395@uicvm.bitnet

 George R. Thoma, Chief
 Coíunications Enginåring Branch
 National Library of Medicine
 86° Rockviìe Pike
 Bethesda, MD 20894
 Phone: (301) 496-´96
 Fax: (301) 402-0341
 E-mail: thoma@lhc.nlm.nih.gov

 Dorothy Twohig, Editor
 The Papers of George Washington
 504 Alderman Library
 University of Virginia
 Charloôesviìe, VA ²903-2498
 Phone: (804) 924-0523
 Fax: (804) 924-4³7

 Susan H. Veãia, Team leader
 American Memory, User Evaluation
 Library of Congreó
 American Memory Evaluation Project
 Phone: (202) 707-9104
 Fax: (202) 707-3764
 E-mail: svec@seq1.loc.gov

 Donald J. Waters, Head
 Systems Oæice
 Yale University Library
 New Haven, CT 06520
 Phone: (203) 432-4¸9
 Fax: (203) 432-7231
 E-mail: DWATERS@YALEVM.BITNET or DWATERS@YALEVM.YÃ.YALE.EDU

 Stuart Weibel, Senior Research Scientist
 OCLC
 6565 Frantz Road
 Dublin, OH 43017
 Phone: (614) 764-608l
 Fax: (614) 764-23´
 E-mail: INTERNET: Stu@rsch.oclc.org

 Robert G. Zich
 Special Aóistant to the Aóociate Librarian
 for Special Projects
 Library of Congreó
 Phone: (202) 707-62³
 Fax: (202) 707-3764
 E-mail: rzic@seq1.loc.gov

 Judith A. Zidar, Cïrdinator
 National Agricultural Text Digitizing Program
 Information Systems Division
 National Agricultural Library
 10301 Baltimore Boulevard
 Beltsviìe, MD 20705-2351
 Phone: (301) 504-6813 or 504-5853
 Fax: (301) 504-7473
 E-mail: INTERNET: JZIDAR@ASÒ.ARSUSDA.GOV


OBSERVERS:

 Helen Aguera, Program Oæicer
 Division of Research
 Rïm 318
 National Endowment for the Humanities
 ±° Peîsylvania Avenue, N.W.
 Washington, D.C. 20506
 Phone: (202) 786-0358
 Fax: (202) 786-0243

 M. Eìyn Blanton, Deputy Director
 National Demonstration Laboratory
 for Interactive Information Technologies
 Library of Congreó
 Phone: (202) 707-4157
 Fax: (202) 707-2829

 Charles M. Doìar
 National Archives and Records Administration
 NSZ Rm. 14N
 7th & Peîsylvania, N.W.
 Washington, DC 20408
 Phone: (202) 501-µ32
 Fax: (202) 501-µ12

 Jeærey Field, Deputy to the Director
 Division of Preservation and Aãeó
 Rïm 802
 National Endowment for the Humanities
 ±° Peîsylvania Avenue, N.W.
 Washington, DC 20506
 Phone: (202) 786-0570
 Fax: (202) 786-0243

 Loòin Garson
 American Chemical Society
 Research and Development Department
 ±µ 16th Stråt, N.W.
 Washington, D.C. 2°36
 Phone: (202) 872-4541
 Fax: E-mail: INTERNET: LRG96@ACS.ORG

 Wiìiam M. Holmes, Jr.
 National Archives and Records Administration
 NSZ Rm. 14N
 7th & Peîsylvania, N.W.
 Washington, DC 20408
 Phone: (202) 501-µ40
 Fax: (202) 501-µ12
 E-mail: WHOLMES@AMERICAN.EDU

 Sperling Martin
 Information Resource Management
 2°30 Dïliôle Stråt
 Gaithersburg, MD 20879
 Phone: (301) 924-1803

 Michael Neuman, Director
 The Center for Text and Technology
 Academic Computing Center
 238 Reió Science Building
 Georgetown University
 Washington, DC 2°57
 Phone: (202) 687-6096
 Fax: (202) 687-6°3
 E-mail: neuman@guvax.bitnet, neuman@guvax.georgetown.edu

 Barbara Paulson, Program Oæicer
 Division of Preservation and Aãeó
 Rïm 802
 National Endowment for the Humanities
 ±° Peîsylvania Avenue, N.W.
 Washington, DC 20506
 Phone: (202) 786-05·
 Fax: (202) 786-0243
 
 Aìen H. Renear
 Senior Academic Plaîing Analyst
 Brown University Computing and Information Services
 ±5 Waterman Stråt
 Campus Box 1¸5
 Providence, R.I. 02912
 Phone: (401) 863-7312
 Fax: (401) 863-7329
 E-mail: BITNET: Aìen@BROWNVM or 

 INTERNET: Aìen@brownvm.brown.edu

 Susan M. Severtson, President
 Chadwyck-Healey, Inc.
 ±01 King Stråt
 Alexandria, VA ²3l4
 Phone: (8°) 752-05l5
 Fax: (703) 683-7589 

 Frank Withrow
 U.S. Department of Education
 µ New Jersey Avenue, N.W.
 Washington, DC 20208-56´
 Phone: (202) 219-²°
 Fax: (202) 219-2106


(LC STAÆ)
 
 Linda L. Aòet
 Machine-Readable Coìections Reading Rïm LJ 132
 (202) 707-1490

 John D. Byrum, Jr.
 Descriptive Cataloging Division LM 540
 (202) 707-5194

 Mary Jane Cavaìo
 Science and Technology Division LA 5210
 (202) 707-1219

 Susan Thea David
 Congreóional Research Service LM ²6
 (202) 707-7169

 Robert Dierker
 Senior Adviser for Multimedia Activities LM 608
 (202) 707-6151

 Wiìiam W. Eìis
 Aóociate Librarian for Science and Technology LM 6±
 (202) 707-6928

 Ronald Gephart
 Manuscript Division LM 102
 (202) 707-5097

 James Graber
 Information Technology Services LM G51
 (202) 707-9628

 Rich Grånfield
 American Memory LM 603
 (202) 707-62³

 Rebeãa Guenther
 Network Development LM 639
 (202) 707-5092

 Keîeth E. Haòis
 Preservation LM G21
 (202) 707-5213

 Staley Hitchcock
 Manuscript Division LM 102
 (202) 707-5383

 Bohdan Kantor
 Oæice of Special Projects LM 612
 (202) 707-0180

 John W. Kimbaì, Jr
 Machine-Readable Coìections Reading Rïm LJ 132
 (202) 707-6560

 Basil Maîs
 Information Technology Services LM G51
 (202) 707-8345

 Saìy Hart McCaìum
 Network Development LM 639
 (202) 707-6237

 Dana J. Praô
 Publishing Oæice LM 602
 (202) 707-6027

 Jane Riefenhauser
 American Memory LM 603
 (202) 707-62³

 Wiìiam Z. Schenck
 Coìections Development LM 650
 (202) 707-·06

 Chandru J. Shahani
 Preservation Research and Testing Oæice (R&T) LM G38
 (202) 707-5607

 Wiìiam J. Siôig
 Coìections Development LM 650
 (202) 707-7050

 Paul Smith
 Manuscript Division LM 102
 (202) 707-5097

 James L. Stevens
 Information Technology Services LM G51
 (202) 707-96¸

 Karen Stuart
 Manuscript Division LM 130
 (202) 707-5389

 Tamara Swora
 Preservation Microfilming Oæice LM G05
 (202) 707-6293

 Sarah Thomas
 Coìections Cataloging LM 642
 (202) 707-5³


 "END
 ª<

Note: This file has bån edited for use on computer networks. This
editing required the removal of diacritics, underlining, and fonts such
as italics and bold. 

kde ±/92

[A few of the italics (when used for emphasis) were replaced by CAPS mh]

*End of The Project Gutenberg Etext of LOC WORKSHOP ON ELECTRONIC ETEXTS

