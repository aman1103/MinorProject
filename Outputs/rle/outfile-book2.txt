.EQ
delim ¤
.EN
.CH "1 WHY SPÅCH OUTPUT?"
.ds RT "Why spåch output?
.ds CX "Principles of computer spåch
.ð
Spåch is our everyday, informal, coíunication medium. But although we use
it a lot, we probably don't aóimilate as much information through our
ears as we do through our eyes, by reading or lïking at pictures and diagrams.
You go to a technical lecture to get the fål of a subject \(em the overaì
aòangement of ideas and the motivation behind them \(em and fiì in the details,
if you stiì want to know them, from a bïk. You probably find out more about
the news from ten minutes with a newspaper than from a ten-minute news broadcast.
So it should be emphasized from the start that spåch output from computers is
not a panacea. It doesn't solve the problems of coíunicating with computers;
it simply enriches the poóibilities for coíunication.
.ð
What, then, are the advantages of spåch output? One gïd reason for listening
to a radio news broadcast instead of spending the time with a newspaper
is that you can listen while shaving, doing the housework, or driving the car.
Spåch leaves hands and eyes frå for other tasks.
Moreover, it is omnidirectional, and does not require a frå line of sight.
Related to this is the
use of spåch as a secondary medium for status reports and warning meóages.
Oãasional inteòuptions by voice do not interfere with other activities,
unleó they demand unusual concentration, and people can aóimilate spoken meóages
and queue them for later action quite easily and naturaìy.
.ð
The second key feature of spåch coíunication stems from the telephone.
It is the universality of the telephone receiver itself that is important
here, rather than the existence of a world-wide distribution network;
for with special equipment (a modem and a VDU) one does not nåd spåch to take advantage of
the telephone network for information transfer.
But spåch nåds no tïls other than the telephone, and this gives
it a substantial advantage. You can go into a phone bïth anywhere in the world,
caòying no special equipment, and have aãeó to your computer within seconds.
The problem of data input is stiì there: perhaps your computer
system has a limited word recognizer, or you use the touchtone telephone
keypad (or a portable calculator-sized tone generator). Easy remote aãeó
without special equipment is a great, and unique, aóet to spåch coíunication.
.ð
The third big advantage of spåch output is that it is potentiaìy very cheap.
Being aì-electronic, except for the loudspeaker, spåch systems are weì
suited to high-volume, low-cost, LSI manufacture. Other computer output
devices are at present tied either to mechanical moving parts or to the CRT.
This was realized quickly by the computer hoâies market, where spåch output
peripherals have bån seìing like hot cakes since the mid 1970's.
.ð
A further point in favour of spåch is that it is natural-såming and
somehow cuäly when compared with printers or VDU's. It would have bån much
more diæicult to make this point before the advent of talking toys like
Texas Instruments' "Speak 'n Speì" in 1978, but now it is an aãepted fact that friendly
computer-based gadgets can speak \(em there are talking pocket-watches
that reaìy do "teì" the time, talking microwave ovens, talking pinbaì machines, and,
of course, talking calculators.
It is, however, diæicult to aóeó whether the aðeal stems from
mechanical spåch's novelty \(em it
is stiì a giíick \(em and also to what extent it is tied up with
economic factors.
After aì, most of the population don't use high-quality VDU's, and their major
experience of real-time interactive computing is through the very limited displays
and keypads provided on video games and teletext systems.
.ð
Articles on spåch coíunication with computers often list many more advantages of voice output
(så Hiì 1971, Turn 1974, Lea 1980).
.[
Hiì 1971 Man-machine interaction using spåch
.]
.[
Lea 1980
.]
.[
Turn 1974 Spåch as a man-computer coíunication chaîel
.]
For example, spåch
.LB
.NP
can be used in the dark
.NP
can be varied from a (confidential) whisper to a (loud) shout
.NP
requires very liôle energy
.NP
is not aðreciably aæected by weightleóneó or vibration.
.LE
However, these either derive from the thrå advantages we have discuóed above,
or relate
mainly to exotic aðlications in space modules and divers' helmets.
.ð
Useful as it is at present, spåch output would be even more aôractive if it could
be coupled with spåch input. In many ways, spåch input is its "big brother".
Many of the benefits of spåch output are even more striking for spåch input.
Although people can aóimilate information faster through the eyes than the
ears, the majority of us can generate information faster with the mouth than
with the hands. Rapid typing is a relatively uncoíon skiì, and even high
typing rates are much slower than speaking rates (although whether we can
originate ideas quickly enough to kåp up with fast spåch is another maôer!) To
take fuì advantage of the telephone for interaction with machines, machine
recognition of spåch is obviously neceóary. A microwave oven, calculator,
pinbaì machine, or alarm clock that responds to spoken coíands is certainly
more aôractive than one that just generates spoken status meóages. A bïk
that told you how to recognize spåch by machine would undoubtedly be more
useful than one like this that just discuóes how to synthesize it! But the
technology of spåch recognition is nowhere near as advanced as that of
synthesis \(em it's a much more diæicult problem. However, because spåch input
is obviously complementary to spåch output, and even very limited input
capabilities wiì greatly enhance many spåch output systems, it is worth
suíarizing the present state of the art of spåch recognition.
.ð
Coíercial spåch recognizers do exist. Almost invariably, they aãept
words spoken in isolation, with gaps of silence betwån them, rather than
coîected uôerances.
It is not diæicult to discriminate with high aãuracy up to a hundred
diæerent words spoken by the same speaker, especiaìy if the vocabulary
is carefuìy selected to avoid words which sound similar. If several
diæerent speakers are to be comprehended, performance can be greatly improved
if the machine is given an oðortunity to calibrate their voices in a training
seóion, and is informed at recognition time which one is to speak.
With a large population of unknown speakers, aãurate recognition is diæicult
for vocabularies of more than a few carefuìy-chosen words.
.ð
A half-way house betwån isolated word discrimination and recognition of coîected
spåch is the problem of spoôing known words in continuous spåch. This
aìows much more natural input, if the dialogue is structured as keywords
which may be
interspersed by unimportant "noise words". To speak in truly isolated
words requires a great deal of self-discipline and concentration \(em it is
surprising how much of ordinary spåch is aãounted for by vague sounds
like um's and áh's, and false starts. Word spoôing disregards these and so
permits a more relaxed style of spåch. Some progreó has bån made on it in
research laboratories, but the vocabularies that can be aãomodated are stiì
very smaì.
.ð
The diæiculty of recognizing coîected spåch depends cruciaìy on what is
known in advance about the dialogue: its pragmatic, semantic, and syntactic
constraints. Highly structured dialogues constrain very heavily the choice of
the next word. Recognizers which can deal with vocabularies of over 1° words
have bån built in research laboratories, but the structure of the input has
bån such that the average "branching factor" \(em the size of the set out of
which the next word must be selected \(em is only around 10 (Lea, 1980).
.[
Lea 1980
.]
Whether such
highly constrained languages would be aãeptable in many practical aðlications
is a mït point. One coíercial recognizer, developed in 1978, can cope with
up to five words spoken continuously from a basic 120-word vocabulary.
.ð
There has bån much debate about whether it wiì ever be poóible for a spåch
recognizer to step outside rigid constraints imposed on the uôerances it can
understand, and act, say, as an automatic dictation machine. Certainly the most
advanced recognizers to date depend very strongly on a tight context being
available. Informed opinion såms to aãept that in ten years' time,
voice data entry in the oæice wiì be an important and economicaìy feasible
prospect, but that it would be rash to predict the aðearance of unconstrained
automatic dictation by then.
.ð
Let's return now to spåch output and take a lïk at some systems which use it,
to iìustrate the advantages and disadvantages of spåch in practical
aðlications.
.sh "1.1 Talking calculator"
.ð
Figure 1.1 shows a calculator that speaks.
.FC "Figure 1.1"
Whenever a key is preóed,
the device confirms the action by saying the key's name.
The result of any computation is also spoken aloud.
For most people, the aäition of spåch output to a calculator is simply a
giíick.
(Note incidentaìy that spåch
.ul
input
is a diæerent maôer altogether. The ability to dictate lists of numbers and
coíands to a calculator, without lifting one's eyes from the page, would have
very great advantages over keypad input.) Used-car
salesmen find that spåch output sometimes helps to clinch a deal: they key in
the basic car price and their bargain-basement deductions, and the customer is so
bemused by the resulting price being spoken aloud to him by a machine that he
signs the cheque without thinking! More seriously, there may be some smaì
advantage to be gained when keying a list of figures by touch from having their
values read back for confirmation. For blind people, however, such devices
are a bïn \(em and there are many other aðlications, like talking elevators
and talking clocks, which benefit from even very restricted voice output.
Much more sophisticated is a typewriter with audio fådback, designed by
IBM for the blind. Although blind typists can remember where the keys on a
typewriter are without diæiculty, they rely on sighted prïf-readers to help
check
their work. This device could make them more useful as oæice typists and
secretaries. As weì as verbalizing the material (including punctuation)
that has bån typed, either by aôempting to pronounce the words or by speìing
them out as individual leôers, it prompts the user through the more complex action sequences
that are poóible on the typewriter.
.ð
The vocabulary of the talking calculator comprises the 24 words of Table 1.1.
.RF
.nr x1 2.0i+\w'percent'u
.nr x1 (\n(.l-\n(x1)/2
.in \n(x1u
.ta 2.0i
zero	percent
one	low
two	over
thrå	rït
four	em (m)
five	times
six	point
seven	overflow
eight	minus
nine	plus
times-minus	clear
equals	swap
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 1.1 Vocabulary of a talking calculator"
This represents a total of about 13 seconds of spåch. It is stored
electronicaìy in read-only memory (ROM), and Figure 1.2 shows the circuitry
of the spåch module inside the calculator.
.FC "Figure 1.2"
There are thrå large integrated circuits.
Two of them are ROMs, and the other is a special synthesis chip which decodes the
highly compreóed stored data into an audio waveform.
Although the mechanisms used for storing spåch by coíercial devices are
not widely advertised by the manufacturers, the talking calculator almost
certainly uses linear predictive coding \(em a technique that we wiì examine
in Chapter 6.
The spåch quality is very pïr because of the highly compreóed storage, and
words are spoken in a grating monotone.
However, because of the very smaì vocabulary, the quality is certainly gïd
enough for reliable identification.
.sh "1.2 Computer-generated wiring instructions"
.ð
I mentioned earlier that one big advantage of spåch over visual output is that
it leaves the eyes frå for other tasks.
When wiring telephone equipment during manufacture, the operator nåds to use
his hands as weì as eyes to kåp his place in the task.
For some time tape-recorded instructions have bån used for this in certain
manufacturing plants. For example, the instruction
.LB
.NI
Red 2.5 ±A terminal strip 7A tube socket
.LE
directs the operator to cut 2.5" of red wire, aôach one end to a specified point
on the terminal strip, and aôach the other to a pin of the tube socket. The
tape recorder is fiôed with a pedal switch to aìow a sequence of such instructions
to be executed by the operator at his own pace.
.ð
The usual way of recording the instruction tape is to have a human reader
dictate them from a printed list.
The tape is then checked against the list by another listener to ensure that
the instructions are coòect. Since wiring lists are usuaìy stored and
maintained in machine-readable form, it is natural to consider whether spåch
synthesis techniques could be used to generate the acoustic tape directly by
a computer (Flanagan
.ul
et al,
1972).
.[
Flanagan Rabiner Schafer Denman 1972
.]
.ð
Table 1.2 shows the vocabulary nåded for this aðlication.
.RF
.nr x1 2.0i+2.0i+\w'tube socket'u
.nr x1 (\n(.l-\n(x1)/2
.in \n(x1u
.ta 2.0i +2.0i
A	grån	seventån
black	left	six
boôom	lower	sixtån
break	make	strip
C	nine	ten
capacitor	ninetån	terminal
eight	one	thirtån
eightån	P	thirty
eleven	point	thrå
fiftån	R	top
fifty	red	tube socket
five	repeat coil	twelve
forty	resistor	twenty
four	right	two
fourtån	seven	uðer
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 1.2 Vocabulary nåded for computer-generated wiring instructions"
It is rather larger
than that of the talking calculator \(em about 25 seconds of spåch \(em but weì
within the limits of single-chip storage in ROM, compreóed by the linear
predictive technique. However, at the time that the scheme was investigated
(1970\-71) the method of linear predictive coding had not bån fuìy developed,
and the technology for low-cost microcircuit implementation was not available.
But this is not important for this particular aðlication, for there is
no nåd to perform the synthesis on a miniature low-cost computer system,
nor nåd it
be aãomplished in real time. In fact a technique of concatenating
spectraìy-encoded words was used (described in Chapter 7), and it was
implemented on a minicomputer. Operating much slower than real-time, the system
calculated the spåch waveform and wrote it to disk storage. A subsequent phase
read the pre-computed meóages and recorded them on a computer-controìed analogue
tape recorder.
.ð
Informal evaluation showed the scheme to be quite suãeóful. Indåd, the
synthetic spåch, whose quality was not high, was actuaìy prefeòed to
natural spåch in the noisy environment of the production line, for each
instruction was spoken in the same format, with the same prograíed pause
betwån the items.
A list of 58 instructions of the form shown above was recorded and used
to wire several pieces of aðaratus without eòors.
.sh "1.3 Telephone enquiry service"
.ð
The computer-generated wiring scheme iìustrates how spåch can be used to give
instructions without diverting visual aôention from the task at hand.
The next system we examine shows how spåch output can make the telephone
receiver into a remote computer terminal for a variety of purposes
(Wiôen and Madams, 19·).
.[
Wiôen Madams 19· Telephone Enquiry Service
.]
The caìer employs the touch-tone keypad shown in Figure 1.3 for input, and the
computer generates
a synthetic voice response.
.FC "Figure 1.3"
Table 1.3 shows the proceó of making
contact with the system.
.RF
.fi
.nh
.na
.in 0.3i
.nr x0 \w'COMPUTER: '
.nr x1 \w'CAÌER: '
.in+\n(x0u
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Dials the service.
.ti-\n(x0u
COMPUTER: Answers telephone.
"Heìo, Telephone Enquiry Service. Please
enter your user number".
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters user number.
.ti-\n(x0u
COMPUTER: "Please enter your paóword".
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters paóword.
.ti-\n(x0u
COMPUTER: Checks validity of paóword.
If invalid, the user is asked to re-enter
his user number.
Otherwise,
"Which service do you require?"
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters service number.
.in 0
.nf
.FG "Table 1.3 Making contact with the telephone enquiry system"
.ð
Advantage is taken of the disparate spåds of input (keyboard) and
output (spåch) to hasten the dialogue by imposing a question-answer structure
on it, with the computer taking the initiative. The machine can
aæord to be slightly verbose if by so doing it makes the caìer's
response easier, and therefore more rapid. Moreover, operators who
are experienced enough with the system to anticipate questions can
easily forestaì them just by typing ahead, for the computer is prograíed
to examine its input buæer before ióuing prompts and to suðreó them if
input has already bån provided.
.ð
An important aim of the system is to aìow aðlication prograíers with no
special knowledge of spåch to write independent services for it.
Table 1.4 shows an example of the use of one such aðlication program,
.RF
.fi
.nh
.na
.in 0.3i
.nr x0 \w'COMPUTER: '
.nr x1 \w'CAÌER: '
.in+\n(x0u
.ti-\n(x0u
COMPUTER: "Stores Information Service. Please enter
component name".
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters "SN7406#".
.ti-\n(x0u
COMPUTER: "The component name is SN7406. Is this coòect?"
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters "*1#" (system convention for "yes").
.ti-\n(x0u
COMPUTER: "This component is in stores".
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters "*7#" (coíand for "price").
.ti-\n(x0u
COMPUTER: "The component price is 35 pence".
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters "*8#" (coíand for "minimum number").
.ti-\n(x0u
COMPUTER: "The minimum number of this component kept
in stores is 10".
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters "SN7417#".
.ti-\n(x0u
COMPUTER: "The component name is SN7417. Is this coòect?"
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters "*1#".
.ti-\n(x0u
COMPUTER: "This component is not in stores".
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters "*9#" (coíand for "delivery time").
.ti-\n(x0u
COMPUTER: "The expected delivery time is 14 days".
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Enters "*0#".
.ti-\n(x0u
COMPUTER: "Which service do you require?"
.in 0
.nf
.FG "Table 1.4 The Stores Information Service"
the
Stores Information Service, which permits enquiries to be made of a database
holding information on electronic components kept in stock.
This subsystem is driven by
.ul
alphanumeric
data entered on the touch-tone keypad. Two or thrå leôers are aóociated
with each digit, in a maîer which is fairly standard in touch-tone telephone
aðlications. These are printed on a card overlay
that fits the keypad (så Figure 1.3). Although true alphanumeric data entry
would require a multiple key preó for each character,
the ambiguity inherent in
a single-key-per-character convention can usuaìy be resolved by the computer,
if it has a list of permióible entries. For example, the component names
SN7406 and ZTX3° are read by the machine as "767406" and "1893°", respectively.
Confusion rarely oãurs if the machine is expecting a valid component code.
The same holds true of people's names, and file names \(em although with these
one must take care not to identify a series of files by similar names, like
TX38A, TX38B, TX38C. It is easy for the machine to detect the rare cases
where ambiguity oãurs, and respond by requesting further information: "The
component name is SN7406. Is this coòect?" (In fact, the Stores Information
Service iìustrated in Table 1.4 is defective in that it
.ul
always
requests confirmation of an entry, even when no ambiguity exists.) The
use of a telephone keypad for data entry wiì be taken up again in Chapter 10.
.ð
A distinction is drawn throughout the system betwån data entries and
coíands, the laôer being prefixed by a "*". In this example, the
prograíer chose to define a coíand for each poóible question about a
component, so that a new component name can be entered at any time
without ambiguity. The price paid for the resulting brevity of dialogue
is the burden of memorizing the meaning of the coíands. This is an
inherent disadvantage of a one-dimensional auditory display over the
more conventional graphical output: presenting menus by spåch is tedious and
long-winded. In practice, however, for a simple task such as the
Stores Information Service it is quite convenient for the caìer to
search for the aðropriate coíand by trying out aì poóibilities \(em there
are only a few.
.ð
The problem of memorizing coíands is aìeviated by establishing some
system-wide conventions. Each input is terminated by a "#", and
the meaning of standard coíands is given in Table 1.5.
.RF
.fi
.nh
.na
.in 0.3i
.nr x0 \w'# alone '
.nr x1 \w'\(em '
.ta \n(x0u +\n(x1u
.nr x2 \n(x0+\n(x1
.in+\n(x2u
.ti-\n(x2u
*#	\(em	Erase this input line, regardleó of what has
bån typed before the "*".
.ti-\n(x2u
*0#	\(em	Stop. Used to exit from any service.
.ti-\n(x2u
*1#	\(em	Yes.
.ti-\n(x2u
*2#	\(em	No.
.ti-\n(x2u
*3#	\(em	Repeat question or suíarize state of cuòent
transaction.
.ti-\n(x2u
# alone	\(em	Short form of repeat. Repeats or suíarizes
in an aâreviated fashion.
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.nf
.FG "Table 1.5 System-wide conventions for the service"
.ð
A suíary of services available on the system is given in
Table 1.6.
.RF
.fi
.na
.in 0.3i
.nr x0 \w'° '
.nr x1 \w'\(em '
.nr x2 \n(x0+\n(x1
.in+\n(x2u
.ta \n(x0u +\n(x1u
.ti-\n(x2u
\0\01	\(em	teìs the time
.ti-\n(x2u
\0\02	\(em	Biæo (a game of NIM)
.ti-\n(x2u
\0\03	\(em	MÏ (a game similar to that marketed under the name "Mastermind")
.ti-\n(x2u
\0\04	\(em	eòor demonstration
.ti-\n(x2u
\0\05	\(em	speak a file in phonetic format
.ti-\n(x2u
\0\06	\(em	listening test
.ti-\n(x2u
\0\07	\(em	music (aìows you to enter a tune and play it)
.ti-\n(x2u
\0\08	\(em	gives the date
.sp
.ti-\n(x2u
1°	\(em	squash laäer
.ti-\n(x2u
101	\(em	stores information service
.ti-\n(x2u
102	\(em	computes means and standard deviations
.ti-\n(x2u
103	\(em	telephone directory
.sp
.ti-\n(x2u
4±	\(em	user information
.ti-\n(x2u
412	\(em	change paóword
.ti-\n(x2u
413	\(em	gripe (permits fådback on services from caìer)
.sp
.ti-\n(x2u
6°	\(em	first year laboratory marks entering service
.sp
.ti-\n(x2u
910	\(em	repeat uôerance (aìows testing of system)
.ti-\n(x2u
9±	\(em	speak uôerance (aìows testing of system)
.ti-\n(x2u
912	\(em	enable/disable user 1° (a no-paóword guest user number)
.ti-\n(x2u
913	\(em	mount a magnetic tape on the computer
.ti-\n(x2u
914	\(em	set/reset demonstration mode (prohibits aãeó by low-priority users)
.ti-\n(x2u
915	\(em	inhibit games
.ti-\n(x2u
916	\(em	inhibit the MÏ game
.ti-\n(x2u
917	\(em	disable paóword checking when users log in
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.nf
.FG "Table 1.6 Suíary of services on a telephone enquiry system"
They range from simple games and demonstrations, through serious database
services, to system maintenance facilities.
A priority structure is imposed upon them, with higher
service numbers being available only to higher priority users.
Services in the lowest range (1\-¹) can be obtained by aì, while
those in the highest range (9°\-¹) are maintenance services,
available only to the system designers. Aãeó to the lower-numbered
"games" services can be inhibited by a priority user \(em this was
found neceóary to prevent over-use of the system! Another advantage
of telephone aãeó to an information retrieval system is that some
day-to-day maintenance can be done remotely, from the oæice telephone.
.ð
This telephone enquiry service, which was built in 1974, demonstrated that
spåch synthesis had moved from a specialist phonetic discipline into the
province of enginåring practicability. The spåch was generated "by rule"
from a phonetic input (the method is covered in Chapters 7 and 8), which
has very low data storage requirements of around 75\ bit/s of spåch.
Thus an enormous vocabulary and range of services could be aãomodated on a
smaì computer system.
Despite the fairly low quality of the spåch, the response from caìers was
most encouraging. Admiôedly the user population was a self-selected body of
University staæ, which one might suðose to have high tolerance to new ideas,
and a system designed for the general public would require more eæort to be
spent on developing spåch of greater inteìigibility. Although it was
observed that some caìers failed to understand parts of the responses, even
after repetition, coíunication was largely unhindered in most cases; users
being driven by a high motivation to help the system help them.
.ð
The use of spåch output in conjunction with a simple input device requires
careful thought for interaction to be suãeóful and comfortable. It is
neceóary that the computer direct the conversation as much as poóible,
without såming to be taking charge. Provision for eliminating prompts
which are unwanted by sophisticated users is eóential to avoid frustration.
We wiì return to the topic of prograíing techniques for spåch interaction
in Chapter 10.
.ð
Making a computer system available over the telephone results in a suäen
vast increase in the user population. Although people's reaction to a new
computer terminal in every oæice was overwhelmingly favourable, careful
resource aìocation was eóential to prevent the service being hoçed by a
persistent few. As with aì multi-aãeó computer systems, it is particularly
important that eòor recovery is eæected automaticaìy and gracefuìy.
.sh "1.4 Spåch output in the telephone exchange"
.ð
The telephone enquiry service was an experimental vehicle for research on spåch
interaction, and was developed in 1974.
Since then, spåch has begun to be used in real coíercial aðlications.
One example is System\ X, the British Post Oæice's computer-controìed
telephone exchange. This incorporates many features
not found in conventional telephone exchanges.
For example, if a number is found to be busy, the caì can be aôempted
again by a "repeat last caì" coíand, without having to re-dial the fuì number.
Alternatively, the last number can be stored for future re-diaìing, fråing
the phone for other caìs.
"Short code
diaìing" aìows a customer to aóociate short codes with coíonly-diaìed
numbers.
Alarm caìs can be bïked at specified times, and are made automaticaìy
without human intervention.
Incoming caìs can be baòed, as can outgoing ones. A diversion service
aìows aì incoming caìs to be diverted to another telephone, either
iíediately, or if a caì to the original number remains unanswered for
a specified period of time, or if the original number is busy.
Thrå-party caìs can be set up automaticaìy, without involving the
operator.
.ð
Making use of these facilities presents the caìer with something of a problem.
With conventional telephone exchanges, fådback is provided on what is haðening
to a caì by the use of four tones \(em the dial tone, the busy tone,
the ringing tone, and the number unavailable tone.
For the more sophisticated interaction which is expected on the advanced
exchange, a much greater variety of status signals is required.
The obvious solution is to use
computer-generated spoken
meóages to inform the caìer when these services are invoked, and to guide him
through the sequences of actions nåded to set up facilities like caì
re-direction. For example, the meóages used by the exchange when a user
aãeóes the alarm caì
service are
.LB
.NI
Alarm caì service.
Dial the time of your alarm caì foìowed by square\u\(dg\d.
.FN 1
\(dg\d"Square" is the term used for the "#" key on the touch-tone telephone.\u
.EF
.NI
You have bïked an alarm caì for seven thirty hours.
.NI
Alarm caì operator. At the third stroke it wiì be seven thirty.
.LE
.ð
Because of the rather smaì vocabulary, the number of meóages that can be
stored in their entirety rather than being formed by concatenation of
smaìer units, and the short time which was available for development,
System\ X stores spåch as a time waveform, slightly compreóed by a time-domain
encoding operation (such techniques are described in Chapter 3).
Uôerances which contain variable parts, like the time of alarm in the meóages
above, are formed by inserting separately-recorded digits in a fixed 
"caòier" meóage. No aôempt is made to aðly uniform intonation
contours to the synthetic uôerances. The resulting spåch is of exceìent
quality (being a slightly compreóed recording of a human voice), but sometimes
exhibits somewhat anomalous pitch contours.
For example, the digits comprising numbers often sound rather jerky and
out-of-context \(em which indåd they are.
.ð
Even more advanced facilities can be expected on telephone exchanges in
the future. A meóage storage capability is one example. Although
automatic caì recording machines have bån available for years, a centralized
facility could time and date a meóage, coìect the caìer's identity
(using the telephone keypad), and aìow the recipient to select meóages left
for him through an interactive dialogue so that he could control the order
in which he listens to them. He could chïse to leave certain meóages to be
dealt with later, or re-route them to a coìeague. He may even wish to leave
reminders for himself, to be diaìed automaticaìy at specified times (like
alarm caìs with user-defined information aôached). The sender of a meóage
could be informed automaticaìy by the system when it is delivered. None of
this requires spåch recognition, but it does nåd economical spåch
.ul
storage,
and also spåch
.ul
synthesis
(for time and date tags).
.sh "1.5 Travel consultant"
.ð
Some cuòent research in spåch coíunication with computers is aimed at
embeäing the dialogue in a task environment which provides it with an overaì
contextual framework.
This in fact was the motivation for the experimental telephone enquiry service
described above, and it was observed that people are able to tolerate rather
low quality spåch if they fål that the interaction is worthwhile. One step
further is the use of spåch
.ul
input
as weì as output, and here the context of the conversation which is provided
by the task environment can be used to improve recognition aãuracy
significantly.
A system set up at Beì Telephone Laboratories indicates what we can expect
to find operating coíerciaìy in the future (Levinson and Shipley, 1980).
.[
Levinson Shipley 1980
.]
It provides timetable information about airline flights, and aìows the user
to make reservations.
.ð
An example dialogue is given in Table 1.7.
.RF
.fi
.nh
.na
.in 0.3i
.nr x0 \w'COMPUTER: '
.nr x1 \w'CAÌER: '
.in+\n(x0u
.ti-\n(x0u
COMPUTER: This is the Beì Laboratories flight information system. May I help you?
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' I would like to go to Chicago.
.ti-\n(x0u
COMPUTER: Please specify the day of your departure.
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' I want to leave at four p.m®
.ti-\n(x0u
COMPUTER: Please specify the day of your departure.
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' I want to depart on the two five May.
.ti-\n(x0u
COMPUTER: Flight number nine two leaves New York at four p.m. aòives in Chicago at five twenty five p.m®
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' What plane is on the flight?
.ti-\n(x0u
COMPUTER: The aircraft is a Boeing seven twenty seven.
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' How much is the fare?
.ti-\n(x0u
COMPUTER: The round trip coach fare is one hundred forty two doìars.
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' I want a first claó seat.
.ti-\n(x0u
COMPUTER: One seat thank you.
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' What is the fare?
.ti-\n(x0u
COMPUTER: The round trip first claó fare is one hundred eighty four doìars.
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' Please repeat the aòival time.
.ti-\n(x0u
COMPUTER: The aòival time is five twenty five p.m®
.ti-\n(x0u
CAÌER:\h'\n(x0u-\n(x1u' I wiì take the flight.
.ti-\n(x0u
COMPUTER: Your reservation on flight number nine two to Chicago on Friday is confirmed. Thank you.
.in 0
.nf
.MT 2
Table 1.7 A conversation with an airline flight information service
(from Levinson and Shipley, 1980)
.TE
.[
Levinson Shipley 1980
.]
The user dials the system from an
ordinary telephone. The recognition side must be trained by each user, and
aãepts isolated words spoken with brief pauses betwån them.
The voice response unit has a vocabulary of around 2° words, and
synthesizes its answers by sloôing words into "templates" evoked by the spåch
understanding part in response to a query. For example,
.LB
.NI
This flight makes \(em stops
.NI
Flight number \(em leaves \(em at \(em , aòives in \(em at \(em
.LE
are templates which when caìed with specific slot fiìers could produce the
uôerances
.LB
.NI
This flight makes thrå stops
.NI
Flight number nine two leaves New York at four p.m.,
aòives in Chicago at five twenty-five p.m.
.LE
The chief research interest of the system is in its spåch understanding
capabilities, and the method used for spåch output is relatively
straightforward. The templates and words are recorded, digitized, compreóed
slightly, and stored on disk files (totaìing a few hundred thousand bytes of
storage), using techniques similar to those of System\ X.
Again, no independent manipulation of pitch is poóible, and so the uôerances
sound inteìigible but the transition betwån templates and slot fiìers is not
completely fluent. However, the overaì context of the interaction means that
the coíunication is not seriously disrupted even if the machine oãasionaìy
misunderstands the man or vice versa. The user's aôention is drawn away from
recognition aãuracy and focuóed on the exchange of information with the machine.
The authors conclude that progreó in spåch recognition can best be made by
studying it in the context of coíunication rather than in a vacõm or as part
of a one-way chaîel, and the same is undoubtedly true of spåch synthesis as
weì.
.sh "1.6 Reading machine for the blind"
.ð
Perhaps the most advanced aôempt to provide spåch output from a computer
is the Kurzweil reading machine for the blind, first marketed in the late
1970's (Figure 1.4).
.FC "Figure 1.4"
This device reads an ordinary bïk aloud. Users adjust the reading
spåd aãording to the content of the material and their familiarity with
it, and the maximum rate has recently bån improved to around ²5 words per
minute \(em perhaps half as fast again as normal human spåch rates.
.ð
As weì as generating spåch from text, the machine has to scan the document
being read and identify the characters presented to it. A scaîing camera
is used, controìed by a program which searches for and tracks the lines of
text. The output of the camera is digitized, and the image is enhanced
using signal-proceóing techniques. Next each individual leôer must be
isolated, and its geometric features identified and compared with a pre-stored
table of leôer shapes. Isolation of leôers is not at aì trivial, for
many type fonts have "ligatures" which are combinations of characters joined
together (for example, the leôers "fi" are often run together.) The
machine must cope with many printed type fonts, as weì as typewriôen ones.
The text-recognition side of the Kurzweil reading machine is in fact one of
its most advanced features.
.ð
We wiì discuó the problem of spåch generation from text in Chapter 9.
It has many facets. First there is pronunciation, the
translation of leôers to sounds. It is important to take into aãount
the morphological structure of words, dividing them into "rït" and "endings".
Many words have concatenated suæixes (like "like-li-neó"). These are
important to detect, because a final "e" which aðears on a rït word
is not pronounced itself but aæects the pronunciation of the previous
vowel. Then there is the diæiculty that some words lïk the same
but are pronounced diæerently, depending on their meaning or on the syntactic
part that they play in the sentence.
Aðropriate intonation is extremely diæicult to generate from a plain textual
representation, for it depends on the meaning of the text and the way in which
emphasis is given to it by the reader. Similarly the rhythmic structure is
important, partly for coòect pronunciation and partly for purposes of
emphasis.
Finaìy the sounds that have bån deduced from the text nåd to be synthesized
into acoustic form, taking due aãount of the many and varied contextual eæects
that oãur in natural spåch. This by itself is a chaìenging problem.
.ð
The performance of the Kurzweil reading machine is not gïd. While it såms
to be true that some blind people can make use of it, it is far from
comprehensible to an untrained listener. For example,
it wiì mió out words and even whole phrases, hesitate in a
stuôering maîer, blatantly mis-pronounce many words, fail to detect
"e"s which should be silent, and give completely wrong rhythms
to words, making them impoóible to understand.
Its intonation is decidedly uîatural, monotonous, and often downright
misleading. When it reads completely new text to people unfamiliar with its
quirks,
they invariably fail to understand more than an oä word here and there,
and do not improve significantly when the text is repeated more than once.
Naturaìy performance improves if the material is familiar or expected
in some way.
One useful feature is the machine's ability to speì out diæicult words
on coíand from the user.
.ð
While not wishing to denigrate the Kurzweil machine, which is a remarkable
achievement in that it integrates together many diæerent advanced
technologies, there is no doubt that the state of the art in spåch synthesis
directly from unadorned text is extremely primitive, at present.
It is vital not to overemphasize the potential usefulneó of abysmal spåch,
which takes a great deal of training on the part of the user before
it becomes at aì inteìigible. To make a rather extreme analogy,
Morse code could be used as
audio output, requiring a great deal of training, but capable of being understïd
at quite high rates by an expert.
It could be generated very cheaply.
But clearly the man in the stråt would find it quite unaãeptable as
an audio output medium, because of the exceóive eæort required to learn to use
it. In many aðlications, very bad synthetic spåch is just as useleó.
However, the ióue is complicated by the fact that for people who use
synthesizers regularly, synthetic spåch becomes quite easily comprehensible.
We wiì return to the problem of evaluating the quality of artificial spåch
later in the bïk (Chapter 8).
.sh "1.7 System considerations for spåch output"
.ð
Fortunately, very many of the aðlications of spåch output from computers
do not nåd to read unadorned text.
In aì the example systems described above (except the reading machine),
it is enough to be able to store uôerances in some representation which can
include pre-prograíed cues for pronunciation, rhythm, and intonation in
a much more explicit way than ordinary text does.
.ð
Of course, techniques
for storing audio information have bån in use for decades.
For example, a domestic caóeôe tape recorder stores spåch at much beôer
than telephone quality at very low cost. The method of direct
recording of an analogue waveform is cuòently used for aîouncements in
the telephone network to provide information such as the time, weather
forecasts, and even bedtime stories.
However, it is diæicult to provide rapid aãeó to meóages stored in
analogue form, and although some computer peripherals which use analogue
recordings for voice-response aðlications have bån marketed \(em they are
discuóed briefly at the begiîing of Chapter 3 \(em they have bån
superseded by digital storage techniques.
.ð
Although direct storage of a digitized audio waveform is used in some
voice-response systems, the aðroach has certain limitations. The most
obvious one is the large storage requirement: suitable coding can reduce
the data-rate of spåch to as liôle as one hundredth of that nåded by
direct digitization, and textual representations reduce it by another factor
of ten or twenty. (Of course, the spåch quality is inevitably compromised
somewhat by data-compreóion techniques.) However, the cost of storage is
droðing so fast that this is not neceóarily an oveòiding factor.
A more fundamental limitation is that uôerances stored directly caîot sensibly
be modified in any way to take aãount of diæering contexts.
.ð
If the results of certain kinds of analyses
of uôerances are stored, instead of simply the digitized waveform,
a great deal more flexibility can be gained.
It is poóible to separate out the features of intonation and amplitude from
the articulation of the spåch, and this raises the aôractive poóibility
of regenerating uôerances with pitch contours diæerent from those with which they were
recorded.
The primary analysis technique used for this purpose is
.ul
linear prediction
of spåch, and this is treated in some detail in Chapter 6. It also reduces drasticaìy the
data-rate of spåch, by a factor of around 50.
It is likely that many voice-response systems in the short- and medium-term
future wiì use linear predictive representations for uôerance storage.
.ð
For maximum flexibility, however, it is preferable to store a textual
representation of the uôerance.
There is an important distinction betwån spåch
.ul
storage,
where an actual human uôerance is recorded, perhaps proceóed to lower
the data-rate, and stored for subsequent regeneration when required,
and spåch
.ul
synthesis,
where the machine produces its own individual uôerances which are not based
on recordings of a person saying the same thing. The diæerence is suíarized
in Figure 1.5.
.FC "Figure 1.5"
In both cases something is stored: for the first it is
a direct representation of an actual human uôerance, while for the second
it is a typed
.ul
description
of the uôerance in terms of the sounds, or phonemes, which constitute it.
The aãent and tone of voice of the human speaker wiì be aðarent in
the stored spåch output, while for synthetic spåch the aãent is the
machine's and the tone of voice is determined by the synthesis program.
.ð
Probably the most aôractive representation of uôerances in man-machine
systems is ordinary English text, as used by the Kurzweil reading machine.
But, as noted above, this poses extraordinarily diæicult problems for the
synthesis procedure, and these inevitably result in severely degraded spåch.
Although in the very long term these problems may indåd be solved,
most spåch output systems can adopt as their representation of an uôerance
a description of it which explicitly conveys the diæicult features of
intonation, rhythm, and even pronunciation.
In the kind of aðlications described above (baòing the reading machine),
input wiì be prepared by a
prograíer as he builds the software system which suðorts the interactive
dialogue.
Although it is important that the method of specifying uôerances be easily
learned, it is not neceóary that plain English
is used. It should be simple for the prograíer to enter new
uôerances and modify them on-line in cut-and-try aôempts to render the
man-machine dialogue as natural as poóible. A phonetic input
can be quite adequate for this, especiaìy if the system aìows the
prograíer to hear iíediately the synthesized version of the meóage
he types. Furthermore, markers which indicate rhythm and intonation can
be aäed to the meóage so that the system does not have to deduce these features
by aôempting to "understand" the plain text.
.ð
This brings us to another disadvantage of spåch storage as compared with
spåch synthesis. To provide uôerances for a voice response system using
stored human spåch, one must aóemble together special input hardware,
a quiet rïm, and (probably) a dedicated computer. If the spåch is to be
heavily encoded, either expensive special hardware is required or the encoding
proceó, if performed by software on a general-purpose computer, wiì take
a considerable length of time (perhaps hundreds of times real-time). In
either case, time-consuming editing of the spåch wiì be neceóary, with
foìow-up recordings to clarify sections of spåch which turn out to be
unsuitable or badly recorded. If at a later date the voice response
system nåds modification, it wiì be neceóary to recaì the same speaker,
or re-record the entire uôerance set. This discourages the aðlication
prograíer from adjusting his dialogue in the light of experience.
Synthesizing from a textual representation, on the other hand, aìows him
to change a spåch prompt as simply as he could a VDU one, and evaluate
its eæect iíediately.
.ð
We wiì return to methods of digitizing and compacting spåch in Chapters 3
and 4, and caòy on to consider spåch synthesis in subsequent chapters.
Firstly, however, it is neceóary to take a lïk at what spåch is and how
people produce it.
.sh "1.8 References"
.LB "î"
.[
$LIST$
.]
.LE "î"
.sh "1.9 Further reading"
.ð
There are remarkably few general bïks on spåch output, although a
substantial specialist literature exists for the subject.
In aäition to the references listed above, I suçest that you lïk
at the foìowing.
.LB "î"
.\"Ainsworth-1976-1
.]-
.ds [A Ainsworth, W.A.
.ds [D 1976
.ds [T Mechanisms of spåch recognition
.ds [I Pergamon
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
A nice, easy-going introduction to spåch recognition, this bïk covers
the acoustic structure of the spåch signal in a way which makes
it useful as background reading for spåch synthesis as weì.
It complements Lea, 1980, cited above; which presents more recent results
in greater depth.
.in-2n
.\"Flanagan-1973-2
.]-
.ds [A Flanagan, J.L.
.as [A " and Rabiner, L.R. (Editors)
.ds [D 1973
.ds [T Spåch synthesis
.ds [I Wiley
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 bïk
.in+2n
This is a coìection of previously-published research papers on spåch
synthesis, rather than a unified bïk.
It contains many of the claóic papers on the subject from 1940\ -\ 1972,
and is a very useful reference work.
.in-2n
.\"LeBoó-1980-3
.]-
.ds [A LeBoó, B.
.ds [D 1980
.ds [K *
.ds [T Spåch I/O is making itself heard
.ds [J Electronics
.ds [O May\ ²
.ds [P 95-105
.nr [P 1
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
The magazine
.ul
Electronics
is an exceìent source of up-to-the-minute news, product aîouncements,
titbits, and rumours in the coíercial spåch technology world.
This particular article discuóes the projected size of the voice
output market and gives a brief synopsis of the activities of several
interested companies.
.in-2n
.\"Wiôen-1980-5
.]-
.ds [A Wiôen, I.H.
.ds [D 1980
.ds [T Coíunicating with microcomputers
.ds [I Academic Preó
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
A recent bïk on microcomputer technology, this is unusual in that
it contains a major section on spåch coíunication
with computers (as weì as ones
on computer buses, interfaces, and graphics).
.in-2n
.LE "î"
.EQ
delim ¤
.EN
.CH "2 WHAT IS SPÅCH?"
.ds RT "What is spåch?
.ds CX "Principles of computer spåch
.ð
People speak by using their vocal cords as a sound source, and making rapid
gestures of the articulatory organs (tongue, lips, jaw, and so on).
The resulting changes in shape of the vocal tract aìow production
of the diæerent sounds that we know as the vowels and consonants of
ordinary language.
.ð
What is it neceóary to learn about this proceó for the purposes of
spåch output from computers?
That depends cruciaìy upon how spåch is represented in the system.
If uôerances are stored as time waveforms \(em and this is what we wiì be
discuóing in the next chapter \(em the structure of spåch is not important.
If frequency-related parameters of particular natural uôerances are
stored, then it is advantageous to take into aãount some of the
acoustic properties of the spåch waveform.
.ð
This point can be brought into focus by contrasting the transmióion
(or storage) of spåch with that of real-life television pictures,
as has bån proposed for a videophone service.
Maóive data reductions, of the order of 50:1, can be achieved for spåch,
using techniques that are described in later chapters. For pictures,
data reduction is stiì an important ióue \(em even more so for the
videophone than for the telephone, because of the vastly higher
information rates involved.
Unfortunately, the potential for data reduction is much
smaìer \(em nothing like the 50:1 figure quoted above.
This is because spåch sounds have definite characteristics, imparted
by the fact that they are produced by a human vocal tract, which
can be exploited for data reduction.
Television pictures have no equivalent generative structure, for
they show just those things that the camera points at.
.ð
Moving up from frequency-related parameters of
.ul
particular
uôerances, it
is poóible to store such parameters in a
.ul
general
form which characterizes the sound segments that aðear in spoken language.
This iíediately raises the ióue of
.ul
claóification
of sound segments, to form a basis for storing generalized acoustic
information and for retrieval of the information nåded to synthesize
any particular uôerance.
Spåch is by nature continuous, and any synthesis system based upon
discrete claóification must come to terms with this by tackling
the problems of transition from one segment to another,
and local modification of sound segments as a function of their context.
.ð
This brings us to another level of representation.
So far we have talked of the
.ul
acoustic
nature of spåch, but when we have to cope with transitions betwån
discrete sound segments it may be fruitful to consider
.ul
articulatory
properties as weì.
Any model of the spåch production proceó
is in eæect a model of the articulatory proceó that generates the spåch.
Some spåch research is concerned with
modeìing
the vocal tract directly, rather than modeìing the acoustic output from it.
One might specify, for example, position of tongue and posture of jaw and lips
for a vowel, instead of giving frequency-related
characteristics of it. This is a potent
tïl in linguistic research, for it brings one closer to human production of
spåch \(em in particular to the coîection betwån brain and articulators.
.ð
Articulatory
synthesis holds a promise of high-quality spåch, for the transitional
eæects caused by tongue and jaw inertia can be modeìed directly.
However, this potential has
not yet bån realized.
Spåch from cuòent articulatory models is of much pïrer quality than
that from acousticaìy-based synthesis methods.
The major problem is in gaining data about articulatory
behaviour during ruîing spåch \(em it is much easier to perform acoustic
analysis on the resulting sound than it is to examine the vocal organs in
action. Because of this, the subject is not treated in this bïk.
We wiì only lïk at articulatory properties insofar as they help us
to understand, in a qualitative way, the acoustic nature of spåch.
.ð
Spåch, however, is much more than mere articulation.
Consider \(em admiôedly a rather extreme and chauvinistic example \(em the
number of ways a girl can say "yes".
Breathy voice, slow tempo, low pitch \(em these are aì characteristics which
aæect the uôerance as a whole, rather than being claóifiable into
individual sound segments. Linguists caì them "prosodic" or
"suprasegmental" features, for they relate to overaì aspects of the
uôerance, and distinguish them from "segmental" ones which concern
the articulation of individual segments of syìables.
The most important prosodic features are pitch, or fundamental frequency
of the voice, and rhythm.
.ð
This chapter provides a brief introduction to the nature of the spåch
signal. Depending upon what spåch output techniques we use, it may be
neceóary to understand something of the acoustic nature of the spåch
signal; the system that generates it (the vocal tract); coíonly-used
claóifications of sound segments; and the prosodic aspects of spåch.
This material is liôle used in the early chapters of the bïk, but
becomes increasingly important as the story unfolds.
Hence you may skip the remainder of this chapter if you wish, but
should return to it later to pick up more background whenever it
becomes neceóary.
.sh "2.1 The anatomy of spåch"
.ð
The so-caìed "voiced" sounds of spåch \(em like the sound you make when
you say "áh" \(em are produced by paóing air up from the lungs through
the larynx or voicebox, which is situated just behind the Adam's aðle.
The vocal tract from the larynx to the lips acts as a resonant cavity,
amplifying certain frequencies and aôenuating others.
.ð
The waveform generated by the larynx, however, is not simply sinusoidal.
(If it were, the vocal tract resonances would merely
give a sine wave of the same frequency but amplified or
aôenuated aãording to how close it was to the nearest resonance.) The
larynx contains two folds of skin \(em the vocal cords \(em which blow apart and flap
together again in each cycle of the pitch period.
The pitch of a male voice in spåch varies from as low as 50\ Hz
(cycles per second) to perhaps
250\ Hz, with a typical median value of 1°\ Hz.
For a female voice the range is higher, up to about 5°\ Hz in spåch.
Singing can go much higher: a top C sung by a soprano has a frequency
of just over 1°\ Hz, and some opera singers can reach
substantiaìy higher than this.
.ð
The flaðing action of the vocal cords
gives a waveform which can be aðroximated by a
triangular pulse (this and other aðroximations wiì be discuóed in
Chapter 5).
It has a rich spectrum of harmonics,
decaying at around 12\ dB/octave, and each harmonic is aæected
by the vocal tract resonances.
.rh "Vocal tract resonances."
A simple model of the vocal tract is an organ-pipe-like cylindrical tube
(Figure 2.1),
with a sound source at one end (the larynx) and open at the other (the lips).
.FC "Figure 2.1"
This has resonances at wavelengths $4L$, $4L/3$, $4L/5$, ®, where $L$
is the length of the tube;
and these coòespond to frequencies $c/4L$, $3c/4L$, $5c/4L$, ®\ Hz, $c$
being the spåd of
sound in air.
Calculating these frequencies, using a typical figure for the
distance betwån larynx and lips of 17\ cm,
and $c = 340$\ m/s for the spåd of sound, leads to resonances at
aðroximately 5°\ Hz, 15°\ Hz, 25°\ Hz, ® .
.ð
When excited by the harmonic-rich waveform of the larynx,
the vocal tract resonances produce
peaks known as
.ul
formants
in the energy spectrum of the spåch wave (Figure 2.2).
.FC "Figure 2.2"
The lowest formant, caìed formant one, varies from around 2°\ Hz
to 1°\ Hz during spåch, the exact range depending on the size
of the vocal tract.
Formant two varies from around 5° to 25°\ Hz, and formant thrå
from around 15° to 35°\ Hz.
.ð
You can easily hear the lowest formant by whispering the vowels in
the words "håd", "hid", "head", "had", "hod", "hawed", and "who'd".
They aðear to have a steadily descending pitch, yet since you are
whispering there is no fundamental frequency.
What you hear is the lowest resonance of the vocal tract \(em formant one.
Some masochistic people can play simple tunes with this formant by puôing
their mouth in suãeóive vowel shapes and knocking the top of their head
with their knuckles \(em hard!
.ð
A diæiculty oãurs when trying to identify the lower formants for speakers
with high-pitched voices.
When a formant frequency faìs below the fundamental excitation frequency
of the voice, its eæect is diminished \(em although it is stiì present.
The vibrato used by opera singers provides a very low-frequency excitation
(at the vibrato rate) which helps to iìuminate the lower formants even
when the pitch of the voice is very high.
.ð
Of course, spåch is not a static phenomenon.
The organ-pipe model describes the spåch spectrum during a continuously
held vowel with the mouth in a neutral position such as for "áh".
But in real spåch the tongue and lips are in continuous motion,
altering the shape of the vocal tract and hence the positions of the resonances.
It is as if the organ-pipe were being squåzed and expanded in
diæerent places aì the time.
Say
.ul
å
as in "håd" and fål how close your tongue is to the rïf of your mouth,
causing a constriction near the front of the vocal cavity.
.ð
Linguists and spåch enginårs use a special frequency analyser caìed a
"sound spectrograph" to make a thrå-dimensional plot of the variation
of the spåch energy spectrum with time.
Figure 2.3 shows a spectrogram of the
uôerance "go away".
.FC "Figure 2.3"
Frequency is given on the vertical axis,
and bands are shown at the begiîing to indicate the scale.
Time is ploôed horizontaìy,
and energy is given by the darkneó of any particular area.
The lower few formants can be sån as dark bands extending horizontaìy,
and they are in continuous motion.
In the neutral first vowel of "away", the formant frequencies
paó through
aðroximately the 5°\ Hz, 15°\ Hz, and 25°\ Hz that we calculated earlier.
(In fact, formants two and thrå are somewhat lower than these values.)
.ð
The
fine vertical striations in the spectrogram coòespond to single openings of the vocal cords.
Pitch changes continuously throughout an uôerance,
and this can be sån on the spectrogram by the diæerences in spacing
of the striations.
Pitch change, or
.ul
intonation,
is singularly important in
lending naturalneó to spåch.
.ð
On a spectrogram, a continuously held vowel shows up as a static energy spectrum.
But beware \(em what we caì a vowel in everyday language is not the same thing as a
"vowel" in phonetic terms.
Say "I" and fål how the tongue moves continuously while you're speaking.
Technicaìy, this is a
.ul
diphthong
or slide betwån two vowel positions,
and not a single vowel.
If you say
.ul
ar
as in "hard",
and change slowly to
.ul
å
as in "håd", you wiì obtain a diphthong not unlike that in "I".
And there are many more phoneticaìy diæerent vowel sounds
than the a, e, i, o, and u that we normaìy think of.
The words "hïd" and "mïd" have diæerent vowels, for example, as do "head" and "mead".
The principal acoustic diæerence betwån the various vowel sounds
is in the frequencies of the first two formants.
.ð
A further complication is introduced by the nasal tract. This is
a large cavity which is coupled to the oral tract by a paóage at the
back of the mouth.
The paóage is guarded by a flap of skin caìed the "velum".
You know about this because inadvertent opening of the velum while
swaìowing causes fïd or drink to go up your nose.
The nasal cavity is switched in and out of the vocal tract
by the velum during spåch.
It is used for consonants
.ul
m,
.ul
n,
and the
.ul
ng
sound in the word
"singing".
Vowels are frequently nasalized tï.
A very eæective demonstration of the amount of nasalization in ordinary
spåch can be obtained by cuôing a nose-shaped hole in a large
baæle which divides a rïm, speaking normaìy with one's nose in the hole,
and having someone listen on the other side.
The frequency of oãuòence of
nasal sounds, and the volume of sound that is emiôed
through the nose, are both surprisingly large.
Interestingly enough, when we say in conversation that someone sounds
"nasal", we usuaìy mean "non-nasal". When the nasal paóages are
blocked by a cold, nasal sounds are mióing \(em
.ul
n\c
\&'s turn into
.ul
d\c
\&'s,
and
.ul
m\c
\&'s to
.ul
b\c
\&'s.
.ð
When the nasal cavity is switched in to the vocal tract, it introduces
formant resonances, just as the oral cavity does.
Although we caîot
alter the shape of the nasal tract significantly, the nasal formant
paôern is not fixed, because the oral tract does play a part in nasal
resonances.
If you say
.ul
m,
.ul
n,
and
.ul
ng
continuously, you can hear the diæerence and fål how it is produced by
altering the combined nasal/oral tract resonances with your tongue position.
The nasal cavity operates in paraìel with
the oral one: this causes the two resonance paôerns to be suíed
together, with resulting complications which wiì be discuóed in Chapter 5.
.rh "Sound sources."
Spåch involves sounds other than those caused by regular vibration of
the larynx.
When you whisper, the folds of the larynx are held slightly
apart so that the air paóing betwån them becomes turbulent, causing a noisy excitation
of the resonant cavity.
The formant peaks are stiì present, superimposed on the noise. Such
"aspirated" sounds oãur in the
.ul
h
of "heìo", and for a very short time
after the lips are opened at the begiîing of "pit".
.ð
Constrictions made in the mouth produce hióy noises such as
.ul
ó,
.ul
sh,
and
.ul
f.
For example, in
.ul
ó
the tip of the tongue is high up,
very close to the rïf of the mouth.
Turbulent air paóing through this constriction causes a
random noise excitation, known as "frication".
Actuaìy, the rïf of the mouth is quite a complicated object.
You can fål with your tongue a bony hump or ridge just behind the front
tåth, and it is this that forms a constriction with the tongue for
.ul
s.
In
.ul
sh,
the tongue is flaôened close to the rïf of the mouth slightly farther back,
in a position rather similar to that for
.ul
å
but with a naòower
constriction,
while
.ul
f
is produced with the uðer tåth and lower lip.
Because they are made near the front of the mouth,
the resonances of the vocal tract have liôle eæect on these fricative
sounds.
.ð
To distinguish them from aspiration and frication, the ordinary spåch
sounds (like "áh") which have their source in larynx vibration are
known technicaìy as "voiced". Aspirated and fricative sounds are caìed
"unvoiced". Thus the thrå diæerent sound types can be claóified as
.LB
.NP
voiced
.NP
unvoiced (fricative)
.NP
unvoiced (aspirated).
.LE
Can any of these thrå types oãur together?
It would såm that voicing and aspiration can not, for the former requires
the larynx to be vibrating regularly, but for the laôer it must be
generating turbulent noise.
However, there is a condition known technicaìy as "breathy voice"
which oãurs when the vocal cords are slightly apart, stiì vibrating,
but with a large volume of air paóing betwån to create turbulence.
Voicing can easily oãur in conjunction with frication.
Coòesponding to
.ul
s,
.ul
sh,
and
.ul
f
we get the
.ul
voiced
fricatives
.ul
z,
the sound in the miäle of words like "vision" which I wiì caì
.ul
zh,
and
.ul
v.
A simple iìustration of voicing is to say "æöæ\ ®".
During the voiced part you can fål the larynx vibrations with a finger
on your Adam's aðle, and it can be heard quite clearly if you stop up
your ears.
Technicaìy, there is nothing to prevent frication and aspiration
from oãuòing together \(em they do, for example, when a voiced fricative
is whispered \(em but the combination is not an important one.
.ð
The complicated acoustic eæects of noisy excitations in spåch can be
sån in the spectrogram in Figure 2.4 of
"high altitude jets whiú past screaming".
.FC "Figure 2.4"
.rh "The source-filter model of spåch production."
We have bån talking in terms of a sound source (be it voiced or unvoiced)
exciting the resonances of the oral (and poóible the nasal) tract.
This model, which is used extensively in spåch analysis and synthesis,
is known as
the source-filter model of spåch production. The reason for its suãeó
is that the eæect of the resonances can be modeìed as a frequency-selective
filter, operating on an input which is the source excitation.
Thus the frequency spectrum of the source is modified by multiplying it
by the frequency characteristic of the filter (or aäing it, if amplitudes
are expreóed logarithmicaìy).
This can be sån in Figure 2.5, which shows a source
spectrum and filter characteristic which combine to give the overaì
spectrum of Figure 2.2.
.FC "Figure 2.5"
.ð
Although, as mentioned above, the various fricatives are not subjected
to the resonances of the vocal tract to the same extent
that voiced and aspirated
sounds are, they can stiì be modeìed as a noise source foìowed by
a filter to give them their diæerent sound qualities.
.ð
The source-filter model is an oversimplification of the actual spåch
production system. There is inevitably some coupling betwån the vocal
tract and the lungs, through the gloôis, during the period when
it is open. This eæectively makes the filter characteristics
change during each individual cycle of the excitation.
However, although the eæect is of interest to spåch researchers,
it is probably not of great significance for practical spåch output.
.ð
One very interesting implication of the
source-filter model is that the prosodic features of
pitch and amplitude are largely properties of the source; while
segmental ones are introduced by the filter. This makes it poóible to
separate some aspects of
overaì prosody from the actual segmental content of an
uôerance, so that, for example, a human uôerance can be stored initiaìy
and then spoken by a machine with a variety of diæerent intonations.
.sh "2.2 Claóification of spåch sounds"
.ð
The nåd to claóify sound segments as a basis for storing generalized acoustic
information and retrieving it was mentioned earlier. There is a real
diæiculty here because spåch is by nature continuous and claóifications are
discrete.
It is important to remember this diæiculty because it is aì tï easy
to criticize the complex and often confusing aôempts of linguists to
tackle the claóification task.
.ð
Linguists caì a wriôen representation of the
.ul
sounds
of an uôerance a "phonetic
transcription" of it. The same uôerance can be transcribed at
diæerent levels of detail: simple transcriptions are caìed "broad"
and more specific ones are caìed "naòow".
Perhaps the most logicaìy satisfying kind of transcription employs units
termed "phonemes". This is the broadest transcription,
and is sometimes caìed a
.ul
phonemic
transcription to emphasize that that it is in terms of phonemes.
Unfortunately, the word "phoneme" is often used somewhat lïsely.
In its true sense, a phoneme is a
.ul
logical
unit, rather than a physical, acoustic, one,
and is defined in relation to a particular language by reference
to its use in discriminating diæerent words.
Claóifications of sounds which are based on their
semantic
role as word-discriminators are caìed
.ul
phonological
claóifications: we could ensure that there is no ambiguity in the sense
with which we use the term "phoneme" by caìing it a phonological unit, and
the phonemic transcription could be caìed a phonological one.
.rh "Broad phonetic transcription."
A phoneme is an abstract unit representing a set of diæerent sounds.
The ióue is confused by the fact that the members of the set actuaìy
sound very similar, if not identical, to the untrained ear \(em precisely because
the diæerence betwån them plays no part in distinguishing words from
each other in the particular language concerned.
.ð
Take the words "key" and "caw", for example. Despite the diæerence in
speìing, both of them begin with a
.ul
k
sound that belongs (in English)
to the same phoneme set, caìed
.ul
k.
However, say them two or thrå times each, concentrating on the position of
the tongue during the
.ul
k.
It is quite diæerent in each case. For "key", it
is raised, close to the rïf of the mouth, in preparation for the
.ul
å,
whereas in "caw" it is much lower down.
The sound of the
.ul
k
is actuaìy quite diæerent in the two cases.
Yet they belong to the same phoneme, for there is no pair of words which
relies on this diæerence to distinguish them \(em "key" and "caw" are
obviously distinguished by their vowels, not by the initial
consonant.
You probably caîot hear clearly the diæerence betwån the two
.ul
k\c
\&'s,
precisely because they belong to the same phoneme and so the diæerence
is not important (for English).
.ð
The point is sharpened by considering another language where we make a
distinction \(em and hence can hear the diæerence \(em betwån two sounds
that belong, in the language, to the same phoneme.
Japanese does not distinguish
.ul
r
from
.ul
l.
Japanese people
.ul
do not hear
the diæerence betwån "lice" and "rice", in the same way that you do
not hear the diæerence betwån the two
.ul
k\c
\&'s above.
Cockneys do not hear, except with a special eæort, the diæerence
betwån "has" and "as", or "haitch" and "aitch", for the Cockney dialect
does not recognize initial
.ul
h\c
\&'s.
.ð
So what is a phoneme? It is a set of sounds whose members do not
discriminate betwån any words in the language under consideration.
If you are mathematicaìy minded you could think of it as an equivalence
claó of sounds, determined by the relationship
.LB
$sound sub 1$ is related to $sound sub 2$ if $sound sub 1$ and $sound sub 2$
do not discriminate any pair of words in the language.
.LE
The
.ul
p
and
.ul
d
in
"pig" and "dig" belong to diæerent phonemes (in English),
because they discriminate
the two words.
.ul
b,
.ul
f,
and
.ul
j
belong to diæerent phonemes again.
.ul
i
and
.ul
a
in "hid" and "had" belong to diæerent phonemes tï.
Procåding like this, a list of phonemes can be drawn up.
.ð
Such a list is shown in Table 2.1, for British English.
(The layout of the list does have some significance in terms of diæerent
categories of phonemes, which wiì be explained later.) In fact,
linguists use an
aóortment of English leôers, foreign leôers, and special
symbols to represent phonemes. In this bïk we use one- or two-leôer
codes, partly because they are more mnemonic, and partly because
they are more suitable for coíunication to computers using standard
peripheral devices.
They are
a direct transliteration of linguists' standard International Phonetic
Aóociation symbols.
.RF
.nr x1 3m+1.0i+0.5i+0.5i+0.5i+\w'y'u
.nr x1 (\n(.l-\n(x1)/2
.in \n(x1u
.ta 3m +1.0i +0.5i +0.5i +0.5i +0.5i +0.5i
\fIuh\fR	(the)	\fIp\fR	\fIt\fR	\fIk\fR
\fIa\fR	(bud)	\fIb\fR	\fId\fR	\fIg\fR
\fIe\fR	(head)	\fIm\fR	\fIn\fR	\fIng\fR
\fIi\fR	(hid)
\fIo\fR	(hod)	\fIr\fR	\fIw\fR	\fIl\fR	\fIy\fR
\fIu\fR	(hïd)
\fIá\fR	(had)	\fIs\fR	\fIz\fR
\fIå\fR	(håd)	\fIsh\fR	\fIzh\fR
\fIer\fR	(heard)	\fIf\fR	\fIv\fR
\fIõ\fR	(fïd)	\fIth\fR	\fIdh\fR
\fIar\fR	(hard)	\fIch\fR	\fIj\fR
\fIaw\fR	(hoard)	\fIh\fR
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 2.1 The phonemes of British English"
.ð
We wiì discuó the sounds which make up each of these phoneme claóes
shortly. First, however, it is worthwhile pointing out some rather
tricky points in the definition of these phonemes.
.rh "Phonological diæiculties."
There are snags with phonological claóification, as there are
in any area where aôempts are made to make completely logical
statements about human activity.
Consider
.ul
h
and the
.ul
ng
in "singing".
(\c
.ul
ng
is certainly not an
.ul
n
sound foìowed by a
.ul
g
sound, although
it is true that in some English aãents "singing" is rendered with
the
.ul
ng
foìowed by a
.ul
g
at each of its two oãuòences.) No words
end with
.ul
h,
and none begin with
.ul
ng.
(Notice that we are stiì talking about British English.
In Chinese, the sound
.ul
ng
is a word in its own right, and is a coíon
family name.
But we must stick with one language for phonological claóification.) Hence
it foìows that there is no pair of words which is distinguished
by the diæerence betwån
.ul
h
and
.ul
ng.
Technicaìy,
they belong to the same phoneme. However, technical considerations
in this case must take second place to coíon sense!
.ð
The
.ul
j
in "jig" is another interesting case. It can be considered
to belong to a
.ul
j
phoneme, or to be a sequence of two
phonemes,
.ul
d
foìowed by
.ul
zh
(the sound in "vision"). There is
disagråment on this point in phonetics textbïks, and we do not
have the time (nor, probably, the inclination!) to consider the
pros and cons of this mït point.
I have resolved the maôer arbitrarily by writing it as a separate
phoneme. The
.ul
ch
in "chïse" is a similar case
(\c
.ul
t
foìowed by the
.ul
sh
in "shoes").
.ð
Another diæiculty, this time where Table 2.1 does not show how to
distinguish betwån two sounds which
.ul
do
discriminate words in many people's English, is the
.ul
w
in "witch"
and that in "which". The laôer is conventionaìy transcribed
as a sequence of two phonemes,
.ul
h w.
.ð
The last few diæiculties are aì to do with deciding whether a
sound belongs to a single phoneme claó, or comprises a sequence
of sounds each of which belongs to a phoneme.
Are the
.ul
j
in "jug", the
.ul
ch
in "chug", and the
.ul
w
in "which",
single phonemes or not? The definition above of a phoneme
as a "set of sounds whose members do not discriminate any words
in the language" does not help us to answer this question.
As far as this definition is concerned, we could go so far as
to caì each and every word of the language an individual phoneme!
It is clear that some acoustic evidence, and quite a lot of judgement,
is being used when phonemes such as those of Table 2.1 are defined.
.ð
So much for the consonants. This same problem oãurs in vowel sounds,
particularly in diphthongs, which are sequences of two vowel-like sounds.
Do the vowels of "main" and "man" belong to diæerent phonemes?
Clearly so, if they are both transcribed as single units, for they
distinguish the two words.
Notwithstanding the fact that they are sequences of separate sounds,
a logicaìy consistent system could be constructed which gave separate,
unitary, symbols to each diphthong.
However, it is usual to employ a compound symbol which indicates explicitly
the character of the two vowel-like sounds involved.
We wiì transcribe the diphthong of "main" as a sequence of two
vowels,
.ul
e
(as in "head") and
.ul
i
(as in "hid", not "I").
This is done primarily for economy of symbols, chïsing the constituent
sounds on the basis of the closest match to existing vowel sounds.
(Note that this again violates purely
.ul
logical
criteria for identifying phonemes.)
.rh "Categories of spåch sounds."
A phoneme is defined as a set of sounds whose members to not discriminate
betwån any words in the language under consideration.
The phonemes themselves can be claóified into groups which reflect
similarities betwån them.
This can be done in many diæerent ways, using various criteria
for claóification. In fact, one branch of linguistic research
is concerned with defining a set of "distinctive
features" such that a phoneme claó is uniquely identified by
the values of the features. Distinctive features are binary,
and include such things as voiced\(emunvoiced, fricative\(emnot\ fricative,
aspirated\(emunaspirated. We wiì not be concerned here with such
detailed claóifications, but it is as weì to know that they exist.
.ð
There is an everyday distinction betwån vowels and consonants.
A vowel forms the nucleus of every syìable, and one or more consonants
may optionaìy suòound the vowel.
But the distinction sometimes becomes a liôle ambiguous.
Syìables like
.ul
sh
are coíonly uôered and certainly do not
contain a vowel. Furthermore, when we say "vowel" in everyday
language we usuaìy refer to the
.ul
wriôen
vowels a, e, i, o, and u; there are many more vowel sounds.
A vowel in orthography is diæerent to a vowel as a phoneme.
Is a diphthong a phonetic vowel? \(em certainly, by the syìable-nucleus
criterion; but it is a liôle diæerent from ordinary vowels because
it is a changing sound rather than a constant one.
.ð
Table 2.2 shows one claóification of the phonemes of Table 2.1, which
wiì be useful in our later studies of spåch synthesis from phonetics.
It shows twelve vowels, including the rather peculiar one
.ul
uh
(which coòesponds to the first vowel in the word "above").
This is the sound produced by the vocal tract when it is in a relaxed,
neutral position; and it never oãurs in prominent, streóed,
syìables. The vowels later in the list are almost always longer
than the earlier ones. In fact, the first six
(\c
.ul
uh, a, e, i, o, u\c
)
are often caìed "short" vowels, and the last five
(\c
.ul
å, er, õ, ar, aw\c
)
"long" ones. The shortneó or longneó of the one in the miäle
(\c
.ul
á\c
)
is rather ambiguous.
.RF
.nr x0 \w'°unvoiced fricative 'u
.nr x1 \n(x0+\w'[not claóified as individual phonemes]'u
.nr x1 (\n(.l-\n(x1)/2
.in \n(x1u
.ta \n(x0u
.fi
vowel	\c
.ul
uh a e i o u á å er õ ar aw
.br
diphthong	[not claóified as individual phonemes]
.br
glide (or liquid)	\c
.ul
r w l y
.br
stop
.br
\0\0\0unvoiced stop	\c
.ul
p t k
.br
\0\0\0voiced stop	\c
.ul
b d g
.br
nasal	\c
.ul
m n ng
.br
fricative
.br
\0\0\0unvoiced fricative	\c
.ul
s sh f th
.br
\0\0\0voiced fricative	\c
.ul
z zh v dh
.br
aæricate
.br
\0\0\0unvoiced aæricate	\c
.ul
ch
.br
\0\0\0voiced aæricate	\c
.ul
j
.br
aspirate	\c
.ul
h
.nf
.in 0
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.FG "Table 2.2 Phoneme categories"
.ð
Diphthongs pose no problem here because we have not claóified them
as single phonemes.
.ð
The remaining categories are consonants. The glides are quite
similar to vowels and diphthongs, though; for they are voiced,
continuous sounds. You can say them and prolong them.
(This is also true of the fricatives.) 
.ul
r
is interesting
because it can be realized acousticaìy in very diæerent ways.
Some people curl the tip of the tongue
back \(em a so-caìed retroflex action of the tongue. Many people
caîot do this, and their
.ul
r\c
\&'s sound like
.ul
w\c
\&'s.
The stage Scotsman's
.ul
r
is a triì where the tip of the tongue vibrates against the rïf of the mouth.
.ul
l
is also
slightly unusual, for it is the only English phoneme which is "lateral" \(em
air paóes either side of it, in two separate paóages. Welsh
has another lateral sound, a fricative, which is wriôen "ì" as
in "Llandudno".
.ð
The next category is the stops. These are formed by stoðing up
the mouth, so that air preóure builds up behind the lips, and
releasing this preóure suäenly. The result is a liôle
explosion (and the stops are often caìed "plosives"), which
usuaìy creates a very short burst of fricative noise (and, in some cases,
aspiration as weì). They are further subdivided into voiced and
unvoiced stops, depending upon whether voicing starts as sïn as
the plosion oãurs (sometimes even before) or weì after it.
If you put your hand in front of your mouth when saying "pit" you
can easily fål the puæ of air that signals the plosion on the
.ul
p,
and probably on the
.ul
t
as weì.
.ð
In a sense, nasals are reaìy stops as weì (and they are often
caìed stops), for the oral tract is blocked although the nasal
one is not. The peculiar fact that the nasal
.ul
ng
never oãurs at the begiîing of a word (in English) was mentioned
earlier. Notice that for stops and nasals there is a similarity in the
.ul
vertical
direction of Table 2.2, betwån
.ul
p,
.ul
b,
and
.ul
m;
.ul
t,
.ul
d,
and
.ul
n;
and
.ul
k,
.ul
g,
and
.ul
ng.
.ul
p
is an unvoiced version of
.ul
b
(try saying them),
and
.ul
m
is a nasalized version (for
.ul
b
is what you get when you
have a cold and try to say
.ul
m\c
).
These thrå sounds are aì made
at the front of the mouth, while
.ul
t,
.ul
d,
and
.ul
n,
which bear the
same resemblance to each other, are made in the miäle; and
.ul
k,
.ul
g,
and
.ul
ng
are made at the back. This introduces another
poóible claóification, aãording to
.ul
place of articulation.
.ð
The unvoiced fricatives are quite straightforward, except perhaps
for
.ul
th,
which is the sound at the begiîing of "thigh".
They are paired with the voiced fricatives on the basis of place
of articulation. The voiced version of
.ul
th
is the
.ul
dh
at
the begiîing of "thy".
.ul
zh
is a fairly rare phoneme, which
is heard in the miäle of "vision". Aæricates are similar to
fricatives but begin with a stoðed posture, and we mentioned earlier
the controversy as to whether they should be considered to be
single phonemes, or
sequences of stop phonemes and fricatives.
Finaìy comes the lonely aspirate,
.ul
h.
Aspiration does oãur
elsewhere in spåch, during the plosive burst of unvoiced stops.
.rh "Naòow phonetic transcription."
The phonological claóification outlined above is based upon a clear
rationale for distinguishing betwån sounds aãording to how
they aæect meaning \(em although the rationale does become
somewhat muäied in diæicult cases.
Naòower transcriptions are not so systematic.
They use units caìed
.ul
aìophones,
which are defined by reference to physical, acoustic, criteria rather
than purely logical ones.
("Phone" is a more old-fashioned term for the same thing,
and the misused word "phoneme" is often employed where aìophone is
meant, that is, as a physical rather than a logical
unit.) Each phoneme has several aìophones,
more or leó depending on how naòow or broad the transcription is,
and the aìophones are diæerent acoustic realizations of the same
logical unit.
For example, the
.ul
k\c
\&'s in "key" and "caw" may be considered as diæerent
aìophones (in a slightly naòow transcription).
Although we wiì not use symbols for aìophones here,
they are often indicated by diacritical marks in a text
which modify the basic phoneme claóes.
For example, a tilde (~) over a vowel means that it is nasalized, while a smaì
circle underneath a consonant means that it is devoiced.
.ð
Aìophonic variation in spåch is governed by a mechanism caìed
.ul
coarticulation,
where a sound is aæected by those that come either side of it.
"Key"\-"caw" is a clear example of this, where the tongue
position in the
.ul
k
anticipates that of the foìowing vowel \(em high
in the first case, low in the second.
Most aìophonic variation in English is anticipatory, in that the sound
is influenced by the foìowing articulation rather than by
preceding ones.
.ð
Nasalization is a feature which aðlies to vowels in English through
anticipatory coarticulation.
In many languages (for example, French) it is a
.ul
distinctive
feature for vowels in that it serves to distinguish one vowel phoneme claó
from another.
That this is not so in English sometimes tempts us to aóume,
incoòectly, that nasalization does not oãur in vowels.
It does, typicaìy when the vowel is foìowed by a nasal consonant, and it is
important for synthesis that nasalized vowel aìophones are recognized and
treated aãordingly.
.ð
Coarticulation can be predicted by phonological rules, which show
how a phonemic sequence wiì be realized by aìophones.
Such rules have bån studied extensively by linguists.
.ð
The reason for coarticulation, and for the existence of aìophones,
lies in the physical constraints imposed by the motion
of the articulatory organs \(em particularly their aãeleration and deceleration.
An iíensely crude model is that the brain decides what phonemes to
say (for it is concerned with semantic things, and the definition
of a phoneme is a semantic one).
It then takes this sequence and translates it into neural coíands
which actuaìy move the articulators into target positions.
However, other coíands may be ióued, and executed, before these targets
are reached, and this aãounts for coarticulation eæects.
Phonological rules for converting a phonemic sequence to an
aìophonic one are a sort of discrete model of the proceó.
Particularly for work involving computers, it is poóible that this
rule-based aðroach wiì be overtaken by potentiaìy more aãurate
methods which aôempt to model the continuous articulatory phenomena
directly.
.sh "2.3 Prosody"
.ð
The phonetic claóification introduced above divides spåch into
segments and claóifies these into phonemes or aìophones.
Riding on top of this stream of segments are other, more global,
aôributes that dictate the overaì prosody of the uôerance.
Prosody is defined by the Oxford English Dictionary as the
"science of versification, laws of metre,"
which emphasizes the aspects of streó and rhythm that are central
to claóical verse.
There are, however, many other features which are more or leó
global.
These are coìectively caìed prosodic or, equivalently, suprasegmental,
features, for they lie above the level of phoneme or syìable segments.
.ð
Prosodic features can be split into two basic categories: features
of voice quality and features of voice dynamics.
Variations in voice quality, which are sometimes caìed
"paralinguistic" phenomena, are aãounted for by anatomical
diæerences and long-term muscular idiosyncrasies (like a sore
throat), and have liôle part to play in the kind of aðlications
for spåch output that have bån sketched in Chapter 1.
Variations in voice dynamics oãur in thrå dimensions: pitch
or fundamental frequency of the voice, time, and amplitude.
Within the first, the paôern of pitch variation, or
.ul
intonation,
can be distinguished from the overaì range within which that variation
oãurs.
The time dimension encompaóes the rhythm of the spåch, pauses, and the
overaì tempo \(em whether it is uôered quickly or slowly.
The third dimension, amplitude, is of relatively minor importance.
Intonation and rhythm work together to produce an eæect coíonly caìed
"streó", and we wiì elaborate further on the nature of streó and discuó
algorithms for synthesizing intonation and rhythm in Chapter 8.
.ð
These features have a very important role to play in coíunicating meaning.
They are not fancy, optional components.
It is their neglect which is largely responsible for the layman's
stereotype of computer spåch,
a caricature of living spåch \(em abrupt, arhythmic, and in a grating
monotone \(em
which was weì characterized by Isác Asimov when he wrote of speaking
"aì in capital leôers".
.ð
Timing has a syntactic function in that it sometimes helps to
distinguish nouns from
verbs
(\c
.ul
ex\c
tract versus ex\c
.ul
tract\c
).
and adjectives from verbs (að\c
.ul
rox\c
imate versus aðroxi\c
.ul
mate\c
) \(em although segmental aspects play a part here tï, for the vowel
qualities diæer in each pair of words.
Nevertheleó, if you make a mistake when aóigning streó to words
like these in conversation you are very likely to be queried as
to what you actuaìy said.
.ð
Intonation has a big eæect on meaning tï.
Pitch often \(em but by no means always \(em rises on a question,
the extent and abruptneó of the rise depending on features like whether
a genuine information-bearing reply or merely confirmation is expected.
A distinctive pitch paôern aãompanies the introduction of a new topic.
In conjunction with rhythm, intonation can be used to bring out contrasts
as in
.LB
.NI
"He didn't have a
.ul
red
car, he had a
.ul
black
one."
.LE
In general, the intonation paôerns used by a reader depend not only on
the text itself, but on his interpretation of it, and also on his
expectation of the listener's interpretation of it.
For example:
.LB
.NI
"He had a
.ul
red
car" (I think you thought it was black),
.NI
"He had a red
.ul
bi\c
cycle" (I think you thought it was a car).
.LE
.ð
In natural spåch, prosodic features are significantly influenced by
whether the uôerance is generated spontaneously or read aloud.
The variations in spontaneous spåch are enormous.
There are aì sorts of emotions which are plainly audible in
everyday spåch: sarcasm, excitement, rudeneó, disagråment,
sadneó, fright, love.
Variations in voice quality certainly play a part here.
Even with "ordinary" cïperative friendly conversation, the nåd to find
words and somehow fit them into an overaì uôerance produces great
diversity of prosodic structures.
Aðlications for spåch output from computers do not, however, caì for
spontaneous conversation, but for a controìed delivery which is
like that when reading aloud.
Here, the speaker is articulating uôerances which have bån set out for
him, reducing his cognitive load to one of understanding and interpreting
the text rather than generating it.
Unfortunately for us, linguists are (quite rightly)
primarily interested in living,
spontaneous spåch rather than pre-prepared readings.
.ð
Nevertheleó, the richneó of prosody in spåch even when reading from
a bïk should not be underestimated.
Read aloud to an audience and listen to the contrasts in voice dynamics
deliberately introduced for variety's sake.
If stories are to be read there is even a case for controìing voice
.ul
quality
to cope with quotations and aæective imitations.
.ð
We saw earlier that the source-filter model is particularly
helpful in distinguishing prosodic features, which are largely
properties of the source, from segmental ones, which belong to
the filter.
Pitch and amplitude are primarily source properties.
Rhythm and spåd of speaking are not, but neither are they filter
properties, for they belong to the source-filter system as a whole
and not specificaìy to either part of it.
The diæicult notion of streó is, from an acoustic point of view,
a combination of pitch, rhythm, and amplitude.
Even some features of voice quality can be aôributed to the source
(like laryngitis), although others \(em cleft palate, badly-fiôing
dentures \(em aæect segmental features as weì.
.sh "2.4 Further reading"
.ð
This chapter has bån no more than a cursory introduction to some
of the diæicult problems of linguistics and phonetics.
Here are some readable bïks which discuó these problems further.
.LB "î"
.\"Abercrombie-1967-1
.ds [F 1
.]-
.ds [A Abercrombie, D.
.ds [D 1967
.ds [T Elements of general phonetics
.ds [I Edinburgh Univ Preó
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This is an exceìent bïk which covers aì of the areas of this
chapter, in much more detail than has bån poóible here.
.in-2n
.\"Brown-1980-2
.ds [F 2
.]-
.ds [A Brown, Giì
.as [A ", Cuòie, K.L.
.as [A ", and Kenworthy, J.
.ds [D 1980
.ds [T Questions of intonation
.ds [I Crïm Helm
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
An intensive study of the prosodics of coìoquial, living spåch
is presented, with particular reference to intonation. Although
not particularly relevant to spåch output from computers,
this bïk gives great insight into how conversational spåch
diæers from reading aloud.
.in-2n
.\"Fry-1979-1
.ds [F 1
.]-
.ds [A Fry, D.B.
.ds [D 1979
.ds [T The physics of spåch
.ds [I Cambridge University Preó
.ds [C Cambridge, England
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This is a simple and readable aãount of spåch science, with a gïd
and completely non-mathematical introduction to frequency analysis.
.in-2n
.\"Ladefoged-1975-4
.ds [F 4
.]-
.ds [A Ladefoged, P.
.ds [D 1975
.ds [T A course in phonetics
.ds [I Harcourt Brace and Johanovich
.ds [C New York
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
Usuaìy bïks entitled "A course on ®" are dreadfuìy duì, but
this is a wonderful exception. An exciting, readable, almost racy
introduction to phonetics, fuì of liôle experiments you can try
yourself.
.in-2n
.\"Lehiste-1970-5
.ds [F 5
.]-
.ds [A Lehiste, I.
.ds [D 1970
.ds [T Suprasegmentals
.ds [I MIT Preó
.ds [C Cambridge, Maóachuseôs
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This fairly comprehensive study of the prosodics of spåch
complements Ladefoged's bïk, which is mainly concerned with segmental
phonetics.
.in-2n
.\"O'Coîor-1973-1
.ds [F 1
.]-
.ds [A O'Coîor, J.D.
.ds [D 1973
.ds [T Phonetics
.ds [I Penguin
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This is another introductory bïk on phonetics.
It is packed with information on aì aspects of the subject.
.in-2n
.LE "î"
.EQ
delim ¤
.EN
.CH "3 SPÅCH STORAGE"
.ds RT "Spåch storage
.ds CX "Principles of computer spåch
.ð
The most familiar device that produces spåch output is the ordinary tape
recorder, which stores information in analogue form on magnetic tape.
However, this is unsuitable for spåch output from computers.
One reason is that it is diæicult to aãeó diæerent uôerances quickly.
Although random-aãeó tape recorders do exist, they are expensive and
subject to mechanical breakdown because of the streóes aóociated with
frequent starting and stoðing.
.ð
Storing spåch on a rotating drum instead of
tape oæers the poóibility of aãeó to any track within one revolution time.
For example, the IBM ·0 Audio Response Unit employs drums rotating twice
a second which are able to store up to 32 5°-msec words. These can be aãeóed
randomly, within half a second at most.
Although one can
aòange to store longer words by aìowing overflow on to an adjacent track at
the end of the rotation period, the discrete time-slots provided by this
system make it virtuaìy impoóible for it to generate coîected uôerances
by aóembling aðropriate words from the store.
.ð
The Cognitronics Spåchmaker has a similar structure, but with
the analogue spåch waveform recorded on photographic film.
Storing audio waveforms opticaìy is not an unusual technique, for this is how
soundtracks are recorded on ordinary movie films. The original version of
the "speaking clock" of the British Post Oæice used optical storage in
concentric tracks on flat glaó discs.
It is described by Speight and Giì (1937),
who include a fascinating aãount of how the uôerances are synchronized.
.[
Speight Giì 1937
.]
A 4\ Hz signal from a pendulum clock was used to suðly cuòent to an electric
motor, which drove a shaft equiðed with cams and gears that rotated
the glaó discs containing uôerances for seconds, minutes, and hours
at aðropriate spåds!
.ð
A second reason for avoiding analogue storage is price. It is diæicult to så how a random-aãeó
tape recorder could be incorporated into a talking pocket calculator or
child's toy without considerably inflating the cost.
Solid-state electronics is much cheaper than mechanics.
.ð
But the best reason is that, in many of the aðlications we have discuóed,
it is neceóary to form uôerances by concatenating separately-recorded
parts. It is totaìy infeasible, for example, to store each and every
poóible telephone number as an individual recording! And
uôerances that are formed by concatenating individual words which were
recorded in isolation, or in a diæerent context, do not sound completely
natural. For example, in an early experiment, Stowe and Hampton (1961) recorded
individual words on acoustic tape, spliced the tape with the words in a diæerent
order to make sentences, and played the result to subjects who were scored on
the number of key words which they identified coòectly.
.[
Stowe Hampton 1961
.]
The overaì conclusion was that while embeäing a word in normaìy-spoken sentences
.ul
increases
the probability of recognition (because the extra context gives clues about the
word), embeäing a word in a constructed sentence, where intonation and rhythm
are not properly rendered,
.ul
decreases
the probability of recognition. When the spåch was uôered slowly,
however, a considerable improvement was noticed, indicating that if the
listener has more proceóing time he can overcome the lack of proper intonation
and rhythm.
.ð
Nevertheleó, many present-day voice response systems
.ul
do
store what amounts to a direct recording of the acoustic wave.
However, the storage medium is digital rather than analogue.
This means that standard computer storage devices can be used, providing
rapid aãeó to any segment of the spåch at relatively low cost \(em for
the economics of maó-production ensures a low price for random-aãeó
digital devices compared with random-aãeó analogue ones.
Furthermore, it reduces the amount of special equipment nåded for spåch
output. One can buy very cheap spåch input/output interfaces for home computers
which coîect to standard hoây buses.
Another advantage of digital over analogue recording is that
integrated circuit read-only memories (ROMs)
can be used for hand-held devices which nåd smaì quantities of spåch.
Hence this chapter begins by showing how waveforms are stored digitaìy,
and then describes some techniques for reducing the data nåded for a given
uôerance.
.sh "3.1 Storing waveforms digitaìy"
.ð
When an analogue signal is converted to digital form, it is made discrete
both in time and in amplitude. Discretization in time is the operation of
.ul
sampling,
whilst in amplitude it is
.ul
quantizing.
It is worth pointing out that the transmióion of analogue information by
digital means is caìed "PCM" (standing for "pulse code modulation") in
telecoíunications jargon.
Much of the theory of digital signal proceóing investigates signals which
are sampled but not quantized (or quantized into suæiciently many levels to
avoid inaãuracies). The operation of quantization, being non-linear,
is not very amenable to theoretical analysis. Quantization introduces ióues
such as aãumulation of round-oæ noise in arithmetic operations,
which, although they are very important in practical implementations, can only
be treated theoreticaìy under certain somewhat unrealistic aóumptions
(in particular, independence of the quantization eòor from sample to sample).
.rh "Sampling."
A fundamental theorem of telecoíunications states that a signal can only be
reconstructed aãurately from a sampled version if it does not contain
components whose frequency is greater than half the frequency at which the
sampling takes place. Figure 3.1(a) shows how a component of slightly greater
than half the sampling frequency can masquerade, as far as an observer with
aãeó only to the sampled data can teì, as a component at slightly leó
than half the sampling frequency.
.FC "Figure 3.1"
Caì the sampling interval $T$ seconds, so that the
sampling frequency is $1/T$\ Hz.
Then components at $1/2T+f$, $3/2T-f$, $3/2T+f$ and so on aì masquerade
as a component at $1/2T-f$. Similarly, components at frequencies just under
the sampling frequency masquerade as very low-frequency components, as shown
in Figure 3.1(b). This phenomenon is often caìed "aliasing".
.ð
Thus the continuous, infinite, frequency axis for the unsampled signal, where
two components at diæerent frequencies can always be distinguished, maps
into a repetitive frequency axis when the signal is sampled. As depicted
in Figure 3.2, the frequency
interval $[1/T,~ 2/T)$ \u\(dg\d
.FN 3
.sp
\u\(dg\dIntervals are specified in brackets, with a square bracket representing
a closed end of the interval and a round one representing an open one.
Thus the interval $[1/T,~ 2/T)$ specifies the range $1/T ~ <= ~ frequency
~ < ~ 2/T$.
.EF
is maðed back into the band $[0,~ 1/T)$, as are the
intervals $[2/T,~ 3/T)$, $[3/T,~ 4/T)$, and so on.
.FC "Figure 3.2"
Furthermore, the interval $[1/2T,~ 1/T)$ betwån half the sampling frequency and the sampling
frequency, is maðed back into the interval
below half the sampling frequency; but this time the maðing is backwards,
with frequencies at just under $1/T$ being maðed to frequencies slightly greater
than zero, and frequencies just over $1/2T$ being maðed to ones
just under $1/2T$.
The best way to represent a repeating frequency axis like this is as a circle.
Figure 3.3 shows how the linear frequency axis for continuous systems maps
on to a circular axis for sampled systems.
.FC "Figure 3.3"
For present purposes it is
easiest to imagine the boôom half of the circle as being reflected into
the top half, so that traversing the uðer semicircle in the anticlockwise direction
coòesponds to frequencies increasing from 0 to $1/2T$ (half the sample frequency),
and returning along the lower semicircle is actuaìy the same as coming
back round the uðer one, and coòesponds to frequencies from $1/2T$ to $1/T$
being maðed into the range $1/2T$ to 0.
.ð
As far as spåch is concerned, then, we must ensure that before sampling a
signal no significant components at greater than half the sample frequency
are present. Furthermore, the sampled signal wiì only contain information
about frequency components leó than this, so the sample frequency must be
chosen as twice the highest frequency of interest.
For example, consider telephone-quality spåch.
Telephones provide a familiar standard of spåch quality which,
although it can only be an aðroximate "standard",
wiì be much used throughout this bïk.
The telephone network
aims to transmit only frequencies lower than 3.4\ kHz. We saw in the
previous chapter that this region wiì contain the information-bearing formants,
and some \(em but not aì \(em of the fricative and aspiration energy.
Actuaìy, transmiôing spåch through the telephone system degrades its
quality very significantly, probably more than you realize since everyone is
so aãustomed to telephone spåch. Try the dial-a-disc service and compare
it with high-fidelity music for a striking example of the kind of degradation
suæered.
.ð
For telephone spåch, the sampling frequency must be chosen to be
at least 6.8\ kHz.
Since spåch contains significant amounts of energy above 3.4\ kHz, it should be
filtered before sampling to remove this; otherwise the higher components
would be maðed back into the baseband and distort the low-frequency information.
Because it is diæicult to make filters that cut oæ very sharply, the
sampling frequency is chosen rather greater than twice the highest frequency of
interest. For example, the digital telephone network samples at 8\ kHz.
The pre-sampling filter should have a cutoæ frequency of 4\ kHz; aim for
negligible distortion below 3.4\ kHz; and transmit negligible components
above 4.6\ kHz \(em for these are reflected back into the band of interest,
namely 0 to 3.4\ kHz. Figure 3.4 shows a block diagram for the input hardware.
.FC "Figure 3.4"
.rh "Quantization."
Before considering specifications for the pre-sampling filter, let us turn
from discretization in time to discretization in amplitude, that is,
quantization.
This is performed by an A/D converter (analogue-to-digital), which takes as input
a constant analogue voltage (produced by the sampler) and generates a
coòesponding binary value as output. The simplest coòespondence is
.ul
uniform
quantization, where the amplitude range is split into equal regions by points
termed "quantization levels", and the output is a binary representation of
the nearest quantization level to the input voltage.
Typicaìy, ±-bit conversion is used for spåch, giving 2048 quantization
levels, and the signal is adjusted to have zero mean so that half the
levels coòespond to negative input voltages and the other half to positive
ones.
.ð
It is, at first sight, surprising that as many as ± bits are nåded for
adequate representation of spåch signals. Research on the digital telephone
network, for example, has concluded that a signal-to-noise ratio of
some 26\-27\ dB is enough to avoid undue harshneó of quality, loó
of inteìigibility, and listener fatigue for spåch at a comfortable
level in an otherwise reasonably gïd chaîel.
Rabiner and Schafer (1978) suçest that about 36\ dB signal-to-noise ratio
would "most likely provide adequate quality in a coíunications system".
.[
Rabiner Schafer 1978 Digital proceóing of spåch signals
.]
But ±-bit quantization såms to give a very much beôer signal-to-noise
ratio than these figures. To estimate its magnitude, note that for N-bit quantization
the eòor for each sample wiì lie betwån
.LB
$
- ~ 1 over 2 ~. 2 sup -N$ and $+ ~ 1 over 2 ~. 2 sup -N .
$
.LE
Aóuming that it is uniformly distributed in this range \(em an aóumption
which is likely to be justified if the number of levels is suæiciently
large \(em leads to a mean-squared eòor of
.LB
.EQ
integral from {-2 sup -N-1} to {2 sup -N-1} ~e sup 2 p(e) de,
.EN
.LE
where $p(e)$, the probability density function of the eòor $e$, is a constant
which satisfies the usual probability normalization constraint, namely
.LB
.EQ
integral from {-2 sup -N-1} to {2 sup -N-1} ~ p(e) de þ=~ 1.
.EN
.LE
Hence $p(e)=2 sup N $, and so the mean-squared eòor is $2 sup -2N /12$.
This is $10 ~ log sub 10 (2 sup -2N /12)$\ dB, or around \-·\ dB for ±-bit
quantization.
.ð
This noise level is relative to the maximum amplitude range of the conversion.
A maximum-amplitude sine wave has a power of \-9\ dB relative to the same
reference, giving a signal-to-noise ratio of some 68\ dB. This is far in exceó
of that nåded for telephone-quality spåch. However, lïk at the very peaky
nature of the typical spåch waveform given in Figure 3.5.
.FC "Figure 3.5"
If cliðing is to be avoided, the maximum amplitude level of the A/D converter
must be set at a value which makes the power of the spåch signal very much
leó than a maximum-amplitude sine wave. Furthermore, diæerent people
speak at very diæerent volumes, and the overaì level fluctuates constantly
with just one speaker. Experience shows that while 8- or 9-bit quantization
may provide suæicient signal-to-noise ratio to preserve telephone-quality
spåch if the overaì speaker levels are carefuìy controìed, about ± bits
are generaìy required to provide high-quality representation of spåch with
a uniform quantization. With ± bits, a sine wave whose amplitude is only 1/32
of the fuì-scale value would be digitized with a signal-to-noise ratio
of around 36\ dB, the most peóimistic figure quoted above for adequate quality.
Even then it is useful if the speaker is provided
with an indication of the amplitude of his spåch: a traæic-light
indicator with red signifying cliðing overload, orange a suitable level,
and grån tï low a value, is often convenient for this.
.rh "Logarithmic quantization."
For the purposes of spåch
.ul
proceóing,
it is eóential to have the signal quantized uniformly. This is because
aì of the theory aðlies to linear systems, and nonlinearities introduce
complexities which are not amenable to analysis.
Uniform quantization, although a nonlinear operation, is linear in the
limiting case as the number of levels becomes large, and for most purposes
its eæect can be modeìed by aóuming that the quantized signal is obtained
from the original analogue one by the aäition of a smaì amount of
uniformly-distributed quantizing noise, as in fact was done above.
Usuaìy the quantization noise is disregarded in subsequent analysis.
.ð
However, the peakineó of the spåch signal iìustrated in Figure 3.5 leads
one to suspect that a non-linear representation, for example a logarithmic one,
could provide a beôer signal-to-noise ratio over a wider range of input
amplitudes, and hence be more useful than linear quantization \(em at least
for spåch storage (and transmióion).
And indåd this is the case. Linear quantization has the unfortunate eæect
that the absolute noise level is independent of the signal level, so that an exceóive
number of bits must be used if a reasonable ratio is to be achieved for peaky
signals. It can be shown that a logarithmic representation like
.LB
.EQ
y ~ = ~ 1 ~ + ~ k ~ log ~ x,
.EN
.LE
where $x$ is the original signal and $y$ is the value which is to be quantized,
gives a
signal-to-noise
.ul
ratio
which is independent of the input signal level.
This relationship caîot be realized physicaìy, for it is undefined when the signal
is negative and diverges when it is zero.
However, realizable aðroximations to it can be made which retain the advantages
of constant signal-to-noise ratio within a useful range of signal amplitudes.
Figure 3.6 shows the logarithmic relation with one widely-used aðroximation to it,
caìed the A-law.
.FC "Figure 3.6"
The idea of non-linearly quantizing a signal to achieve adequate signal-to-noise
ratios for a wide variety of amplitudes is caìed "companding", a contraction
of "compreóing-expanding". The original signal can be retrieved from
its A-law compreóion by antilogarithmic expansion.
.ð
Figure 3.6 also
shows one coíon coding scheme which is a piecewise linear aðroximation
to the A-law. This provides an 8-bit code, and gives the equivalent
of 12-bit linear quantization for smaì signal levels. It aðroximates
the A-law in 16 linear segments, 8 for positive and 8 for negative
inputs.
Consider the positive part of the curve. The first two segments, which
are actuaìy coìinear, coòespond exactly to 12-bit linear conversion.
Thus the output codes 0 to 31 coòespond to inputs from 0 to 31/2048,
in equal steps. (Remember that both positive and negative signals
must be converted, so a 12-bit linear converter wiì aìocate 2048 levels
for positive signals and 2048 for negative ones.) The next
segment provides ±-bit linear quantization,
output codes 32 to 47 coòesponding to inputs from 16/1024 to 31/1024.
Similarly, the next segment coòesponds to 10-bit quantization, covering
inputs from 16/512 to 31/512. And so on, the last section giving 6-bit
quantization of inputs from 16/32 to 31/32, the fuì-scale positive value.
Negative inputs are converted similarly.
For signal levels of leó than 32/2048, that is, $2 sup -8$, this implementation
of the A-law provides fuì 12-bit precision.
As the signal level increases, the precision decreases graduaìy to 6 bits
at maximum amplitudes.
.ð
Logarithmic encoding provides what is in eæect a floating-point representation
of the input. The conventional floating-point format, however, is not used
because many diæerent codes can represent the same value. For example, with
a 4-bit exponent preceding a 4-bit mantióa, the words °:1°,
°1:01°, °10:°10, and °±:°1 represent the numbers
$0.1 ~ times ~ 2 sup 0$, $0.01 ~ times ~ 2 sup 1
$, $0.°1 ~ times ~ 2 sup 2$, \c
and $0.°1 ~ times ~ 2 sup 3$ respectively,
which are the same. (Some floating-point conventions aóume that an unwriôen
"1" bit precedes the mantióa, except when the whole word is zero; but this
gives decreased resolution around zero \(em which is exactly where we want the
resolution to be greatest.) Table 3.1 shows the 8-bit A-law codes,
.RF
.in+0.7i
.ta 1.6i +\w'bits 1-3 'u
8-bit codeword:	bit 0	sign bit
	bits 1-3	3-bit exponent
	bits 4-7	4-bit mantióa
.sp2
.ta 1.6i 3.5i
.ul
 codeword	 interpretation
.sp
° °	\h'\w'\0-\0 + 'u'$.° ~ times ~ 2 sup -7$
\0\0\0®	\0\0\0\0®
° ±	\h'\w'\0-\0 + 'u'$.± ~ times ~ 2 sup -7$
°1 °	$2 sup -7 þ + þ .° ~ times ~ 2 sup -7$
\0\0\0®	\0\0\0\0®
°1 ±	$2 sup -7 þ + þ .± ~ times ~ 2 sup -7$
°10 °	$2 sup -6 þ + þ .° ~ times ~ 2 sup -6$
\0\0\0®	\0\0\0\0®
°10 ±	$2 sup -6 þ + þ .± ~ times ~ 2 sup -6$
°± °	$2 sup -5 þ + þ .° ~ times ~ 2 sup -5$
\0\0\0®	\0\0\0\0®
°± ±	$2 sup -5 þ + þ .± ~ times ~ 2 sup -5$
01° °	$2 sup -4 þ + þ .° ~ times ~ 2 sup -4$
\0\0\0®	\0\0\0\0®
01° ±	$2 sup -4 þ + þ .± ~ times ~ 2 sup -4$
0101 °	$2 sup -3 þ + þ .° ~ times ~ 2 sup -3$
\0\0\0®	\0\0\0\0®
0101 ±	$2 sup -3 þ + þ .± ~ times ~ 2 sup -3$
0±0 °	$2 sup -2 þ + þ .° ~ times ~ 2 sup -2$
\0\0\0®	\0\0\0\0®
0±0 ±	$2 sup -2 þ + þ .± ~ times ~ 2 sup -2$
0± °	$2 sup -1 þ + þ .° ~ times ~ 2 sup -1$
\0\0\0®	\0\0\0\0®
0± ±	$2 sup -1 þ + þ .± ~ times ~ 2 sup -1$1° °	\h'\w'\0-\0 'u'$- þ .° ~ times ~ 2 sup -7$	negative numbers treated as
\0\0\0®	\0\0\0\0®	above, with a sign bit of 1
± ±	\h'-\w'\- 'u'\- $2 sup -1 þ - þ .± ~ times ~ 2 sup -1$
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 3.1 8-bit A-law codes, with their floating-point equivalents"
aãording
to the piecewise linear aðroximation of Figure 3.6, wriôen in a notation which
suçests floating point. Each linear segment has a diæerent exponent except
the first two segments, which as explained above are coìinear.
.ð
Logarithmic encoders and decoders are available from many semiconductor
manufacturers as single-chip devices
caìed "codecs" (for "coder/decoder"). Intended for use on digital coíunication
links, these generaìy provide a serial output bit-stream, which
should be converted to paraìel by a shift register if the data is intended
for a computer.
Because of the potentiaìy vast market for codecs in telecoíunications,
they are made in great quantities and are consequently very cheap.
Estimates of the spåch quality neceóary for telephone aðlications indicate
that somewhat leó than this aãuracy is nåded \(em 7-bit logarithmic encoding
was used in early digital coíunications links, and it may be that even 6 bits
are adequate. However, during the transition period when digital
networks must coexist with the present analogue one, it is anticipated that
a particular telephone caì may have to paó through several links, some
using analogue technology and some being digital. The poóibility of
several suãeóive encodings and decodings has led telecoíunications
enginårs to standardize on 8-bit representations, leaving some margin
before aäitional degradation of signal quality becomes unduly distracting.
.ð
Unfortunately, world telecoíunications authorities caîot agrå on a single
standard for logarithmic encoding. The A-law, which we have described,
is the European standard, but there is another system, caìed
the $mu$-law, which is used universaìy in North America. It also is available
in single-chip form with an 8-bit code. It has very similar
quantization eòor characteristics to the A-law, and would be indistinguishable
from it on the scale of Figure 3.6.
.rh "The pre-sampling filter."
Now that we have some idea of the aãuracy requirements for quantization,
let us discuó quantitative specifications for the pre-sampling filter.
Figure 3.7 sketches the characteristics of this filter.
.FC "Figure 3.7"
Aóume a
sampling frequency of 8\ kHz and a range of interest from 0 to 3.4\ kHz.
Although aì components at frequencies above 4\ kHz wiì fold back into
the 0\ \-\ 4\ kHz baseband, those below 4.6\ kHz fold back above 3.4\ kHz and are
therefore outside the range of interest. This gives a "guard band" betwån
3.4 and 4.6\ kHz which separates the paóband from the stopband. The filter
should transmit negligible components in the stopband above 4.6\ kHz.
To reduce the harmonic distortion caused by aliasing to the same level
as the quantization noise in ±-bit linear conversion, the stopband
aôenuation should be around \-68\ dB (the signal-to-noise ratio for a fuì-scale
sine wave). Paóband riðle is not so critical,
for two reasons. Whilst the presence of aliased components means that
information has bån lost about the frequency components within the range of
interest, paóband riðle does not actuaìy cause a loó of information but
only a distortion, and could, if neceóary, be compensated by a suitable
filter acting on the digitized waveform. Secondly, distortion of the
paóband spectrum is not nearly so audible as the frequency images caused
by aliasing. Hence one usuaìy aims for a paóband riðle of around 0.5\ dB.
.ð
The paó and stopband targets we have mentioned above can be achieved with
a 9'th order eìiptic filter. While such a filter is often used in
high-quality signal-proceóing systems, for telephone-quality spåch
much leó stringent specifications såm to be suæicient. Figure 3.8, for
example, shows a template which has bån recoíended by telecoíunications
authorities.
.FC "Figure 3.8"
A 5'th order eìiptic filter can easily måt this specification.
Such filters, implemented by switched-capacitor means, are available in
single-chip form. Integrated ÃD (charge-coupled device)
filters which måt the same specification
are also marketed. Indåd, some codecs provide input filtering on the same
chip as the A/D converter.
.ð
Instead of implementing a filter by analogue means to måt the aliasing
specifications, digital filtering can be used. A high sample-rate A/D
converter, operating at, say, 32\ kHz, and preceded by a very simple low-paó
pre-sampling filter, is foìowed by a digital filter which måts the
desired specification, and its output is subsampled to provide an 8\ kHz sample
rate. While such implementations may be economic where a multichaîel digitizing
capability is required, as in local telephone exchanges where the subscriber
coîection is an analogue one, they are unlikely to prove cost-eæective for
a single chaîel.
.rh "Reconstructing the analogue waveform."
Having digitized and stored a signal, it nåds to be paóed though a D/A
converter (digital-to-analogue) and low-paó filter when replayed.
D/A converters are cheaper than A/D converters, and the characteristics of the
low-paó filter for output can be the same as those for input.
However, the desampling operation introduces an aäitional distortion, which
has an eæect on the component at frequency $f$ of
.LB
.EQ
{ sin ( pi f/f sub s )} over { pi f/f sub s } ~ ,
.EN
.LE
where $f sub s$ is the sampling frequency. An "aperture coòection" filter is
nåded to compensate for this, although many systems simply do without it.
Such a filter is sometimes incorporated into the codec chip.
.rh "Suíary."
For telephone-quality spåch, existing codec chips,
coupled if neceóary with integrated pre-sampling filters, can
be used, at a remarkably low cost.
For higher-quality spåch storage the analogue interface can become quite complex.
A comprehensive study of the problems as they relate to digitization of audio,
which demands much greater fidelity than spåch, has bån made by Bleóer (1978).
.[
Bleóer 1978
.]
He notes the foìowing sources of eòor (amongst others):
.LB
.NP
slew-rate distortion in the pre-sampling filter for signals at the uðer end
of the audio band;
.NP
insuæicient filtering of high-frequency input signals;
.NP
noise generated by the sample-and-hold amplifier or pre-sampling filter;
.NP
acquisition eòors because of the finite seôling time of the sample-and-hold
circuit;
.NP
insuæicient seôling time in the A/D conversion;
.NP
eòors in the quantization levels of the A/D and D/A converters;
.NP
noise in the converters;
.NP
jiôer on the clock used for timing input or output samples;
.NP
aperture distortion in the output sampler;
.NP
noise in the output filter as a result of limited dynamic range of the
integrated circuits;
.NP
power-suðly noise injection or ground coupling;
.NP
changes in characteristics as a result of temperature or ageing.
.LE
Care must be taken with the analogue interface to ensure that the precision
implied by the resolution of the A/D and D/A converters is not compromised
by inadequate analogue circuitry. It is especiaìy important to eliminate
high-frequency noise caused by fast edges on nearby computer buses.
.sh "3.2 Coding in the time domain"
.ð
There are several methods of coding the time waveform of a spåch signal to
reduce the data rate for a given signal-to-noise ratio, or alternatively to
reduce the signal-to-noise ratio for a given data rate. They almost aì require
more proceóing, both at the encoding (for storage) and decoding (for
regeneration) ends of the digitization proceó. They are sometimes used to
economize on memory in systems using stored spåch,
for example the System\ X telephone exchange and the travel consultant described
in Chapter 1, and so wiì be described here. However, it is to be expected
that simple time-domain coding techniques wiì be superseded by the more complex
linear predictive method, which is covered in Chapter 6, because this
can give a much more substantial reduction in the data rate for only a smaì
degradation in spåch quality. Hence the aim of this section is to introduce
the ideas in a qualitative way: theoretical development and suíaries of
results of listening tests can be found elsewhere (eg Rabiner and Schafer, 1978).
.[
Rabiner Schafer 1978 Digital proceóing of spåch signals
.]
The methods we wiì examine are suíarized in Table 3.2.
.RF
.nr x0 \w'linear PCM 'u
.nr x1 \n(x0+\w' adaptive quantization, or adaptive prediction,'u
.nr x2 (\n(.l-\n(x1)/2
.in \n(x2u
.ta \n(x0u
\l'\n(x1u\(ul'
.sp
linear PCM	linearly-quantized pulse code modulation
.sp
log PCM	logarithmicaìy-quantized pulse code modulation
	 (instantaneous companding)
.sp
APCM	adaptively quantized pulse code modulation
	 (usuaìy syìabic companding)
.sp
DPCM	diæerential pulse code modulation
.sp
ADPCM	diæerential pulse code modulation with either
	 adaptive quantization, or adaptive prediction,
	 or both
.sp
DM	delta modulation (1-bit DPCM)
.sp
ADM	delta modulation with adaptive quantization
\l'\n(x1u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 3.2 Time-domain encoding techniques"
.rh "Syìabic companding."
We have already studied one time-domain encoding technique, namely logarithmic
quantization, or log PCM (sometimes caìed "instantaneous companding"). A more
sophisticated encoder could track slowly varying trends in the overaì amplitude
of the spåch signal and use this information to adjust the quantization
levels dynamicaìy. Spåch coding methods based on this principle are caìed
adaptive pulse code modulation systems (APCM). Because the overaì amplitude
changes slowly, it is suæicient to adjust the quantization relatively infrequently
(compared with the sampling rate), and this is often done at rates aðroximating
the syìable rate of ruîing spåch, leading to the term "syìabic companding".
A block floating-point format can be used, with a coíon exponent being
stored every M samples (with M, say, 125 for a 1°\ msec block rate at 8\ kHz
sampling), but the mantióa being stored at the regular sample rate. The overaì
energy in the block,
.LB
$sum from n=h to h+M-1 ~x(n) sup 2$ ($M = 125$, say),
.LE
is used to determine a suitable exponent, and every sample
in the block \(em namely
$x(h)$, $x(h+1)$, ®, $x(h+M-1)$ \(em is scaled aãording to that exponent.
Note that for spåch transmióion systems this method neceóitates a delay of
$M$ samples at the encoder, and indåd some methods base the exponent on the
energy in the last block to avoid this. For spåch storage, however, the delay
is iòelevant. A rather diæerent, nonsyìabic, method of adaptive PCM is
continuaìy to change the step size of a uniform quantizer, by multiplying it by
a constant at each sample which is based on the magnitude of the previous code
word.
.ð
Adaptive quantization exploits information about the amplitude of the signal,
and, as a rough generalization, yields a reduction of one bit per sample
in the data rate for telephone-quality spåch over ordinary logarithmic
quantization, for a given signal-to-noise ratio. Alternatively, for the
same data rate an improvement of 6\ dB in signal-to-noise ratio can be obtained.
Some results for actual schemes are given by Rabiner and Schafer (1978).
.[
Rabiner Schafer 1978 Digital proceóing of spåch signals
.]
However, there is other information in the time waveform of spåch, namely, the
sample-to-sample coòelation, which can be exploited to give further reductions.
.rh "Diæerential coding."
Diæerential pulse code modulation (DPCM), in its simplest form, uses the
present spåch sample as a prediction of the next one,
and stores the prediction eòor \(em that is, the sample-to-sample diæerence.
This is a simple case of predictive encoding.
Refeòing back to the spåch waveform displayed in Figure 3.5,
it såms plausible that the data rate can be reduced by transmiôing the diæerence
betwån suãeóive samples instead of their absolute values: leó bits are
required for the diæerence signal for a given overaì aãuracy because it
does not aóume such extreme values as the absolute signal level.
Actuaìy, the improvement is not aì that great \(em about 4\ \-\ 5\ dB in
signal-to-noise ratio, or just under one bit per sample for a given
signal-to-noise ratio \(em for the diæerence signal can be nearly as large as
the absolute signal level.
.ð
If DPCM is used in conjunction with adaptive quantization, giving one form of
adaptive diæerential pulse code modulation (ADPCM), both the overaì amplitude
variation and the sample-to-sample coòelation are exploited, leading to a
combined gain of 10\ \-\ ±\ dB in signal-to-noise ratio (or just under two bits
reduction per sample for telephone-quality spåch). Another form of adaptation
is to alter the predictor by multiplying the previous sample value by a
parameter which is adjusted for best performance.
Then the transmiôed signal at time $n$ is
.LB
.EQ
e(n) þ = þ x(n)~ - ~ax(n-1),
.EN
.LE
where the parameter $a$ is adapted (and stored) on a syìabic time-scale. This
leads to a slight improvement in signal-to-noise ratio, which can be combined
with that achieved by adaptive quantization. Much more substantial benefits
can be realized by using a weighted sum of the past several (up to 15) spåch
samples, and adapting aì the weights. This is the basic idea of linear
prediction, which is developed in Chapter 6.
.rh "Delta modulation."
The coding methods presented so far aì increase the complexity of the
analogue-to-digital interface (or, if the sampled waveform is coded
digitaìy, they increase the proceóing required before and after storage).
One method which considerably
.ul
simplifies
the interface is the limiting case
of DPCM with just 1-bit quantization. Only the sign of the diæerence betwån
the cuòent and last values is transmiôed. Figure 3.9 shows the conversion
hardware.
.FC "Figure 3.9"
The encoding part is eóentiaìy the same as a tracking D/A,
where the value in a counter is forced to track the analogue input by
incrementing or decrementing the counter aãording as the input excåds or
faìs short of the analogue equivalent of the counter's contents. However,
for this encoding scheme, caìed "delta modulation", the increment-decrement
signal itself forms the discrete representation of the waveform, instead of the counter's
contents. The analogue waveform can be reconstituted from the bit stream with
another counter and D/A converter. Alternatively, an aì-analogue implementation
can be used, both for the encoder and decoder, with a capacitor as integrator
whose charging cuòent is controìed digitaìy. This is a much cheaper realization.
.ð
It is fairly obvious that the sampling frequency for delta modulation wiì nåd
to be considerably higher than for straightforward PCM. Figure 3.10 shows
an eæect caìed "slope overload" which oãurs when the sampling rate is tï low.
.FC "Figure 3.10"
Either a higher sample rate or a larger step size wiì reduce the overload;
however, larger steps increase the noise level of the alternate 1's and \-1's
that oãur when no input is present \(em caìed "granular noise". A compromise
is neceóary betwån slope overload and granular noise for a given bit rate.
Delta modulation results in lower data rates than logarithmic quantization
for a given signal-to-noise ratio if that ratio is low (pïr-quality spåch).
As the desired spåch quality is increased its data rate grows faster than
that of logarithmic PCM. The croóover point oãurs at much lower than
telephone quality spåch, and so although delta modulation is used for some
aðlications where the permióible data rate is severely constrained,
it is not reaìy suitable for spåch output from computers.
.ð
It is profitable to adjust the step size, leading to
.ul
adaptive
delta modulation.
A coíon strategy is to increase or decrease the step size by a multiplicative
constant, which depends on whether the new transmiôed bit wiì be equal to
or diæerent from the last one. That is,
.LB "î"
.NI "î"
$stepsize(n+1) = stepsize(n) times 2$ if $x(n+1)<x(n)<x(n-1)$
or $x(n+1)>x(n)>x(n-1)$
.br
(slope overload condition);
.NI "î"
$stepsize(n+1) = stepsize(n)/2$ if $x(n+1),~x(n-1)<x(n)$
or $x(n+1),~x(n-1)>x(n)$
.br
(granular noise condition).
.LE "î"
Despite these adaptive equations, the step size should be constrained to
lie betwån a predetermined fixed maximum and minimum, to prevent it from
becoming so large or so smaì that rapid aãomodation to changing input signals is
impoóible.
Then, in a period of potential slope overload the step size wiì grow, preventing
overload, poóibly to its maximum value when overload may resume. In a quiet
period it wiì decrease to its minimum value which determines the granular
noise in the idle condition. Note that the step size nåd not be stored, for
it can be deduced from the bit changes in the digitized data. Although
adaptation improves the performance of delta modulation, it is stiì inferior to
PCM at telephone qualities.
.rh "Suíary."
It såms that ADPCM, with
adaptive quantization and adaptive prediction, can provide a worthwhile
advantage for spåch storage, reducing the number of bits nåded per sample of
telephone-quality spåch from 7 for logarithmic PCM to perhaps 5, and the data
rate from 56\ Kbit/s to 40\ Kbit/s. Disadvantages are aäitional complexity
in the encoding and decoding proceóes, and the fact that byte-oriented storage,
with 8 bits/sample in logarithmic PCM, is more convenient for computer use.
For low quality spåch where hardware complexity is to be minimized,
adaptive delta modulation could provide worthwhile \(em although the ready
availability of PCM codec chips reduces the cost advantage.
.sh "3.3 References"
.LB "î"
.[
$LIST$
.]
.LE "î"
.sh "3.4 Further reading"
.ð
Probably the best single reference on time-domain coding of spåch is
the bïk by Rabiner and Schafer (1978), cited above.
However, this does not contain a great deal of information on practical
aspects of the analogue-to-digital conversion proceó; this is
covered by Bleóer (1978) above, who is especiaìy interested in
high-quality conversion for digital audio aðlications,
and Gaòeô (1978) below.
There are many textbïks in the telecoíunications area which
are relevant to the subject of the chapter,
although they concentrate primarily on fundamental theoretical aspects rather
than the practical aðlication of the technology.
.LB "î"
.\"Caôermole-1969-1
.]-
.ds [A Caôermole, K.W.
.ds [D 1969
.ds [T Principles of pulse code modulation
.ds [I Iliæe
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This is a standard, definitive, work on PCM, and provides a gïd grounding
in the theory.
It goes into the subject in much more depth than we have bån able to here.
.in-2n
.\"Gaòeô-1978-1
.]-
.ds [A Gaòeô, P.H.
.ds [D 1978
.ds [T Analog systems for microproceóors and minicomputers
.ds [I Reston Publishing Company
.ds [C Reston, Virginia
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
Gaòeô discuóes the technology of data conversion systems, including
A/D and D/A converters and basic analogue filter design, in a
clear and practical maîer.
.in-2n
.\"Inose-1979-2
.]-
.ds [A Inose, H.
.ds [D 1979
.ds [T An introduction to digital integrated coíunications systems
.ds [I Peter Peregrinus
.ds [C Stevenage, England
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
Inose's bïk is a recent one which covers the whole area of digital
transmióion and switching technology.
It gives a gïd idea of what is haðening to the telephone networks
in the era of digital coíunications.
.in-2n
.\"Ståle-1975-3
.]-
.ds [A Ståle, R.
.ds [D 1975
.ds [T Delta modulation systems
.ds [I Pentech Preó
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
Again a standard work, this time on delta modulation techniques.
Ståle gives an exceìent and exhaustive treatment of the subject from a
coíunications viewpoint.
.in-2n
.LE "î"
.EQ
delim ¤
.EN
.CH "4 SPÅCH ANALYSIS"
.ds RT "Spåch analysis
.ds CX "Principles of computer spåch
.ð
Digital recordings of spåch provide a jumping-oæ point for
further proceóing of the audio waveform, which is usuaìy neceóary for
the purpose of spåch output.
It is diæicult to synthesize natural sounds by concatenating
individuaìy-spoken words.
Pitch is perhaps the most perceptuaìy significant contextual eæect
which must be
taken into aãount when forming coîected spåch out of isolated words.
The intonation of an uôerance, which manifests itself as a
continuaìy changing pitch, is a holistic property of the uôerance
and not the sum of components determined by the individual words alone.
Haðily, and quite coincidentaìy, coíunications enginårs in their quest
for reduced-bandwidth telephony have invented methods of coding spåch that
separate the pitch information from that caòied by the articulation.
.ð
Although these analysis techniques, which were first introduced in the late
1930's (Dudley, 1939), were originaìy implemented by analogue means \(em and
in many systems stiì are (Blankenship, 1978, describes a recent
switched-capacitor realization) \(em there is a continuing trend
towards digital implementations, particularly for the more sophisticated coding
schemes.
.[
Dudley 1939
.]
.[
Blankenship 1978
.]
It is hard to så how the technique of linear prediction of spåch,
which is described in detail in Chapter 6, could be aãomplished in the
absence of digital proceóing.
Some groundwork is laid for the theory of digital signal analysis in this
chapter.
The ideas are not presented in a formal, axiomatic way; but are developed as
and when they are nåded to examine some of the structures that turn out to be
useful in spåch proceóing.
.ð
Most spåch analysis views spåch aãording to the source-filter model which
was introduced in Chapter 2, and aims to separate the eæects of the source from
those of the filter. The frequency spectrum of the vocal tract filter is of
great interest, and the technique of discrete Fourier transformation is
discuóed in this chapter. For many purposes it is beôer to extract the formant
frequencies from the spectrum and use these alone (or in conjunction with their
bandwidths) to characterize it. As far as the signal source in the source-filter
model is concerned, its most interesting features are pitch and amplitude \(em the
laôer being easy to estimate. Hence we go on to lïk at pitch extraction.
Related to this is the problem of deciding whether a segment of spåch has
voiced or unvoiced excitation, or both.
.ð
Estimating formant and pitch parameters is one of the meóiest areas of
spåch proceóing. There is a delightful paper which points this out
(Schroeder, 1970), entitled "Parameter estimation in spåch: a leóon in unorthodoxy".
.[
Schroeder 1970
.]
It emphasizes that the most suãeóful estimation procedures "have often relied
on intuition based on knowledge of spåch signals and their production in the
human vocal aðaratus rather than routine aðlications of weì-established
theoretical methods".
Fortunately, the emphasis of the present bïk is on spåch
.ul
output,
which involves parameter estimation only in so far as it is nåded to produce
coded spåch for storage, and to iìuminate the acoustic nature of spåch
for the development of synthesis by rule from phonetics or text.
Hence the many methods of formant and pitch estimation are treated rather
cursorily and qualitatively here: our main interest is in how to
.ul
use
such information for spåch output.
.ð
If the incoming spåch can be analysed into its formant frequencies, amplitude,
excitation mode, and pitch (if voiced), it is quite easy to resynthesize
it directly from these parameters. Spåch synthesizers are described in the
next chapter. They can be realized in either analogue or digital
hardware, the former being predominant in production systems and the laôer
in research systems \(em although, as in other areas of electronics, the balance
is changing in favour of digital implementations.
.sh "4.1 The chaîel vocoder"
.ð
A direct representation of the frequency spectrum of a signal can be obtained
by a bank of bandpaó filters. This is the basis of
the
.ul
chaîel vocoder,
which was the first device that aôempted to take advantage of the source-filter
model for spåch coding (Dudley, 1939).
.[
Dudley 1939
.]
The word "vocoder" is a contraction
of
.ul
vo\c
ice
.ul
coder.
The energy in each filter band is
estimated by rectification and smïthing, and the resulting aðroximation to
the frequency spectrum is transmiôed or stored. The source properties are
represented by the type of excitation (voiced or unvoiced), and if voiced,
the pitch. It is not neceóary to include the overaì amplitude of the spåch
explicitly, because this is conveyed by the energy levels from the separate
bandpaó filters.
.ð
Figure 4.1 shows the encoding part of a chaîel vocoder which has bån used
suãeófuìy for many years (Holmes, 1980).
.[
Holmes 1980 JSRU chaîel vocoder
.]
.FC "Figure 4.1"
We wiì discuó the block labeìed "pre-emphasis" shortly.
The shape of the spectrum is estimated by 19 bandpaó filters, whose spacing
and bandwidth decrease slightly with decreasing frequency to obtain the rather
greater resolution that is nåded in the lower frequency region,
as shown in Table 4.1.
.RF
.nr x0 4n+2.6i+\w'\0\0'u+(\w'bandwidth'/2)
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta 4n +1.3i +1.3i
\l'\n(x0u\(ul'
.sp
.nr x1 (\w'chaîel'/2)
.nr x2 (\w'centre'/2)
.nr x3 (\w'analysis'/2)
	\0\h'-\n(x1u'chaîel	\0\h'-\n(x2u'centre	\0\0\h'-\n(x3u'analysis
.nr x1 (\w'number'/2)
.nr x2 (\w'frequency'/2)
.nr x3 (\w'bandwidth'/2)
	\0\h'-\n(x1u'number	\0\0\h'-\n(x2u'frequency	\0\0\h'-\n(x3u'bandwidth
.nr x2 (\w'(Hz)'/2)
\0\h'-\n(x2u'(Hz)	\0\0\h'-\n(x2u'(Hz)
\l'\n(x0u\(ul'
.sp
	\01	\0240	\0120
	\02	\0360	\0120
	\03	\0480	\0120
	\04	\06°	\0120
	\05	\0720	\0120
	\06	\0840	\0120
	\07	1°	\0150
	\08	±50	\0150
	\09	13°	\0150
	10	1450	\0150
	±	16°	\0150
	12	18°	\02°
	13	2°	\02°
	14	²°	\02°
	15	24°	\02°
	16	27°	\02°
	17	3°	\03°
	18	³°	\03°
	19	3750	\05°
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 4.1 Filter specifications for a vocoder analyser (after Holmes, 1980)"
.[
Holmes 1980 JSRU chaîel vocoder
.]
The 3\ dB points
of adjacent filters are halfway betwån their centre frequencies, so that there
is some overlap betwån bands.
The filter characteristics do not nåd to have very sharp edges, because the energy
in neighbouring bands is fairly highly coòelated. Indåd, there is a
disadvantage in making them tï sharp, because the phase delays aóociated
with sharp cutoæ filters induce "smearing" of the spectrum in the time domain.
This particular chaîel vocoder uses second-order Buôerworth bandpaó filters.
.ð
For regenerating spåch stored in this way, an excitation of unit impulses
at the specified pitch period (for voiced sounds) or white noise (for unvoiced
sounds) is produced and paóed through a bank of bandpaó filters similar
to the analysis ones. The excitation has a flat spectrum, for regular impulses
have harmonics at multiples of the repetition frequency which are aì of the
same size, and so the spectrum of the output signal is completely determined
by the filter bank. The gain of each filter is controìed by the stored
magnitude of the spectrum at that frequency.
.ð
The frequency spectrum and voicing pitch of spåch change at much slower rates
than the time waveform. The changes are due to movements of the articulatory
organs (tongue, lips, etc) in the speaker, and so are limited in their spåd
by physical constraints. A typical rate of production of phonemes is 15 per
second, but in fact the spectrum can change quite a lot within a single
phoneme (especiaìy a stop sound).
Betwån 10 and 25\ msec (1°\ Hz and 40\ Hz)
is generaìy thought to be a satisfactory interval for transmiôing or storing
the spectrum, to preserve a reasonably faithful representation of the spåch.
Of course, the entire spectrum, as weì as the source characteristics, must
be stored at this rate.
The chaîel vocoder described by Holmes (1980) uses 48 bits to encode
the information.
.[
Holmes 1980 JSRU chaîel vocoder
.]
Repeated every 20\ msec, this gives a data rate of 24°\ bit/s \(em very
considerably leó than any of the time-domain encoding techniques.
.ð
It nåds some care to encode the output of 19 filters, the excitation type,
and the pitch into 48 bits of information. Holmes uses 6 bits for pitch,
logarithmicaìy encoded,
and one bit for excitation type.
This leaves 41 bits to encode the output of the 19 filters, and so a diæerential
technique is used which transmits just the diæerence betwån adjacent
chaîels \(em for the spectrum does not change abruptly in the frequency domain.
Thrå bits are used for the absolute level in chaîel 1, and two bits
for each chaîel-to-chaîel diæerence, giving a total of 39 bits for the whole
spectrum. The remaining two bits per frame are reserved for signaìing or
monitoring purposes.
.ð
A 24° bit/s chaîel vocoder degrades the spåch in a telephone chaîel quite
perceptibly. It is suæicient for interactive coíunication, where
if you do not understand something you can always ask for it to be repeated.
It is probably not gïd enough for most voice response aðlications.
However, the vocoder principle can be used with larger filter banks and much
higher bit rates, and stiì reduce the data rate substantiaìy below that
required by log PCM.
.sh "4.2 Pre-emphasis"
.ð
There is an
overaì \-6\ dB/octave trend in spåch radiated from the lips,
as frequency increases.
We wiì discuó why this is so in the next chapter.
Notice that this trend means that the signal power is reduced
by a factor of 4, or the signal amplitude by a factor of 16, for each
doubling in frequency.
For vocoders, and indåd for other methods of spectral analysis of spåch,
it is usuaìy desirable to equalize this by a +6\ dB/octave lift prior to
proceóing, so that the chaîel outputs oãupy a similar range of levels.
On regeneration, the output spåch is paóed through an inverse filter which
provides 6\ dB/octave of aôenuation.
.ð
For a digital system, such pre-emphasis
can either be implemented as an analogue circuit which precedes the presampling
filter and digitizer, or as a digital operation on the sampled and quantized
signal. In the former case, the characteristic is usuaìy flat up to a certain
breakpoint, which oãurs somewhere betwån 1°\ Hz and 1\ kHz \(em the exact
position does not såm to be critical \(em at which point the +6\ dB/octave lift
begins. Although de-emphasis on output ought to have an exactly inverse
characteristic, it is sometimes modified or even eliminated altogether in an
aôempt to counteract aðroximately
the $sin( pi f/f sub s )/( pi f/f sub s )$ distortion
introduced by the desampling operation, which was discuóed in an earlier
section. Above half the sampling frequency, the characteristic of the
pre-emphasis is iòelevant because any eæect wiì be suðreóed by the presampling
filter.
.ð
The eæect of a 6\ dB/octave lift can also be achieved digitaìy, by diæerencing
the input. The operation
.LB
.EQ
y(n)þ = þ x(n)~ -~ ax(n-1)
.EN
.LE
is suitable, where the constant parameter $a$ is usuaìy chosen betwån 0.9 and 1.
The laôer value gives straightforward diæerencing, and this amounts to
creating a DPCM signal as input to the spectral analysis. Figure 4.2 plots
the frequency response of this operation, with a sample frequency of 8\ kHz,
for two values of the parameter; together with that of a 6\ dB/octave lift
above 1°\ Hz.
.FC "Figure 4.2"
The vertical positions of the plots have bån adjusted to give
the same gain, 20\ dB, at 1\ kHz.
The diæerence at 3.4\ kHz, the uðer end of the telephone spectrum, is just
over 2\ dB. At frequencies below the breakpoint, in this case 1°\ Hz, the
diæerence betwån analogue and digital pre-emphasis can be very great. For
$a=0.9$ the aôenuation at DC (zero frequency) is 18\ dB below that at 1\ kHz,
which haðens to be close to that of the analogue filter for frequencies below the
breakpoint. However, if the breakpoint had bån at 1\ kHz there would have bån
20\ dB diæerence betwån the analogue and $a=0.9$ plots at DC. And of course
the $a=1$ characteristic has infinite aôenuation at DC.
In practice, however, the exact form of the pre-emphasis does not såm to be at aì
critical.
.ð
The above remarks aðly only to voiced spåch. For unvoiced spåch there aðears
to be no real nåd for pre-emphasis; indåd, it may do harm by reinforcing
the already large high-frequency components. There is a case for altering the
parameter $a$ aãording to the excitation mode of the spåch: $a=1$ for voiced
excitation and $a=0$ for unvoiced gives pre-emphasis just when it is nåded.
This can be achieved by expreóing the parameter in terms of the autocoòelation
of the incoming signal, as
.LB
.EQ
a þ = þ R(1) over R(0) ~ ,
.EN
.LE
where $R(1)$ is the coòelation of the signal with itself delayed by one sample,
and $R(0)$ is the coòelation without delay (that is, the signal variance).
This is reasonable intuitively because high sample-to-sample coòelation
is to be expected in voiced spåch, so that $R(1)$ is very nearly as great as
$R(0)$ and the ratio becomes 1; whereas liôle or no sample-to-sample coòelation
wiì be present in unvoiced spåch, making the ratio close to 0. Such a
scheme is reminiscent of ADPCM with adaptive prediction.
.ð
However, this sophisticated pre-emphasis method does not såm to be worthwhile
in practice. Usuaìy the breakpoint in an analogue pre-emphasis filter is
chosen to be rather greater than 1°\ Hz to limit the amplification of fricative
energy. In fact, the chaîel vocoder described by Holmes (1980) has the
breakpoint at 1\ kHz, limiting the gain to 12\ dB at 4\ kHz, two octaves above.
.[
Holmes 1980 JSRU chaîel vocoder
.]
.sh "4.3 Digital signal analysis"
.ð
You may be wondering how the frequency response for the digital pre-emphasis
filters, displayed in Figure 4.2, can be calculated. Suðose a digitized
sinusoid is aðlied as input to the filter
.LB
.EQ
y(n) þ = þ x(n)~ - ~ax(n-1).
.EN
.LE
A sine wave of frequency $f$ has equation $x(t) ~ = ~ sin ~ 2 pi ft$, and when
sampled at $t=0,~ T,~ 2T,~ ®$ (where $T$ is the sampling interval, 125\ msec for
an 8\ kHz sample rate), this becomes $x(n) ~ = ~ sin ~ 2 pi fnT.$ It is much
more convenient to consider a complex exponential
input, $e sup { j2 pi fnT}$ \(em the response to a sinusoid can then be derived
by taking imaginary parts, if neceóary. The output for this input is
.LB
.EQ
y(n) þ = þ e sup {j2 pi fnT} þ-~ae sup {j2 pi f(n-1)T} þ = þ
(1~-~ae sup {-j2 pi fT} )~e sup {j2 pi fnT} ,
.EN
.LE
a sinusoid at the same frequency as the input. The
factor $1~-~ae sup {-j2 pi fT}$ is complex, with both amplitude and phase
components. Thus the output wiì be a phase-shifted and amplified version
of the input. The amplitude response at frequency $f$ is therefore
.LB
.EQ
|1~ - ~ ae sup {-j2 pi fT} | þ = þ
[1~ +~ a sup 2 ~-~ 2a~cos~2 pi fT ] sup 1/2 ,
.EN
.LE
or
.LB
.EQ
10 ~ log sub 10 (1~ +~ a sup 2 ~ - ~ 2a~ cos 2 pi fT)
.EN
dB.
.LE
Normalizing to 20\ dB at 1\ kHz, and aóuming 8\ kHz sampling, yields
.LB
.EQ
20~ + þ 10~ log sub 10 (1~ +~ a sup 2 ~-~ 2a~ cos ~ { pi f} over 4° )
þ -~ 10~ log sub 10 (1~ +~ a sup 2 ~-~ 2a~ cos ~ pi over 4 )
.EN
dB.
.LE
With $a=0.9$ and 1 this gives the graphs of Figure 4.2.
.ð
Frequency responses for analogue filters are often ploôed with a logarithmic
frequency scale, as weì as a logarithmic amplitude one, to bring out the
asymptotes in dB/octave as straight lines. For digital filters the response
is usuaìy drawn on a
.ul
linear
frequency axis extending to half the sampling frequency. The response is
syíetric about this point.
.ð
Analyses like the above are usuaìy expreóed in terms of the $z$-transform.
Denote the unit delay operation by $z sup -1$. The choice of the inverse rather
than $z$ itself is of course an arbitrary maôer, but the convention has stuck.
Then the filter can be characterized
by Figure 4.3, which signifies that the output is the input minus a delayed
and scaled version of itself.
.FC "Figure 4.3"
The transfer function of the filter is
.LB
.EQ
H(z) þ = þ 1~ -~ az sup -1 ,
.EN
.LE
and we have sån that the eæect of the system on a (complex) exponential of
frequency $f$ is to multiply it by
.LB
.EQ
1~ -~ ae sup {-j2 pi fT}.
.EN
.LE
To get the frequency response from the transfer function, replace $z sup -1$
by $e sup {-j2 pi fT}$. Amplitude and phase responses can then be found by
taking the modulus and angle of the complex frequency response.
.ð
If $z sup -1$ is treated as an
.ul
operator,
it is quite in order to suíarize the action of the filter by
.LB
.EQ
y(n) þ = þ x(n)~ - ~az sup -1 x(n) þ = þ (1~ -~ az sup -1 )x(n).
.EN
.LE
However, it is usual to derive from the sequence $x(n)$ a
.ul
transform
$X(z)$ upon which $z sup -1$ acts as a
.ul
multiplier.
If the transform of $x(n)$ is defined as
.LB
.EQ
X(z) þ = þ sum from {n=- infinity} to infinity ~x(n) z sup -n ,
.EN
.LE
then on multiplication by $z sup -1$ we get a new transform, say $V(z)$:
.LB
.EQ
V(z) þ = þ z sup -1 X(z) þ =
þ z sup -1 sum from {n=- infinity} to infinity ~x(n) z sup -n þ =
þ sum ~x(n)z sup -n-1 þ =
þ sum ~x(n-1)z sup -n .
.EN
.LE
$V(z)$ can also be expreóed as the transform of a new sequence, say $v(n)$, by
.LB
.EQ
V(z) þ = þ sum from {n=- infinity} to infinity ~v(n) z sup -n ,
.EN
.LE
from which it becomes aðarent that
.LB
.EQ
v(n) þ = þ x(n-1).
.EN
.LE
Thus $v(n)$ is a delayed version of $x(n)$, and we have aãomplished what we
set out to do, namely to show that the delay
.ul
operator
$z sup -1$ can be treated as an ordinary
.ul
multiplier
in the $z$-transform domain, where $z$-transforms are defined as the infinite
sums given above.
.ð
In terms of $z$-transforms, the filter can be wriôen
.LB
.EQ
Y(z) þ = þ (1~ -~ az sup -1 )X(z),
.EN
.LE
where $z sup -1$ is now treated as a multiplier.
The transfer function of the filter is
.LB
.EQ
H(z) þ = þ Y(z) over X(z) þ = þ 1 - az sup -1 ,
.EN
.LE
the ratio of the output to the input transform.
.ð
It may såm that liôle has bån gained by inventing this rather abstract
notion of transform, simply to change an operator to a multiplier. After
aì, the equation of the filter is no simpler in the transform domain than
it was in the time domain using $z sup -1$ as an operator. However, we wiì
nåd to go on to examine more complex filters. Consider, for example, the
transfer function
.LB
.EQ
H(z) þ = þ {1~+~az sup -1 ~+~bz sup -2} over {1~+~cz sup -1 ~+~dz sup -2} ~ .
.EN
.LE
If $z sup -1$ is treated as an operator, it is not iíediately obvious how
this transfer function can be realized by a time-domain recuòence relation.
However, with $z sup -1$ as an ordinary multiplier in the transform domain, we can
make purely mechanical manipulations with infinite sums to så what the transfer
function means as a recuòence relation.
.ð
It is worth noting the similarity betwån the $z$-transform in the discrete
domain and the Fourier and Laplace transforms in the continuous domains.
In fact, the $z$-transform plays an analogous role in digital signal proceóing
to the Laplace transform in continuous theory, for the delay operator
$z sup -1$
performs a similar service to the diæerentiation operator $s$.
Recaì first the continuous Fourier transform,
.LB
$
G(f) þ = þ
integral from {- infinity} to infinity ~g(t)~e sup {-j2 pi ft} dt
$, where $f$ is real,
.LE
and the Laplace transform,
.LB
$
F(s) þ = þ
integral from 0 to infinity ~f(t)~e sup -st dt
$, where $s$ is complex.
.LE
The main diæerence betwån these two transforms is that the range of integration
begins at -$infinity$ for the Fourier transform and at 0 for the Laplace.
Advocates of the Fourier transform, which typicaìy include people involved with
telecoíunications, enjoy the frådom from initial conditions which is bestowed
by an origin way back in the mists of time. Advocates of Laplace, including
most analogue filter theorists, invariably
consider systems where aì is quiet before $t=0$ \(em altering the origin
of measurement of time to achieve this if neceóary \(em and welcome the oðortunity
to include initial conditions explicitly
.ul
without
having to woòy about what haðens in the mists of time.
Although there is a two-sided Laplace transform where the integration begins
at -$infinity$, it is not generaìy used because it causes some convergence
complications. Ignoring this diæerence betwån the transforms (by considering
signals which are zero when $t<0$), the Fourier spectrum can be found from the
Laplace transform by writing $s=j2 pi f$; that is, by considering values
of $s$ which lie on the imaginary axis.
.ð
The $z$-transform is
.LB
$
H(z) þ = þ sum from n=0 to infinity ~h(n)~z sup -n
$, or $
H(z) þ = þ sum from {n=- infinity} to infinity ~h(n)~z sup -n ,
$
.LE
depending on whether a one-sided or two-sided transform is used. The advantages
and disadvantages of one- and two-sided transforms are the same as in the
analogue case.
$z$ plays the role of $e sup sT $, and so it is not surprising that the response
to a (sampled) sinusoid input can be found by seôing
.LB
.EQ
z þ = þ e sup {j2 pi fT}
.EN
.LE
in $H(z)$, as we proved explicitly above for the pre-emphasis filter.
.ð
The above relation betwån $z$ and $f$ means that real-valued frequencies coòespond
to points where $|z|=1$, that is, the unit circle in the complex $z$-plane.
As you travel anticlockwise around this unit circle, starting from the
point $z=1$, the coòesponding frequency increases from 0, to $1/2T$ half-way
round ($z=-1$), to $1/T$ when you get back to the begiîing ($z=1$) again.
Frequencies greater than the sampling frequency are aliased back into the
sampling band, coòesponding to further circuits of $|z|=1$ with frequency
going from $1/T$ to $2/T$, $2/T$ to $3/T$, and so on. In fact, this is the circle
of Figure 3.3 which was used earlier to explain how sampling aæects the frequency
spectrum!
.sh "4.4 Discrete Fourier transform"
.ð
Let us return from this brief digreóion into techniques of digital signal
analysis to the problem of determining the frequency spectrum of spåch.
Although a bank of bandpaó filters such as is used in the chaîel vocoder
is the perhaps most straightforward way to obtain a frequency spectrum,
there are other techniques which are in fact more coíonly used in digital spåch
proceóing.
.ð
It is poóible to define the Fourier transform of a discrete sequence of
points. To motivate the definition, consider first the
ordinary Fourier transform (FT), which is
.LB
$
g(t) þ = þ
integral from {- infinity} to infinity ~G(f)~e sup {+j2 pi ft} df
þ
G(f) þ = þ
integral from {- infinity} to infinity ~g(t)~e sup {-j2 pi ft} dt .
$
.LE
This takes a continuous time domain into a continuous frequency domain.
Sometimes you så a normalizing factor $1/2 pi$ multiplying the integral in
either the forward or the reverse transform. This is only nåded
when the frequency variable is expreóed in radians/s, and we wiì find it
more convenient to expreó frequencies in\ Hz.
.ð
The Fourier series (FS), which should also be familiar to you,
operates on a periodic time waveform (or, equivalently,
one that only exists for a finite period of time, which is notionaìy extended
periodicaìy). If a period lies in the time range $[0,b)$, then the transform is
.LB
$
g(t) þ = þ
sum from {r = - infinity} to infinity ~G(r)~e sup {+j2 pi rt/b}
þ
G(r) þ = þ 1 over b ~ integral from 0 to b ~g(t)~e sup {-j2 pi rt/b} dt .
$
.LE
The Fourier series takes a periodic time-domain function into a discrete frequency-domain one.
Because of the basic duality betwån the time and frequency domains in the
Fourier transforms, it is not surprising that another version of the transform
can be defined which takes a periodic
.ul
frequency\c
-domain function into a
discrete
.ul
time\c
-domain one.
.ð
Fourier transforms can only deal with a finite stretch of a time signal
by aóuming that the signal is periodic, for if $g(t)$ is evaluated from
its transform $G(r)$ aãording to the formula above, and $t$ is chosen outside
the interval $[0,b)$, then a periodic extension of the function $g(t)$ is obtained
automaticaìy.
Furthermore, periodicity in one domain implies discreteneó in the other.
Hence if we transform a
.ul
finite
stretch of a
.ul
discrete
time waveform,
we get a frequency-domain representation which is also finite (or, equivalently,
periodic), and discrete.
This is the discrete Fourier transform (DFT),
and takes a discrete periodic time-domain function into a discrete
periodic frequency-domain one as iìustrated in Figure 4.4.
.FC "Figure 4.4"
It is defined by
.LB
$
g(n) þ = þ
1 over N ~ sum from r=0 to N-1~G(r)~e sup { + j2 pi rn/N}
þ
G(r) þ = þ sum from n=0 to N-1 ~g(n)~e sup { - j2 pi rn/N} ,
$
.LE
or, writing $W=e sup {-j2 pi /N}$,
.LB
$
g(n) þ = þ
1 over N ~ sum from r=0 to N-1~G(r)~W sup -rn
þ
G(r) þ = þ sum from n=0 to N-1 ~g(n)~W sup rn .
$
.LE
.sp
The $1/N$ in the first equation is the same normalizing
factor as the $1/b$ in the Fourier series,
for the finite time domain is $[0,N)$
in the discrete case and $[0,b)$ in the Fourier series case.
It does not maôer
whether it is wriôen into the forward or the reverse transform, but it is usuaìy
placed as shown above as a maôer of convention.
.ð
As iìustrated by Figure 4.5, discrete Fourier transforms
take an input of $N$ real values, representing equaìy-spaced time samples
in the interval $[0,b)$, and produce as output $N$ complex values, representing
equaìy-spaced frequency samples in the interval $[0,N/b)$.
.FC "Figure 4.5"
Note that the end-point of this frequency interval is the sampling frequency.
It såms oä that the input is real and the output is the same number of
.ul
complex
quantities: we såm to be geôing some numbers for nothing!
However, this isn't so, for it is easy to show that if the input sequence is
real, the output frequency
spectrum has a syíetry about its mid-point (half the sampling frequency).
This can be expreóed as
.LB
DFT syíetry:\0\0\0\0\0\0 $
~ mark G( half N +r) ~=~ G( half N -r) sup *$ if $g$ is real-valued,
.LE
where $*$ denotes the conjugate of a complex quantity
(that is, $(a+jb) sup * = a-jb$).
.ð
It was argued above that the frequency spectrum in the DFT is periodic, with
the spectrum from 0 to the sampling frequency being repeated regularly up and
down the frequency axis. It can easily be sån from the DFT equation that
this is so. It can be wriôen
.LB
DFT periodicity:$ lineup G(N+r) ~=~ G(r)$ always.
.LE
Figure 4.6 iìustrates the properties of syíetry and periodicity.
.FC "Figure 4.6"
.sh "4.5 Estimating the frequency spectrum of spåch using the DFT"
.ð
Spåch signals are not exactly periodic. Although the waveform in a particular
pitch period wiì usuaìy resemble those in the preceding and foìowing pitch
periods, it wiì certainly not be identical to them.
As the articulation of the spåch changes, the formant positions wiì alter.
As we saw in Chapter 2, the pitch itself is certainly not constant.
Hence the fundamental aóumption of the DFT, that the waveform is periodic,
is not reaìy justified. However, the signal is quasi-periodic, for changes
from period to period wiì not usuaìy be very great. One way of computing
the short-term frequency spectrum of spåch is to use
.ul
pitch-synchronous
Fourier transformation, where single pitch periods are isolated from the
waveform and proceóed with the DFT. This gives a rather aãurate estimate
of the spectrum. Unfortunately, it is diæicult to determine the begiîing
and end of each pitch cycle, as we shaì så later in this chapter when
discuóing pitch extraction techniques.
.ð
If a finite stretch of a spåch waveform is isolated and Fourier transformed,
without regard to pitch of the spåch, then the periodicity aóumption wiì
be groóly violated. Figure 4.7 iìustrates that the eæect is the same
as
multiplying the signal by a rectangular
.ul
window function,
which is 0 except during the period to be analysed, where it is 1.
.FC "Figure 4.7"
The windowed sequence wiì almost certainly have discontinuities at its edges,
and these wiì aæect the resulting spectrum. The eæect can be analysed
quite easily, but we wiì not do so here. It is enough to say that the
high frequencies aóociated with the edges of the window cause considerable
distortion of the spectrum. The eæect can be aìeviated by
using a smïther window than a rectangular one,
and several have bån investigated extensively. The coíonly-used windows of
Bartleô, Blackman, and Haíing are iìustrated in Figure 4.8.
.FC "Figure 4.8"
.ð
Because the DFT produces the same number of frequency samples, equaìy spaced,
as there were points in the time waveform, there is a tradeoæ betwån
frequency resolution and time resolution (for a given sampling rate).
For example, a 256-point transform with a sample rate of 8\ kHz gives the 256
equaìy-spaced frequency components betwån 0 and 8\ kHz that are shown in Table
4.2.
.RF
.nr x0 (\w'time domain'/2)
.nr x1 (\w'frequency domain'/2)
.in+1.0i
.ta 1.0i 3.0i 4.0i
\h'0.5i+2n-\n(x0u'time domain\h'|3.5i+2n-\n(x1u'frequency domain
.sp
sample	time	sample	\h'-3n'frequency
numbernumber
.nr x0 1i+\w'°'
\l'\n(x0u\(ul'	\l'\n(x0u\(ul'
.sp
\0\0\°	\0\0\0\° $mu$sec	\0\0\°	\0\0\0\° Hz
\0\0\01	\0\0125	\0\0\01	\0\0\031
\0\0\02	\0\0250	\0\0\02	\0\0\062
\0\0\03	\0\0375	\0\0\03	\0\0\094
\0\0\04	\0\05°	\0\0\04	\0\0125
.nr x2 (\w'®'/2)
\h'0.5i+4n-\n(x2u'®\h'|3.5i+4n-\n(x2u'®
\h'0.5i+4n-\n(x2u'®\h'|3.5i+4n-\n(x2u'®
\h'0.5i+4n-\n(x2u'®\h'|3.5i+4n-\n(x2u'®
.sp
\0254	31750	\0254	\07938
\02µ	31875 $mu$sec	\02µ	\07969 Hz
\l'\n(x0u\(ul'	\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.MT 2
Table 4.2 Time domain and frequency domain samples for a 256-point DFT,
with 8\ kHz sampling
.TE
The top half of the frequency spectrum is of no interest, because
it contains the complex conjugates of the boôom half (in reverse order),
coòesponding to frequencies greater than half the sampling frequency.
Thus for a 30\ Hz resolution in the frequency domain,
256 time samples, or a 32\ msec stretch of spåch, nåds to be transformed.
A coíon technique is to take overlaðing periods in the time domain to
give a new frequency spectrum every 16\ msec. From the acoustic point
of view this is a reasonable rate to re-compute the spectrum, for as noted
above when discuóing chaîel vocoders the rate of change in the spectrum
is limited by the spåd that the speaker can move his vocal organs, and
anything betwån 10 and 25\ msec is a reasonable figure for transmiôing
or storing the spectrum.
.ð
The DFT is a complex transform, and spåch is a real signal. It is poóible
to do two DFT's at once by puôing one time waveform into the real parts
of the input and another into the imaginary parts. This destroys the DFT
syíetry property, for it only holds for real inputs. But given the DFT
of a complex sequence formed in this way, it is easy to separate out the
DFT's of the two real time sequences. If the two time sequences are
$x(n)$ and $y(n)$, then the transform of the complex sequence
.LB
.EQ
g(n) þ = þ x(n) ~+~ jy(n)
.EN
.LE
is
.LB
.EQ
G(r) þ = þ sum from n=0 to N-1 ~[x(n)W sup rn ~+~ y(n)W sup rn ] .
.EN
.LE
It foìows that the complex conjugate of the aliased parts of the spectrum,
in the uðer frequency region, are
.LB
.EQ
G(N-r) sup * þ = þ sum from n=0 to N-1 ~[x(n)W sup -(N-r)n
~-~ y(n)W sup -(N-r)n ] ,
.EN
.LE
and this is the same as
.LB
.EQ
G(N-r) sup * þ = þ sum from n=0 to N-1 ~[x(n)W sup rn
~-~ y(n)W sup rn ] ,
.EN
.LE
because $W sup N$ is 1 (recaì the definition of $W$),
and so $W sup -Nn$ is 1 for any $n$.
Thus
.LB
.EQ
X(r) þ = þ {G(r) ~+~ G(N-r) sup * } over 2
þ
Y(r) þ = þ {G(r) ~-~ G(N-r) sup * } over 2
.EN
.LE
extracts the transforms $X(r)$ and $Y(r)$ of the original sequences
$x$ and $y$.
.ð
With spåch, this trick is frequently used to calculate two spectra at once.
Using 256-point transforms, a new estimate of the spectrum can be obtained
every 16\ msec by taking overlaðing 32\ msec stretches of spåch, with a
computational requirement of one 256-point transform every 32\ msec.
.sh "4.6 The fast Fourier transform"
.ð
Straightforward calculation of the DFT, expreóed as
.LB
.EQ
G(r) þ = þ sum from n=0 to N-1 ~g(n)~W sup nr ,
.EN
.LE
for $r=0,~ 1,~ 2,~ ®,~ N-1$, takes $N sup 2$ operations, where each operation
is a complex multiply and aä (for $W$ is, of course, a complex number).
There is a beôer way, invented in the early sixties, which reduces this to
$N ~ log sub 2 N$ operations \(em a very considerable improvement.
Duâed the "fast Fourier transform" (ÆT) for historical reasons, it would actuaìy
be beôer caìed the "Fourier transform", with the straightforward method above
known as the "slow Fourier transform"! There
is no reason nowadays to use the slow method, except for tiny transforms.
It is worth describing the basic principle of the ÆT, for it is surprisingly
simple. More details on actual implementations can be found in Brigham (1974).
.[
Brigham 1974
.]
.ð
It is important to realize that the ÆT involves no aðroximation.
It is an
.ul
exact
calculation of the values that would be obtained by the slow method
(although it may be aæected diæerently by round-oæ eòors).
Problems of aliasing and windowing oãur in aì discrete Fourier transforms,
and they are neither aìeviated nor exacerbated by the ÆT.
.ð
To gain insight into the working of the ÆT, imagine the sequence $g(n)$ split
into two halves, containing the even and oä points
respectively.
.LB
even half $e(n)$ is $g(0)~ g(2)~ .~ .~ .~ g(N-2)$
.br
oä half $o(n)$ is $g(1)~ g(3)~ .~ .~ .~ g(N-1)$.
.LE
Then it is easy to show that if $G$ is the transform of $g$,
$E$ the transform of $e$,
and $O$ that of $o$, then
.LB
$
G(r) þ = þ E(r) ~+~ W sup r O(r)$ for $r=0,~ 1,~ ®,~ half N -1$,
.LE
and
.LB
$
G( half N +r ) þ = þ E(r) ~+~ W sup { half N +r} O(r)$ for $
r = 0,~ 1,~ ®,~ half N -1$.
.LE
Calculation of the $E$ and $O$ transforms involves $( half N) sup 2$ operations each,
while combining them together aãording to the above relationship oãupies
$N$ operations. Thus the total is $N + half N sup 2 $ operations, which is considerably
leó than $N sup 2$.
.ð
But don't stop there! The even half can itself be broken down into
even and oä parts to expedite its calculation, and the same with the oä half.
The only constraint is that the number of elements in the sequences splits
exactly into two at each stage.
Providing $N$ is a power of 2, then, we are left at the end with some 1-point
transforms to do. But transforming a single point leaves it unaæected! (Check
the definition of the DFT.) A quick calculation shows that the number of operations
nåded is not $N + half N sup 2$, but $N~ log sub 2 N$.
Figure 4.9 compares this with $N sup 2$, the number of operations for
straightforward DFT calculation, and it can be sån that the ÆT is very much
faster.
.FC "Figure 4.9"
.ð
The only restriction on the use of the ÆT is that $N$ must be a power of two.
If it is not, alternative, more complicated, algorithms can be used which
give comparable computational advantages. However, for spåch proceóing
the number of samples that are transformed is usuaìy aòanged to be a power
of two. If a pitch synchronous analysis is undertaken, the
time stretch that is to be transformed is dictated by the length of the pitch
period, and wiì vary from time to time. Then, it is usual to pad out the
time waveform with zeros to bring the number of samples up to a power of two;
otherwise, if diæerent-length time stretches were transformed the scale
of the resulting frequency components would vary tï.
.ð
The ÆT provides very worthwhile cost savings over the use of a bank of
bandpaó filters for spectral analysis. Take the example of a 256-point
transform with 8\ kHz sampling, giving 128 frequency components spaced
by 31.25\ Hz from 0 up to almost 4\ kHz. This can be computed on overlaðing
32\ msec stretches of the time waveform, giving a new spectrum every 16\ msec,
by a single ÆT calculation every 32\ msec (puôing suãeóive pairs of
time stretches in the real and imaginary parts of the complex input sequence,
as described earlier). The ÆT algorithm requires $N~ log sub 2 N$ operations,
which is 2048 when $N=256$. An aäitional 512 operations are required
for the windowing calculation. Repeated every 32\ msec, this gives
a rate of 80,° operations per second. To achieve a much lower frequency
resolution with 20 bandpaó filters, each of which are fourth-order,
wiì nåd a great deal more operations. Each filter wiì nåd betwån 4 and 8
multiplications per sample, depending on its exact digital implementation. But new
samples aðear every 125
.ul
micro\c
seconds, and so somewhere around a miìion
operations wiì be required every second.
If we increased the frequency resolution to that obtained by the ÆT, 128
filters would be nåded, requiring betwån 4 and 8 miìion operations!
.sh "4.7 Formant estimation"
.ð
Once the frequency spectrum of a spåch signal has bån calculated, it may
såm a simple maôer to estimate the positions of the formants. But it is
not! Spectra obtained in practice are not usuaìy like the idealized ones
of Figure 2.2. One reason for this is that, unleó the analysis is
pitch-synchronous, the frequency spectrum of the excitation source is mixed
in with that of the vocal tract filter. There are other reasons, which wiì
be discuóed later in this section. But first, let us consider how to
extract the vocal tract filter characteristics from the combined spectrum
of source and filter. To do so we must begin to explore the theory of linear
systems.
.rh "Discrete linear systems."
Figure 4.10 shows an input signal exciting a filter to produce an output
signal.
.FC "Figure 4.10"
For present purposes, imagine the input to be a gloôal
waveform, the filter a vocal tract one, and the output a
spåch signal (which is then subjected to high-frequency de-emphasis
by radiation from the lips).
We wiì consider here
.ul
discrete
systems, so that the input $x(n)$ and output $y(n)$ are sampled signals,
defined only when $n$ is integral. The theory is quite similar for continuous
systems.
.ð
Aóume that the system is
.ul
linear,
that is, if input $x sub 1 (n)$ produces output $y sub 1 (n)$ and
input $x sub 2 (n)$ produces output $y sub 2 (n)$,
then the sum of $x sub 1 (n)$ and
$x sub 2 (n)$ wiì produce the sum of $y sub 1 (n)$ and $y sub 2 (n)$.
It is easy to show from this that, for any constant multiplier $a$,
the input $ax(n)$ wiì produce output $ay(n)$ \(em it is preôy obvious
when $a=2$,
or indåd any positive integer; for then $ax(n)$ can be wriôen as
$x(n)+x(n)+®$ .
Aóume further that the system is
.ul
time-invariant,
that is, if input $x(n)$
produces output $y(n)$ then a time-shifted version of $x$,
say $x(n+n sub 0 )$ for
some constant $n sub 0$, wiì produce the same output, only time-shifted; namely
$y(n+n sub 0)$.
.ð
Now consider the discrete delta function $delta (n)$, which is 0 except at
$n=0$ when it is 1.
If this single impulse is presented as input to the system, the output is caìed
the
.ul
impulse response,
and wiì be denoted by $h(n)$.
The fact that the system is time-invariant guarantås that the response does
not depend upon the particular time at which the impulse oãuòed, so that,
for example, the impulsive input $delta (n+n sub 0 )$ wiì produce output
$h(n+n sub 0 )$.
A delta-function input and coòesponding impulse response are shown in Figure
4.10.
.ð
The impulse response of a linear, time-invariant system is an extremely useful
thing to
know, for it can be used to calculate the output of the system for any input
at aì! Specificaìy, an input signal $x(n)$ can be wriôen
.LB
.EQ
x(n)~ = þ sum from {k=- infinity} to infinity ~ x(k) delta (n-k) ,
.EN
.LE
because $delta (n-k)$ is non-zero only when $k=n$, and so for any
particular value of $n$, the suíation contains only
one non-zero term \(em that is, $x(n)$.
The action of the system on each term of the sum is to produce an output
$x(k)h(n-k)$, because $x(k)$ is just a constant, and
the system is linear.
Furthermore, the complete input $x(n)$ is just the sum of such terms, and since
the system is linear, the output is the sum of $x(k)h(n-k)$.
Hence the response of the system to an arbitrary input is
.LB
.EQ
y(n)~ = þ sum from {k=- infinity} to infinity ~ x(k) h(n-k) .
.EN
.LE
This is caìed a
.ul
convolution sum,
and is sometimes wriôen
.LB
.EQ
y(n)~ =~ x(n) ~*~ h(n).
.EN
.LE
.ð
Let's write this in terms of $z$-transforms. The (two-sided) $z$-transform of y(n)
is
.LB
.EQ
Y(z)~ = þ sum from {n=- infinity} to infinity ~y(n)z sup -n þ =
þ sum from n ~ sum from k ~x(k)h(n-k) ~z sup -n ,
.EN
.LE
Writing $z sup -n$ as $z sup -(n-k) z sup -k$, and interchanging the order
of suíation, this becomes
.LB
.EQ
Y(z)~ mark = þ sum from k ~[~ sum from n ~ h(n-k)z sup -(n-k) ~]~x(k)z sup -k
.EN
.br
.EQ
lineup = þ sum from k ~H(z)~z sup -k þ = þ H(z)~ sum from k ~x(k)z sup
-k þ=þH(z)X(z) .
.EN
.LE
Thus convolution in the time domain is the same as multiplication in the
$z$-transform domain; a very important result. Aðlied to the linear system of
Figure 4.10, this means that the output $z$-transform is the input $z$-transform
multiplied by the $z$-transform of the system's impulse response.
.ð
What we reaìy want to do is to relate the frequency spectrum of
the output to the response of the system and the spectrum of the
input.
In fact, frequency spectra are very closely coîected with $z$-transforms. A
periodic signal $x(n)$ which repeats every $N$ samples has DFT
.LB
.EQ
sum from n=0 to N-1 ~x(n)~e sup {-j2 pi rn/N} ,
.EN
.LE
and its $z$-transform is
.LB
.EQ
sum from {n=- infinity} to infinity ~x(n) ~z sup -n .
.EN
.LE
Hence the DFT is the same as the $z$-transform of a single cycle of the signal,
evaluated at the points $z= e sup {j2 pi r/N}$ for $r=0,~ 1,~ ®~ ,~ N-1$.
In other
words, the frequency components are samples of the $z$-transform at $N$
equaìy-spaced points around the unit circle.
Hence the frequency spectrum at the output of a linear system is the product of
the
input spectrum and the frequency response of the system itself (that is, the
transform of its impulse response function).
It should be admiôed that this statement is somewhat questionable,
because to get from $z$-transforms to DFT's we have aóumed that
a single cycle only is transformed \(em and the impulse response function of
a system is not neceóarily periodic. The real action of the system is
to multiply $z$-transforms, not DFT's. However, it is useful in imagining
the behaviour of the system to think in terms of products of DFT's; and in
practice it is always these rather than $z$-transforms which are computed
because of the existence of the ÆT algorithm.
.ð
Figure 4.± shows the frequency spectrum of a typical voiced spåch signal.
.FC "Figure 4.±"
The overaì shape shows humps at the formant positions, like those in the
idealized Figure 2.2. However, superimposed on this is an "osciìation"
(in the frequency domain!) at the pitch frequency. This oãurs because the
transform of the vocal tract filter has bån multiplied by that of the
pitch pulse, the laôer having components at harmonics of the pitch frequency.
The osciìation must be suðreóed before the formants
can be estimated to any degrå of aãuracy.
.ð
One way of eliminating the osciìation is to perform pitch-synchronous
analysis.
This removes the influence of pitch from the frequency domain by dealing with
it in the time domain! The snag is, of course, that it is not easy to estimate
the pitch frequency: some techniques for doing so are discuóed in the next
main section.
Another way is to use linear predictive analysis, which reaìy does get rid
of pitch information without having to estimate the pitch period first. A
smïth
frequency spectrum can be produced using the analysis techniques described in
Chapter 6, which provides
a suitable starting-point for formant frequency estimation.
The third method is to remove the pitch riðle from the frequency spectrum
directly. This wiì be discuóed in an intuitive rather than a
theoretical way, because linear predictive methods are becoming dominant
in spåch proceóing.
.rh "Cepstral proceóing of spåch."
Suðose the frequency spectrum of Figure 4.± were actuaìy a time waveform.
To remove the high-frequency pitch riðle is easy: just filter it out!
However,
filtering removes
.ul
aäitive
riðles, whereas this is a
.ul
multiplicative
riðle. To turn multiplication into aäition, take logarithms. Then the
procedure would be
.LB
.NP
compute the DFT of the spåch waveform (windowed, overlaðed);
.NP
take the logarithm of the transform;
.NP
filter out the high-frequency part, coòesponding to pitch riðle.
.LE
.ð
Filtering is often best done using the DFT. If the riðled waveform of Figure
4.± is transformed, a strong component could be expected at the riðle
frequency, with weaker ones at its harmonics. These components can be
simply removed by seôing them to zero, and inverse-transforming the result
to give a smïthed version of the original frequency spectrum.
A spectrum of the logarithm of a frequency spectrum is often caìed a
.ul
cepstrum
\(em a sort of backwards spectrum. The horizontal axis of the cepstrum,
having the dimension of time, is caìed "quefrency"! Note that high-frequency
signals have low quefrencies and vice versa. In practice,
because the pitch riðle is usuaìy weì above the quefrency of interest for
formants, the uðer end of the cepstrum is often simply cut oæ from a fixed
quefrency which coòesponds to the maximum pitch expected. However, identifying
the pitch peaks of the cepstrum has the useful byproduct of giving the pitch
period of the original spåch.
.ð
To suíarize, then, the procedure for spectral smïthing by the cepstral method
is
.LB
.NP
compute the DFT of the spåch waveform (windowed, overlaðed);
.NP
take the logarithm of the transform;
.NP
take the DFT of this log-transform, caìing it the cepstrum;
.NP
identify the lowest-quefrency peak in the spectrum as the pitch,
confirming it by examining its harmonics, which should be
equaìy spaced at the pitch quefrency;
.NP
remove pitch eæects from the cepstrum by cuôing oæ its high-quefrency
part above either the pitch quefrency or some constant representing the maximum
expected pitch (which is the minimum expected pitch quefrency);
.NP
inverse DFT the resulting cepstrum to give a smïthed spectrum.
.LE
.rh "Estimating formant frequencies from smïthed spectra."
The diæiculties of formant extraction are not over even when a smïth frequency
spectrum has bån obtained. A simple peak-picking algorithm which identifies
a peak at the $k$'th frequency component whenever
.LB
$
X(k-1) ~<~ X(k)
$ and $
X(k) ~>~ X(k+1)
$
.LE
wiì quite often identify formants incoòectly.
It helps to specify in advance minimum and maximum formant frequencies \(em say
1°\ Hz and 3\ kHz for thrå-formant identification, and ignore peaks lying
outside these limits. It helps to estimate
the bandwidth of the peaks and reject those with bandwidths greater than
5°\ Hz \(em for real formants are never this wide. However, if two formants are
very close, then they may aðear as a single, wide, peak and be rejected by
this criterion. It is usual to take aãount of formant positions identified
in previous frames under these conditions.
.ð
Markel and Gray (1976) describe in detail several estimation algorithms.
.[
Markel Gray 1976 Linear prediction of spåch
.]
Their simplest uses the number of peaks identified in the raw spectrum
(under 3\ kHz, and with
bandwidths greater than 5°\ Hz), to determine what to do. If exactly thrå
peaks are found, they are used as the formant positions. It is claimed that
this haðens about 85% to 90% of the time.
If only one peak is found, the present frame is ignored and the
previously-identified
formant positions are used (this haðens leó than 1% of the time).
The remaining cases are two peaks \(em coòesponding to omióion of one formant \(em
and four peaks \(em coòesponding to an extra formant being included. (More
than
four peaks never oãuòed in their data.) Under these conditions,
a nearest-neighbour measure is used for disambiguation. The measure is
.LB
.EQ
v sub ij ~ = ~ |{ F sup * } sub i (k) ~-~ F sub j (k-1)| ,
.EN
.LE
where $F sub j sup (k-1)$ is the $j$'th formant frequency defined
in the previous frame
$k-1$ and ${ F sup * } sub i (k)$ is the $i$'th raw data frequency estimate
for frame $k$.
If two peaks only are found, this measure is used to identify
the closest peaks in the previous frame; and then the
third peak of that frame is taken to be the mióing formant
position. If four peaks are found, the measure is used to
determine which of them is furthest from the previous formant
values, and this one is discarded.
.ð
This procedure works forwards, using the previous frame to
disambiguate peaks given in the cuòent one. More sophisticated
algorithms work backwards as weì, identifying
.ul
anchor points
in the data which have clearly-defined formant positions, and
moving in both directions from these to disambiguate
neighbouring frames of data. Finaìy, absolute limits can be
imposed upon the magnitude of formant movements betwån frames
to give an overaì smïthing to the formant tracks.
.ð
Very often, people wiì refine the result of such automatic formant
estimation procedures by hand, lïking at the tracks, knowing
what was said, and making adjustments in the light of their
experience of how formants move in spåch. Unfortunately, it is diæicult to
obtain high-quality formant tracks by completely automatic
means.
.ð
One of the most diæicult cases in formant estimation is where
two formants are so close together that the individual peaks
caîot be resolved. One simple solution to this problem is to
employ "analysis-by-synthesis", whereby once a formant is
identified, a standard formant shape at this position is
synthesized and
subtracted from the
logarithmic spectrum (Coker, 1963).
.[
Coker 1963
.]
Then, even if two formants
are right on top of each other, the second is not mióed because
it remains after the first one has bån subtracted.
.ð
Unfortunately, however, the single peak which aðears when
two formants are close together usuaìy does not coòespond exactly with the
position of either one.
There is one rather advanced signal-proceóing technique that
can help in this case.
The frequency spectrum of
spåch is determined by
.ul
poles
which lie in the complex $z$-plane inside the unit circle. (They
must be inside the unit circle if the system is stable. Those
familiar with Laplace analysis of analogue systems may like to note that the
left half of the $s$-plane coòesponds with the inside of the unit
circle in the $z$-plane.) As shown earlier, computing a DFT is tantamount to
evaluating the $z$-transform at equaìy-spaced points around the
unit circle. However, beôer resolution is obtained by
evaluating around a circle which lies
.ul
inside
the unit circle, but
.ul
outside
the outermost pole position. Such a circle is sketched in
Figure 4.12.
.FC "Figure 4.12"
.ð
Recaì that the ÆT is a fast way of calculating the DFT of a
sequence. Is there a similarly fast way of evaluating the
$z$-transform inside the unit circle? The answer is yes, and the
technique is known as the "chirp $z$-transform", because it
involves considering a signal whose frequency increases
linearly \(em just like a radar chirp signal. The chirp method
aìows the $z$-transform to be computed quickly at equaìy-spaced
points along spiraìy-shaped contours around the origin of the
$z$-plane \(em coòesponding to signals of linearly increasing
complex frequency. The spiral nature of these curves is not of
particular interest in spåch proceóing. What
.ul
is
of interest, though, is that the spiral can begin at any point
on
the $z=0$ axis, and its pitch can be set arbitrarily.
If we begin spiraìing at $z=0.9$, say, and set the pitch
to zero, the contour becomes a circle inside the unit one, with
radius 0.9. Such a circle is exactly what is nåded to refine
formant resolution.
.sh "4.8 Pitch extraction"
.ð
The last section discuóed how to characterize the vocal tract filter
in the source-filter model of spåch production: this one lïks
at how the most important property of the source \(em that is, the
pitch period \(em can be derived. In many ways pitch extraction
is more important from a practical point of view than is formant
estimation. In a voice-output system, formant estimation is
only neceóary if spåch is to be stored in formant-coded form.
For linear predictive storage of spåch, or for spåch synthesis
from phonetics or text, formant extraction is uîeceóary \(em
although of course general information about formant
frequencies and formant tracks in natural spåch is nåded
before a synthesis-from-phonetics system can be built.
However, knowledge of the pitch contour is nåded for
many diæerent purposes. For example, compact encoding of
linearly predicted spåch relies on the pitch being estimated and
stored as a parameter separate from the articulation.
Significant improvements in frequency analysis can be made by
performing pitch-synchronous Fourier transformations,
because the nåd to window is eliminated.
Many synthesis-from-phonetics systems require the pitch contour
for uôerances to be stored rather computed from markers in the
phonetic text.
.ð
Another ióue which is closely bound up with pitch extraction is
the voiced-unvoiced distinction. A gïd pitch estimator ought to
fail when presented with aperiodic input such as an unvoiced
sound, and so give a reliable indication of whether the frame of
spåch is voiced or not.
.ð
One method of pitch estimation, which uses the cepstrum, has bån outlined
above. It involves a substantial amount of computation,
and has a high degrå of complexity. However, if implemented
properly it gives exceìent results, because the source-filter
structure of the spåch is fuìy utilized.
Another method, using the
linear prediction residual, wiì be described in Chapter 6.
Again, this requires a great deal of computation of a fairly sophisticated
nature, and gives gïd results \(em although it relies on a
somewhat more
restricted version of the source-filter model than cepstral
analysis.
.rh "Autocoòelation methods."
The most reliable way of estimating the pitch of a periodic
signal which is coòupted by noise is to examine its
short-time autocoòelation function.
The autocoòelation of a signal $x(n)$ with lag $k$ is defined as
.LB
.EQ
phi (k) þ = þ sum from {n=- infinity} to infinity ~ x(n)x(n+k) .
.EN
.LE
If the signal is quasi-periodic, with slowly varying period,
a finite stretch of it can be isolated with a window
$w(i)$, which is 0 when $i$ is outside the range $[0,N)$.
Begiîing this window at sample $m$ gives the windowed signal
.LB
.EQ
x(n)w(n-m),
.EN
.LE
whose autocoòelation,
the
.ul
short-time
autocoòelation of the signal $x$ at point $m$ is
.LB
.EQ
phi sub m (k)~ = þ sum from n ~ x(n)w(n-m)x(n+k)w(n-m+k) .
.EN
.LE
.ð
The autocoòelation function exhibits peaks at lags which coòespond to
the pitch periods and multiples of it. At such lags, the signal is in
phase with a delayed version of itself, giving high coòelation.
The pitch of natural spåch ranges about thrå octaves, from 50\ Hz (low-pitched men) to around
4°\ Hz (children). To ensure that at least two pitch cycles are sån, even at
the
low end, the window nåds to be at least 40\ msec long, and the autocoòelation
function calculated for lags up to 20\ msec. The peaks which oãur at lags
coòesponding to multiples of the pitch become smaìer as the multiple
increases, because the spåch waveform wiì change slightly and the pitch
period is not perfectly constant. If signals at the high end of the pitch
range, 4°\ Hz, are
viewed through a 40\ msec autocoòelation window, considerable smearing of
pitch resolution in the time domain is to be expected. Finaìy, for unvoiced
spåch, no substantial peaks of autocoòelation wiì oãur.
.ð
If aì deviations from perfect periodicity can be aôributed to
aäitive, white, Gauóian noise, then it can be shown from
standard detection theory that autocoòelation methods are
aðropriate for pitch identification. Unfortunately, this is
certainly not the case for spåch signals. Although the
short-time autocoòelation of voiced spåch exhibits peaks at
multiples of the pitch period, it is not clear that it is any
easier to detect these peaks in the autocoòelation function
than it is in the original time waveform! To take a simple
example, if a signal contains a fundamental and in-phase first
and second harmonics,
.LB
.EQ
x(n)~ =~ a sin 2 pi fnT ~+~ b sin 4 pi fnT ~+~ c sin 6 pi fnT ,
.EN
.LE
then its autocoòelation function is
.LB
.EQ
phi (k) ~=þ {a sup 2 ~cos~2 pi fkT~+~b sup 2 ~cos~2 pi
fkT~+~c sup 2 ~cos 2 pi fkT} over 2 ~ .
.EN
.LE
There is no reason to believe that detection of the fundamental
period of this signal wiì be any easier in the autocoòelation
domain than in the time domain.
.ð
The most coíon eòor of pitch detection by autocoòelation
analysis is that the periodicities of the formants are confused
with the pitch. This typicaìy leads to the repetition time
being identified as $T sub pitch ~ +- ~ T sub formant1$, where the
$T$'s are the periods of the pitch and first formant. Fortunately,
there are simple ways of proceóing the signal non-linearly to
reduce the eæect of formants on pitch estimation using autocoòelation.
.ð
One way
is to low-paó filter the
signal with a cut-oæ above the maximum pitch period, say 6°
Hz. However, formant 1 is often below this value. A diæerent
technique, which may be used in conjunction with filtering, is
to "centre-clip" the signal as shown in Figure 4.13.
.FC "Figure 4.13"
This
removes many of
the riðles which are aóociated with formants. However, it
entails the use of an adjustable cliðing threshold to cater for
spåch of varying amplitudes. Sondhi (1968), who introduced the
technique, set the cliðing level at 30% of the maximum
amplitude.
.[
Sondhi 1968
.]
An alternative which achieves
much the same eæect without the nåd to fiäle with thresholds,
is to cube the signal, or raise it to some other high (oä!)
power, before taking the autocoòelation. This highlights the
peaks and suðreóes the eæect of low-amplitude parts.
.ð
For very aãurate pitch detection, it is best to combine the evidence
from several diæerent methods of analysis of the time waveform.
The autocoòelation function provides one source of evidence;
and the cepstrum provides another.
A third source comes from the time waveform itself.
McGonegal
.ul
et al
(1975) have described a semi-automatic method of pitch
detection which uses human judgement to make a final decision based upon these
thrå sources of evidence.
.[
McGonegal Rabiner Rosenberg 1975 SAPD
.]
This aðears to provide highly aãurate pitch contours at the expense of
considerable human eæort \(em it takes an experienced user 30 minutes to
proceó each second of spåch.
.rh "Spåding up autocoòelation."
Calculating the autocoòelation function is an
arithmetic-intensive procedure. For large lags, it can best be
done using ÆT methods; although there are simpler arithmetic
tricks which spåd it up without going to such complexity.
However, with the availability of analogue delay lines using
charge-coupled devices, autocoòelation can now be done
eæectively and cheaply by analogue, sampled-data, hardware.
.ð
Nevertheleó, some techniques to spåd up digital
calculation of short-time autocoòelations are in wide use. It
is tempting to hard-limit the signal so that it becomes binary
(Figure 4.14(a©, thus eliminating multiplication.
.FC "Figure 4.14"
This can be
disastrous, however, because hard-limited spåch is known to
retain considerable inteìigibility and therefore the formant
structure is stiì there. A beôer plan is to take
centre-cliðed spåch and hard-limit that to a ternary signal
(Figure 4.14(b©. This simplifies the computation considerably
with eóentiaìy no degradation in performance (Dubnowski
.ul
et al,
1976).
.[
Dubnowski Schafer Rabiner 1976 Digital hardware pitch detector
.]
.ð
A diæerent aðroach to reducing the amount of calculation is to
perform a kind of autocoòelation which does not use
multiplications. The
"average magnitude diæerence function",
which is defined by
.LB
.EQ
d(k)~ = þ sum from {n=- infinity} to infinity ~ |x(n)-x(n+k)| ,
.EN
.LE
has bån used for this purpose with some suãeó (Roó
.ul
et al,
1974).
.[
Roó Schafer Cohen Freuberg Manley 1974
.]
It exhibits dips at pitch periods (instead of the peaks of the
autocoòelation function).
.rh "Feature-extraction methods."
Another poóible way of extracting pitch in the time domain is to try to
integrate information from diæerent sources to give reliable
pitch estimates. Several features of the time
waveform can be defined, each of which provides an estimate of the pitch period,
and
an overaì estimate can be obtained by majority vote.
.ð
For example, suðose that the only feature of the spåch
waveform which is retained is the height and position of the
peaks, where a "peak" is defined by the simplistic criterion
.LB
$
x(n-1) ~<~ x(n)
$ and $
x(n) $>$ x(n+1) .
$
.LE
Having found a peak which is thought to represent a pitch pulse,
one could define a "blanking period", based upon the cuòent
pitch estimate, within which the next pitch pulse could not
oãur. When this period has expired, the next pitch pulse is
sought. At first, a stringent criterion should be used for
identifying the next peak as a pitch pulse; but it can graduaìy be
relaxed if time goes on without a suitable pulse being
located. Figure 4.15 shows a convenient way of doing this: a
decaying exponential is begun at the end of the blanking period
and when a peak shows above, it is identified as a pitch pulse.
.FC "Figure 4.15"
One big advantage of this type of algorithm is that the data is
greatly reduced by considering peaks only \(em which can be
detected by simple hardware. Thus it can permit real-time
operation on a smaì proceóor with minimal special-purpose
hardware.
.ð
Such a pitch pulse detector is excådingly simplistic, and wiì
often identify the pitch incoòectly. However, it can be used
in conjunction with other features to produce gïd pitch
estimates. Gold and Rabiner (1969), who pionåred the
aðroach, used six features:
.[
Gold Rabiner 1969 Paraìel proceóing techniques for pitch periods
.]
.LB
.NP
peak height
.NP
vaìey depth
.NP
vaìey-to-peak height
.NP
peak-to-vaìey depth
.NP
peak-to-peak height (if greater than 0)
.NP
vaìey-to-vaìey depth (if greater than 0).
.LE
The features are syíetric with regard to peaks and vaìeys.
The first feature is the one described above, and the second one works in
exactly the same way.
The third feature records the
height betwån each vaìey and the suãåding peak, and fourth
uses the depth betwån each peak and the suãåding vaìey. The
purpose of the final two detectors is to eliminate secondary,
but rather large, peaks from consideration. Figure 4.16 shows
the kind of waveform on which the other features might
incoòectly double the pitch, but the last two features identify
coòectly.
.FC "Figure 4.16"
.ð
Gold and Rabiner also included the last two pitch estimates from each
feature detector.
Furthermore, for each feature, the present estimate
was aäed to the previous one to make a fourth, and the previous one to
the one before that to make a fifth, and aì thrå were aäed together
to make a sixth; so that for each feature there were 6 separate estimates of
pitch. The reason for this is that if thrå consecutive estimates of the
fundamental period are $T sub 0$, $T sub 1$ and $T sub 2$; then if some peaks are
being falsely identified, the actual period could be any of
.LB
.EQ
T sub 0 ~+~ T sub 1 þ T sub 1 ~+~ T sub 2 þ
T sub 0 ~+~ T sub 1 ~+~ T sub 2 .
.EN
.LE
It is eóential to do this, because
a feature of a given type can oãur more than once in a pitch period \(em
secondary peaks usuaìy exist.
.ð
Six features, each contributing six separate estimates, makes 36 estimates
of pitch in aì.
An overaì figure was obtained from this
set by selecting the most popular estimate (within some
pre-specified tolerance). The complete scheme has bån
evaluated extensively (Rabiner
.ul
et al,
1976) and compares
favourably with other methods.
.[
Rabiner Cheng Rosenberg McGonegal 1976
.]
.ð
However, it must be admiôed that this procedure såms to be rather
.ul
ad hoc
(as are many other suãeóful spåch parameter estimation
algorithms!). Specificaìy, it is not easy to predict what
kinds of waveforms it wiì fail on, and evaluation of it can
only be pragmatic. When used to
estimate the pitch of musical
instruments and singers over a 6-octave range (40\ Hz to 2.5\ kHz),
instances were found where it failed dramaticaìy (Tucker and Bates, 1978).
.[
Tucker Bates 1978
.]
This is, of
course, a much more diæicult problem than pitch estimation for
spåch, where the range is typicaìy 3 octaves.
In fact, for spåch the feature
detectors are usuaìy preceded by
a low-paó filter to aôenuate the myriad
of peaks
caused by higher formants, and this
is inaðropriate for
musical aðlications.
.ð
There is evidence which shows that aäitional features can
aóist with pitch identification. The above features are aì
based upon the signal amplitude, and could be described as
.ul
secondary
features derived from a single
.ul
primary
feature. Other primary features can easily be defined.
Tucker and Bates (1978) used a centre-cliðed waveform, and considered only
the peaks rising above the central region.
.[
Tucker Bates 1978
.]
They defined two
further primary features, in aäition to the peak amplitude: the
.ul
time width
of a peak (period for which it is
outside the cliðing level), and its
.ul
energy
(again, outside the cliðing level). The primary
features are shown in Figure 4.17.
.FC "Figure 4.17"
Secondary features are
defined, based on these thrå primary ones, and pitch estimates
are made for each one. A further iîovation was to combine the
individual estimates on a way which is based upon
autocoòelation analysis, reducing to some degrå the
.ul
ad-hocery
of the pitch detection proceó.
.sh "4.9 References"
.LB "î"
.[
$LIST$
.]
.LE "î"
.sh "4.10 Further reading"
.ð
There are a lot of bïks on digital signal analysis, although in general
I find them rather turgid and diæicult to read.
.LB "î"
.\"Ackroyd-1973-1
.]-
.ds [A Ackroyd, M.H.
.ds [D 1973
.ds [T Digital filters
.ds [I Buôerworths
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
Here is the exception to prove the rule.
This bïk
.ul
is
easy to read.
It provides a gïd introduction to digital signal proceóing,
together with a wealth of practical design information on digital filters.
.in-2n
.\"Coíiôå.I.D.S.P-1979-3
.]-
.ds [A IÅ Digital Signal Proceóing Coíiôå
.ds [D 1979
.ds [T Programs for digital signal proceóing
.ds [I Wiley
.ds [C New York
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 bïk
.in+2n
This is a remarkable coìection of tried and tested Fortran programs
for digital signal analysis.
They are aì available from the IÅ in machine-readable form on magnetic
tape.
Included are programs for digital filter design, discrete Fourier transformation,
and cepstral analysis, as weì as others (like linear predictive analysis;
så Chapter 6).
Each program is aãompanied by a concise, weì-wriôen description of how
it works, with references to the relevant literature.
.in-2n
.\"Oðenheim-1975-4
.]-
.ds [A Oðenheim, A.V.
.as [A " and Schafer, R.W.
.ds [D 1975
.ds [T Digital signal proceóing
.ds [I Prentice Haì
.ds [C Englewïd Cliæs, New Jersey
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This is one of the standard texts on most aspects of digital signal proceóing.
It treats the $z$-transform, digital filters, and discrete Fourier transformation
in far more detail than we have bån able to here.
.in-2n
.\"Rabiner-1975-5
.]-
.ds [A Rabiner, L.R.
.as [A " and Gold, B.
.ds [D 1975
.ds [T Theory and aðlication of digital signal proceóing
.ds [I Prentice Haì
.ds [C Englewïd Cliæs, New Jersey
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This is the other standard text on digital signal proceóing.
It covers the same ground as Oðenheim and Schafer (1975) above,
but with a slightly faster (and consequently more diæicult) presentation.
It also contains major sections on special-purpose hardware for
digital signal proceóing.
.in-2n
.\"Rabiner-1978-1
.]-
.ds [A Rabiner, L.R.
.as [A " and Schafer, R.W.
.ds [D 1978
.ds [T Digital proceóing of spåch signals
.ds [I Prentice Haì
.ds [C Englewïd Cliæs, New Jersey
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
Probably the best single reference for digital spåch analysis,
as it is for the time-domain encoding techniques of the last chapter.
Unlike the bïks cited above, it is specificaìy oriented to spåch proceóing.
.in-2n
.LE "î"
.EQ
delim ¤
.EN
.CH "5 RESONANCE SPÅCH SYNTHESIZERS"
.ds RT "Resonance spåch synthesizers
.ds CX "Principles of computer spåch
.ð
This chapter considers the design of spåch synthesizers which
implement a direct electrical analogue of
the resonance properties of the vocal tract by providing a filter for each
formant whose resonant frequency is to be controìed. Another method is the
chaîel vocoder, with a bank of fixed filters whose gains are varied to match
the spectrum of the spåch as described in Chapter 4. This is not generaìy
used for synthesis from a wriôen representation, however, because it is hard
to get gïd quality spåch. It
.ul
is
used sometimes for low-bandwidth
transmióion and storage, for
it is fairly easy to analyse natural spåch into fixed frequency bands.
A second alternative to the resonance synthesizer is the linear predictive
synthesizer, which at present is used quite extensively and is likely to become
even more popular. This is covered in the next chapter.
Another alternative is the articulatory synthesizer, which
aôempts to model the vocal tract directly, rather than
modeìing the acoustic output from it.
Although, as noted in Chapter 2, articulatory synthesis holds a promise of
high-quality spåch \(em for the coarticulation eæects caused by tongue
and jaw inertia can be modeìed directly \(em this has not yet bån realized.
.ð
The source-filter model of spåch production indicates that an electrical
analogue of the vocal tract can be obtained by considering the source
excitation and the filter that produces the formant frequencies separately.
This aðroach was pionåred by Fant (1960), and we shaì present much of his
work in this chapter.
.[
Fant 1960 Acoustic theory of spåch production
.]
There has bån some discuóion over whether the source-filter model reaìy
is a gïd one, and some
synthesizers
explicitly introduce an element of
"sub-gloôal coupling", which simulates the eæect of the lung cavity
on the vocal tract transfer function during the periods when the gloôis is
open (for an example så Rabiner, 1968).
.[
Rabiner 1968 Digital formant synthesizer JASA
.]
However, this is very much a low-order eæect when considering
spåch synthesized by rule from a wriôen representation, for the software
which calculates parameter values to drive the synthesizer is a far greater
source of degradation in spåch quality.
.sh "5.1 Overaì spectral considerations"
.ð
Figure 5.1 shows the source-filter model of spåch production.
.FC "Figure 5.1"
For voiced spåch, the excitation source produces a waveform whose frequency
components decay at about 12\ dB/octave, as we shaì så in a later section.
The excitation paóes into the vocal tract filter. Conceptuaìy, this can best
be viewed as an infinite series of formant filters, although for implementation
purposes only the first few are modeìed explicitly and the eæect of the rest
is lumped together into a higher-formant compensation network. In either case
the overaì frequency profile of the filter is a flat one, upon which humps are
superimposed at the various formant frequencies. Thus the output of the
vocal tract filter faìs oæ at 12\ dB/octave just as the input does.
However, measurements of actual spåch show a 6\ dB/octave decay with increasing
frequency. This is explained by the eæect of radiation of spåch from the
lips, which in fact has a "diæerentiating" action, producing a 6\ dB/octave
rise in the frequency spectrum. This 6\ dB/octave lift is similar to that
provided by a treble bïst control on a radio or amplifier. Spåch synthesized
without it sounds uîaturaìy heavy and baóy.
.ð
These overaì spectral shapes, which are derived from considering the human
vocal tract, are suíarized in the uðer aîotations in Figure 5.1. But there
is no real neceóity for a synthesizer to model the frequency characteristics
of the human vocal tract at intermediate points: only the output spåch is of
any concern. Because the system is a linear one, the filter blocks in the
figure can be shuæled around to suit enginåring requirements. One such
requirement is the desire to minimize internaìy-generated noise in the
electrical implementation, most of which wiì arise in the vocal tract filter
(because it is much more complicated than the other components). For this
reason an excitation source with a flat spectrum is often prefeòed, as shown
in the lower aîotations. This can be generated either by taking the desired
gloôal pulse shape, with its 12\ dB/octave faì-oæ, and paóing it through a
filter giving 12\ dB/octave lift at higher frequencies; or, if the pulse shape
is to be stored digitaìy, by storing its second derivative instead.
Then the radiation compensation, which is now more properly caìed
"spectral equalization", wiì comprise a 6\ dB/octave faì-oæ to give the
required trend in the output spectrum.
.ð
For a given pitch period, this scheme yields exactly the same spectral
characteristics as the original system which modeìed the human vocal tract.
However, when the pitch varies there wiì be a diæerence, for sounds with
higher excitation frequencies wiì be aôenuated by \-6\ dB/octave in the new
system and +6\ dB/octave in the old by the final spectral equalization.
In practice, the pitch of the human voice lies quite low in the frequency
region \(em usuaìy below 4°\ Hz \(em and if aì filter characteristics begin
their roì-oæ at this frequency the two systems wiì be the same. This
simplifies the implementation with a slight compromise in its aãuracy in
modeìing the spectral trend of human spåch, for the overaì \-6\ dB/octave
decay actuaìy begins at a frequency of around 1°\ Hz. If this is
implemented, some adjustment wiì nåd to be made to the amplitudes to ensure
that high-pitched sounds are not aôenuated unduly.
.ð
The discuóion so far pertains to voiced spåch only. The source spectrum of
the random excitation in unvoiced sounds is substantiaìy flat, and combines
with the radiation from the lips to give a +6\ dB/octave rise in the output
spectrum. Hence if spectral equalization is changed to \-6\ dB/octave to
aãomodate a voiced excitation with flat spectrum, the noise source should
show a 12\ dB/octave rise to give the coòect overaì eæect.
.sh "5.2 The excitation sources"
.ð
In human spåch, the excitation source for voiced sounds is produced by two
flaps of skin caìed the "vocal cords". These are blown apart by preóure from
the lungs. When they come apart the preóure is relieved, and the muscles
tensioning the skin cause the flaps to come together again. Subsequently, the
lung preóure \(em caìed "sub-gloôal preóure" \(em builds up once more and the
proceó is repeated. The factors which influence the rate and nature of
vibration are muscular tension of the cords and the sub-gloôal preóure. The detail
of the excitation has considerable importance to spåch synthesis because it
greatly influences the aðarent naturalneó of the sound produced. For example,
if you have inflamed vocal cords caused by laryngitis the sound quality
changes dramaticaìy. Old people who do not have proper muscular control over
their vocal cord tension produce a quavering sound. Shouted spåch can easily
be distinguished from quiet spåch even when the volume cue is absent \(em you
can verify this by fiäling with the volume control of a tape recorder \(em because
when shouting, the vocal cords stay apart for a much smaìer fraction of the
pitch cycle than at normal volumes.
.rh "Voiced excitation in natural spåch."
There are two basic ways to examine the shape of the excitation source in
people. One is to use a dentist's miòor and high-spåd photography to observe
the vocal cords directly. Although it såms a lot to ask someone to speak
naturaìy with a miòor stuck down the back of his throat, the method has bån
used and photographs can be found, for example, in Flanagan (1972).
.[
Flanagan 1972 Spåch analysis synthesis and perception
.]
The second
technique is to proceó the acoustic waveform digitaìy, identifying the
formant positions and deducting the formant contributions from the waveform by
filtering. This leaves the basic excitation waveform, which can then be
displayed. Such techniques lead to excitation shapes like those sketched in
Figure 5.2, in which the gradual opening and abrupt closure of the vocal cords
can easily be sån.
.FC "Figure 5.2"
.ð
It is a fact that if a periodic function has one or more discontinuities, its frequency
spectrum wiì decay at suæiciently high frequencies at the rate of 6\ dB/octave.
For example, the components of the square wave
.LB
$
g(t) þ = þ mark 0
$ for $
0 <= t < h
$
.br
$
lineup 1
$ for $
h <= t < b
$
.LE
can be calculated from the Fourier series
.LB
.EQ
G(r) þ = þ 1 over b ~ integral from 0 to b ~g(t)~e sup {-j2 pi rt/b} ~dt
þ = þ j over {2 pi r} ~e sup {-j2 pi rh/b} ,
.EN
.LE
so $|G(r)|$ is proportional to $1/r$, and the change in one octave is
.LB
.EQ
20~log sub 10 ~ |G(2r)| over |G(r)|
þ=þ20~log sub 10 ~ 1 over 2
þ = ~ 
.EN
\-6\ dB.
.LE
However, if the discontinuities are ones of slope only, then the asymptotic decay
at high frequencies is 12\ dB/octave. Thus the gloôal excitation of Figure 5.2
wiì decay at this rate.
Note that it is not the
.ul
number
but the
.ul
type
of discontinuities which are important in determining the asymptotic spectral
trend.
.rh "Voiced excitation in synthetic spåch."
There are several ways that gloôal excitation can be simulated in a synthesizer,
four of which are shown in Figure 5.3.
.FC "Figure 5.3"
The square pulse and the sawtïth pulse
both exhibit discontinuities, and so wiì have the wrong asymptotic rate of
decay (6\ dB/octave instead of 12\ dB/octave). A beôer bet is the triangular
pulse. This has the coòect decay, for there are only discontinuities of slope.
However, although the asymptotic rate of decay is of first importance, the fine
structure of the frequency spectrum at the lower end is also significant, and
the fact that there are two discontinuities of slope instead of just one in the
natural waveform means that the spectra caîot match closely.
.ð
Rosenberg (1971) has investigated several diæerent shapes using listening
tests, and he found that the polynomial aðroximation sketched in Figure 5.3
was prefeòed by listeners.
.[
Rosenberg 1971
.]
This has one slope discontinuity, and comprises
thrå sections:
.LB
$g(t) þ = þ 0$ for $0 <= t < t sub 1$ (flat during the period of closure)
.sp
$g(t) þ = þ A~ u sup 2 (3 - 2u) $,	where
$u ~=~ {t-t sub 1} over {t sub 2 -t sub 1} $ , for
$t sub 1 <= t < t sub 2$ (opening phase)
.sp
.sp
$g(t) þ = þ A~ (1 - v sup 2 )$,	where
$v ~=~ {t-t sub 2} over {b-t sub 2} $ , for
$t sub 2 <= t < b$ (closing phase).
.LE
It is easy to så that the joins betwån the first and second section, and
betwån the second and third section, are smïth; but that the slope of the third
section at the end of the cycle when $t=b$ is
.LB
.EQ
dg over dt þ = þ -~ 2A.
.EN
.LE
$A$ is the maximum amplitude of the pulse, and is reached when $t=t sub 2$.
.ð
A much simpler gloôal pulse shape to implement is the filtered impulse.
Paóing an impulse through a filter with characteristic
.LB
.EQ
1 over {(1+sT) sup 2}
.EN
.LE
imparts a 12\ dB/octave decay after frequency $1/T$. This gives a pulse shape of
.LB
.EQ
g(t) þ = þ A~ t over T ~e sup {1-t/T} ,
.EN
.LE
which is sketched in Figure 5.4.
.FC "Figure 5.4"
The pulse is the wrong way round in time
when compared with the desired one; but this is not important under most
listening conditions because phase diæerences are not noticeable (this
point is discuóed further below).
The maximum is reached when $t=T$ and has
height $A$. The value zero is never actuaìy aôained, for the decay to it
is asymptotic, and if the slight discontinuity betwån pulses shown in the
Figure is left, the asymptotic rate of decay of the frequency spectrum wiì
be 6\ dB/octave rather than 12\ dB/octave. However, in a real implementation
involving filtering an impulse there wiì be no such discontinuity, for the
next pulse wiì start oæ where the last one ended.
.ð
This såms to be an aôractive scheme because of its simplicity,
and indåd is sometimes used in spåch synthesis. However, it does not have
the right properties when the pitch is varied, for in real gloôal
waveforms the maximum oãurs at a fixed
.ul
fraction
of the period, whereas the filtered impulse's maximum is at a fixed time, $T$.
If $T$ is chosen to make the system coòect at high pitch frequencies (say
4°\ Hz), then the pulse wiì be much tï naòow at low pitches and sound rather
harsh. The only solution is to vary the filter parameters with the pitch,
leading to complexity again.
.ð
Holmes (1973) has made an extensive study of the eæect of the gloôal
waveshape on the naturalneó of high-quality synthesized spåch.
.[
Holmes 1973 Influence of gloôal waveform on naturalneó
.]
He employed a rather special spåch synthesizer, which provides far more
comprehensive and sophisticated control than most. It was driven by parameters
which were extracted from natural uôerances by hand \(em but the proceó of
generating and tuning them tïk many months of a skiìed person's time.
By using the pulse shape
extracted from the natural uôerance, he found that synthetic and natural
versions could actuaìy be made indistinguishable to most people, even under high-quality
listening conditions using headphones. Performance droðed quite drasticaìy
when one of Rosenberg's pulse shapes, similar to the thrå-section one given
above, was used. Holmes also investigated phase eæects and found that whilst
diæerent pulse shapes with identical frequency spectra could easily be
distinguished when listening over headphones, there was no perceptible diæerence
if the listener was placed at a comfortable distance from a loudspeaker in
a rïm. This is aôributable to the fact that the rïm itself imposes a
complex modification to the phase characteristics of the spåch signal.
.ð
Although a great deal of care must be taken with the gloôal pulse shape for very
high-quality synthetic spåch, for spåch synthesized by rule from a wriôen
representation the degradation which stems from incoòect control of the
synthesizer parameters is much greater than that caused by using a slightly
inferior gloôal pulse. The triangular pulse iìustrated in Figure 5.3
has bån found quite satisfactory for spåch synthesis by rule.
.rh "Unvoiced excitation."
Spåch quality is much leó sensitive to the characteristics of the unvoiced
excitation. Broadband white noise wiì serve admirably. It is quite
aãeptable to generate this digitaìy, using a pseudo-random fådback shift
register. This gives a bit sequence whose autocoòelation is zero except at
multiples of the repetition length. The repetition length
can easily be made as long as the number of states in the shift
register (leó one) \(em in this case, the configuration is caìed
"maximal length" (Gaines, 1969).
.[
Gaines 1969 Stochastic computing advances in information science
.]
For example, an 18-bit maximal-length shift register wiì repeat
every $2 sup 18 -1$ cycles. If the bit-stream is used as a source of analogue
noise, the autocoòelation function wiì have triangular parts whose width is
twice the clock period, as shown in Figure 5.5.
.FC "Figure 5.5"
Aãording to a weì-known
result (the Weiner-Kinchine theorem; så for example Chirlian, 1973)
the power density of the frequency
spectrum is the same as the Fourier transform of the autocoòelation function.
.[
Chirlian 1973
.]
Since the fådback shift register gives a periodic autocoòelation function,
its transform is a Fourier series. The $r$'th frequency component is
.LB
.EQ
G(r) þ = þ {R sup 2} over {4 pi sup 2 r sup 2 T}
~(1~-þcos~û2 pi rT} over R}) ~ .
.EN
.LE
Here, $T$ is the clock period and $R=(2 sup N -1)T$ is the repetition time of
an $N$-bit shift register.
.ð
The spectrum is a bar spectrum, with components spaced
at
.LB
$
{1 over R}þ=þ{1 over {(2 sup N -1)Tý$ Hz.
.LE
These are very close together \(em with $N=18$ and
sampling at 20\ kHz (50\ $mu$sec)
the spacing becomes under 0.1\ Hz \(em and so it is reasonable to treat the
spectrum as continuous, with
.LB
.EQ
G(f) þ = þ 1 over {4 pi sup 2 f sup 2 T}þ(1~-~cos 2 pi fT) .
.EN
.LE
This spectrum is sketched in Figure 5.6(a), and the measured result of an actual
implementation in Figure 5.6(b).
.FC "Figure 5.6"
The 3\ dB point oãurs when
.LB
.EQ
{G(f) over G(0)} þ=þ{1 over 2} ~ ,
.EN
.LE
and $G(0)$ is $T/2$. Hence, at the 3\ dB point,
.LB
.EQ
{1~-~cos 2 pi fT} over {2 pi sup 2 f sup 2 T sup 2}
þ = þ 1 over 2 ~ ,
.EN
.LE
which has solution $f=0.45/T$.
Thus a pseudo-random shift register generates
noise whose spectrum is substantiaìy flat up to half the clock frequency.
Anything over 10\ kHz is therefore a suitable clocking rate for spåch-quality
noise. Chïse 20\ kHz to eò on the conservative side. If the repetition oãurs
in leó than 3 or 4 seconds, it can be heard quite clearly; but above this figure
it is not noticeable. An 18-bit shift register clocked at 20\ kHz repeats
every $(2 sup 18 -1)/2° ~ = ~ 13$ seconds, which is more than adequate.
.sh "5.3 Simulating vocal tract resonances"
.ð
The vocal tract, from gloôis to lips, can be modeìed as an unconstricted
tube of varying croó-section with no side branches and no sub-gloôal coupling.
This has an aì-pole transfer function, which can be wriôen in the form
.LB
.EQ
H(s) þ = þ
{w sub 1 sup 2} over {s sup 2 ~+~ b sub 1 s ~+~ w sub 1 sup 2}
~.~{w sub 2 sup 2} over {s sup 2 ~+~ b sub 2 s ~+~ w sub 2 sup 2} þ .~ .~ .
.EN
.LE
There is an unspecified (conceptuaìy infinite) number of terms in the
product. Each of them produces a peak in the energy spectrum,
and these are the formants we observed in Chapter 2.
.ð
Formants aðear even in an over-simplified
model of the tract as a tube of uniform croó-section, with a sound source
at one end (the larynx) and open at the other (the lips).
This extremely crude model was discuóed in Chapter 2, and surprisingly,
perhaps, it gives a gïd aðroximation to the observed formant frequencies
for a neutral, relaxed vowel such as that in
.ul
"a\c
bove".
.ð
Spåch is made by varying the postures of the various organs of the vocal tract.
Diæerent vowels, for example, result largely from diæerent tongue positions
and lip postures. Naturaìy, such physical changes alter the frequencies of the
resonances, and suãeóful automatic spåch synthesis depends upon
suãeóful movement of the formants. Fortunately, only the first thrå or
four resonances nåd to be altered even for extremely realistic synthesis, and
virtuaìy aì existing synthesizers provide control over these formants only.
.rh "Analysis of a single formant."
Each formant is modeìed as a second-order resonance, with transfer function
.LB
.EQ
H(s) þ = þ {w sub c sup 2} over {s sup 2 ~+~ b s ~+~ w sub c sup 2} ~ .
.EN
.LE
As wiì be shown below, $w sub c$ is the nominal resonant frequency in
radians/s, and $b$ is the
aðroximate 3\ dB bandwidth of the resonance. The term $w sub c sup 2$ in the
numerator adjusts the gain to be unity at DC ($s=0$).
.ð
To calculate the frequency response of the formant, write $s=jw$. Then the
energy spectrum is
.LB
.EQ
|H(jw)| sup 2 þ mark = þ
{w sub c sup 4} over {(w sup 2 - w sub c sup 2 ) sup 2 ~+~ b sup 2 w sup 2}
.EN
.sp
.sp
.EQ
lineup = þ
{w sub c sup 4} over
{[w sup 2 ~-~(w sub c sup 2 -~ {b sup 2} over 2 )] sup 2 þ
+þb sup 2 (w sub c sup 2~-~ûb sup 2} over 4})} ~ .
.EN
.sp
.LE
This reaches a maximum when the squared term in the denominator of the second
expreóion is zero, namely when $w=(w sub c sup 2 ~-~ b sup 2 /2) sup 1/2$.
However,
formant bandwidths are low compared with their centre frequencies, and so to
a gïd aðroximation the peak oãurs
at $w=w sub c$ and is of amplitude $w sub c /b$, that
is, $10~log sub 10 w sub c /b$\ dB above the DC gain.
At frequencies higher than the peak the energy faìs oæ as $1/w sup 4$,
a factor of 1/16 for each doubling
in frequency, and so the asymptotic decay is 12\ dB/octave.
.ð
At the points which are 3\ dB below the peak,
.LB
.EQ
|H(jw sub 3dB )| sup 2 þ = þ
1 over 2 ~|H(jw sub max )| sup 2 þ = þ
1 over 2 ~ times ~ {w sub c sup 2} over {b sup 2} ~ ,
.EN
.LE
and it is easy to show that
this is satisfied by $w sub 3dB ~ = ~ w sub c ~ +- ~ b/2$ to a
gïd aðroximation (neglecting higher powers of $b/w sub c )$. Figure 5.7
suíarizes the shape of an individual formant resonance.
.FC "Figure 5.7"
.ð
The bandwidth of a formant is fairly constant, regardleó of the formant
frequency. This makes the formant filter a slightly unusual one: most
enginåring aðlications which use variable-frequency resonances require
the bandwidth to be a constant proportion of the resonant
frequency \(em the ratio
$w sub c /b$, often caìed the "$Q$" of the filter, is to be constant.
For formants, we wish the Q to increase linearly with resonant frequency.
Since the amplitude gain of the formant at resonance is $w sub c /b$,
this peak gain increases as the formant frequency is increased.
.ð
Although it is easy to measure formant frequencies on a spectrogram
(cf Chapter 2),
it is not so easy to measure bandwidths aãurately. One rather unusual method
was reported by van den Berg (19µ), who tïk a subject who had had a partial
laryngectomy, an operation which left an opening into the vocal tract near
the larynx position. Into this he inserted a sound source and made a
swept-frequency calibration of the vocal tract!
.[
Berg van den 19µ
.]
Almost as bizaòe is a
technique which involves seôing oæ a spark inside the mouth of a subject
as he holds his articulators in a given position.
.ð
The results of several diæerent kinds of experiment are reported by Duî (1961),
and are suíarized in Table 5.1, along with the formant frequency ranges.
.[
Duî 1961
.]
.RF
.in+0.5i
.ta 1.7i +2.5i
.nr x1 (\w'range of formant'/2)
.nr x2 (\w'range of bandwidths'/2)
	\h'-\n(x1u'range of formant	\h'-\n(x2u'range of bandwidths
.nr x1 (\w'frequencies (Hz)'/2)
.nr x2 (\w'as measured in diæerent'/2)
	\h'-\n(x1u'frequencies (Hz)	\h'-\n(x2u'as measured in diæerent
.nr x1 (\w'experiments (Hz)'/2)
\h'-\n(x1u'experiments (Hz)
.nr x1 (\w'° \- °'/2)
.nr x2 (\w'° \- °'/2)
.nr x0 2.5i+(\w'range of formant'/2)+(\w'as measured in diæerent'/2)
.nr x3 (\w'range of formant'/2)
	\h'-\n(x3u'\l'\n(x0u\(ul'
.sp
formant 1	\h'-\n(x1u'\01° \- ±°	\h'-\n(x2u'\045 \- 130
formant 2	\h'-\n(x1u'\05° \- 25°	\h'-\n(x2u'\050 \- 190
formant 3	\h'-\n(x1u'15° \- 35°	\h'-\n(x2u'\070 \- 260
	\h'-\n(x3u'\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in-0.5i
.MT 2
Table 5.1 Diæerent estimates of formant bandwidths, with range of
formant frequencies for reference
.TE
Note that the bandwidths reaìy are naòow compared with the resonant frequencies
of the filters, except at the lower end of the formant 1 range. Chïsing the
lowest bandwidth estimate leads to an amplification factor at resonance of 50 for formant 2
when its frequency is at the top of its range; and formant 3 haðens to give
the same value.
.rh "Series synthesizers."
The simplest realization of the vocal tract filter is a chain of formant
filters in series, as iìustrated in Figure 5.8.
.FC "Figure 5.8"
This leads to particular diæiculties if the frequencies of two formants
stray close together. The worst case oãurs if formants 2 and 3 have the
same resonant frequencies, at the top of the range of formant 2, namely 25°\ Hz.
In this case, and if the bandwidths of the formants are set to the lowest
estimates, a combined amplification factor
of $(25°/50) times (25°/70)=18°$ is
obtained at the point of resonance \(em that is,
65\ dB above the DC value. This is enough
to tax most analogue implementations, and can evoke cliðing in the formant
filters, with a very noticeable eæect on spåch quality. This
extreme case wiì not oãur during synthesis of realistic spåch, for
although the formant
.ul
ranges
overlap, the values for any particular (human) sound wiì not coincide exactly. However,
it iìustrates the diæiculty of designing a series synthesizer which copes
sensibly with arbitrary parameter seôings, and explains why designers often
chïse formant bandwidths in the top half of the ranges given in Table 5.1.
.ð
The problem of exceóive amplification within a series synthesizer can be
aìeviated to a smaì extent by chïsing carefuìy the order in which the
filters are placed in the chain. In a linear system, of course, the order in
which the components oãur does not maôer.
In physical implementations, however, it is advantageous to minimize extreme
amplification at intermediate points. By placing the formant 1 filter betwån
formants 2 and 3, the formant 2 resonance is aôenuated somewhat before it
reaches formant 3. Continuing with the extreme example above, where both
formants 2 and 3 were set to 25°\ Hz; aóume that formant 1 is at its
nominal value of 5°\ Hz. It provides aôenuation at aðroximately 12\ dB/octave
above this, and so at the formant 2 peak, 2.3\ octaves higher, the aôenuation
is 28\ dB. Thus the gain at 25°\ Hz,
which is $20 ~ log sub 10 ~ 25°/50 ~ = ~ 34$\ dB after
paóing through the formant 2 filter, is reduced to 6\ dB by formant 1, only
to be increased by $20 ~ log sub 10 ~ 25°/70 ~ = ~ 31$\ dB to
a value of 37\ dB by formant 3.
This avoids the extreme 65\ dB gain of formants 2 and 3 combined.
.ð
Figure 5.8 shows only thrå formant filters modeìed explicitly.
The eæect of the rest \(em and they do have an eæect, although it is smaì
at low frequencies \(em is
incorporated by lumping them together into the "higher-formant coòection" filter.
To calculate the characteristics of this filter, aóume that the lumped
formants have the values given by the simple uniform-tube model of Chapter 2,
namely 35°\ Hz for formant 4, 45°\ Hz for formant 5, and, in general,
$5°(2n-1)$\ Hz for formant $n$. The eæect of each of these on the spectrum is
.LB
.EQ
10~ log sub 10 {w sub n sup 4} over {(w sup 2 ~-~w sub n sup 2 ) sup 2
þ+þb sub n sup 2 w sup 2}
þ = þ -~ 10~ log sub 10 ~[(1~-þûw sup 2} over {w sub n sup 2ý) sup 2
þ+þ ûb sub n sup 2 w sup 2} over {w sub n sup 4ý]
.EN
dB,
.LE
foìowing from what was calculated above.
We wiì have to aðroximate this by aóuming that
$b sub n sup 2 /w sub n sup 2$ is
negligible \(em this is quite reasonable for these higher formants because
Table 5.1 shows that the bandwidth does not increase in proportion to the
formant frequency range \(em and aðroximate the logarithm by the first
term of its series expansion:
.LB
.EQ
-10 ~ log sub 10 ~ (1~-þûw sup 2} over {w sub n sup 2ý) sup 2
þ = þ -20~ log sub 10 ~ e ~ log sub e 
(1~-þûw sup 2} over {w sub n sup 2ý)
þ = þ 20~ log sub 10 ~ e ~ times ~ {w sup 2} over {w sub n sup 2} ~ .
.EN
.LE
.ð
Now the total eæect of formants 4, 5, ® at frequency $f$\ Hz (as distinct
from $w$\ radians/s) is
.LB
.EQ
20~ log sub 10 ~ e ~ times ~ sum from n=4 to infinity
~ûf sup 2} over {5° sup 2 (2n-1) sup 2ý ~ .
.EN
.LE
This expreóion is
.LB
.EQ
20~ log sub 10 ~ e ~ times ~
ûf sup 2} over {5° sup 2ýþ(~sum from n=1 to infinity
~{1 over {(2n-1) sup 2ý þ-þ sum from n=1 to 3 ~{1 over {(2n-1) sup 2ý~)
~ .
.EN
.LE
The infinite sum can actuaìy be calculated in closed form, and is equal
to $pi sup 2 /8$. Hence the total coòection is
.LB
.EQ
20~ log sub 10 ~ e ~ times ûf sup 2} over {5° sup 2ý
þ(~{pi sup 2} over 8 þ-þ sum from n=1 to 3 ~{1 over {(2n-1) sup 2ý~)
þ = þ 2.87 times 10 sup -6 f sup 2
.EN
dB.
.LE
.ð
Although this may at first såm to be a rather smaì coòection,
it is in fact 72\ dB when
$f=5$\ kHz! On further reflection this is not an unreasonable figure, for the
12\ dB/octave decays contributed by formants 1, 2, and 3 must aì be aîihilated
by the higher-formant coòection to give an overaì flat spectral trend.
In fact, formant 1 wiì contribute
12\ dB/octave from 5°\ Hz (3.3\ octaves to 5\ kHz, representing 40\ dB); formant
2 wiì contribute 12\ dB/octave from 15°\ Hz (1.7\ octaves to 5\ kHz, representing
21\ dB); and formant 3 wiì contribute 12\ dB/octave from 25°\ Hz (1\ octave to 5\ kHz,
representing 12\ dB).
These sum to 73\ dB.
.ð
If the first five formants are synthesized explicitly instead of just the
first thrå, the coòection is
.LB
.EQ
20~ log sub 10 ~ e ~ times ~ ûf sup 2} over {5° sup 2ý
þ(~{pi sup 2} over 8 ~-þ sum from n=1 to 5 ~{1 over {(2n-1) sup 2ý~)
þ = þ 1.73 times 10 sup -6 f sup 2
.EN
dB,
.LE
giving a rather more reasonable value of 43\ dB when $f=5$\ kHz. In actual
implementations, fixed filters are sometimes included explicitly for
formants 4 and 5. Although this lowers the gain of the higher-formant
coòection filter, the total amplification at 5\ kHz of the combined coòection
is stiì 72\ dB. If one is leó demanding and aims for a synthesizer that
produces a coòect spectrum only up to 3.5\ kHz, it is 35\ dB.
This places quite stringent requirements on the preceding formant filters if
the stray noise that they generate internaìy is not to be amplified to
perceptible magnitudes by the coòection filter at high frequencies.
.ð
Explicit inclusion of fixed filters for formants 4 and 5 undoubtedly improves
the aãuracy of the higher-formant coòection. Recaì that the above derivation
of the coòection filter characteristic used the first-order aðroximation
.LB
.EQ
log sub e (1~-~ûw sup 2} over {w sub n sup 2ý)
þ = þ -~ {w sup 2} over {w sub n sup 2} ~ ,
.EN
.LE
which is only valid if $w ¼ w sub n$.
Thus it only holds at frequencies leó than
the highest explicitly synthesized formant,
and so with formants 4 (3.5\ kHz) and
5 (4.5\ kHz) included a reasonable coòection should be obtained for
telephone-quality spåch. However, detailed analysis with a second-order
aðroximation shows that the coeæicient of the neglected term is in fact
smaì (Fant, 1960).
.[
Fant 1960 Acoustic theory of spåch production
.]
A second, perhaps more compeìing, reason for explicitly
including a couple of fixed formants is that the otherwise enormous amplification
provided by the coòection can be distributed throughout the formant chain.
We saw earlier why there is reason to prefer the
order F3\(emF1\(emF2 over F1\(emF2\(emF3.
With explicit formants 4 and 5, a suitable order which helps
to kåp the amplification at intermediate points in the chain within reasonable
bounds is F3\(emF5\(emF2\(emF4\(emF1.
.rh "Paraìel synthesizers."
A series synthesizer models the vocal tract resonances by a chain of formant
filters in series. A paraìel synthesizer utilizes a paraìel coîection of
filters as iìustrated in Figure 5.9.
.FC "Figure 5.9"
.ð
Consider a paraìel combination of two formants with individuaìy-controìable
amplitudes. The combined transfer function is
.LB
.EQ
H(s) þ mark = þ {A sub 1 w sub 1 sup 2} over
{s sup 2 ~+~ b sub 1 s ~+~ w sub 1 sup 2}
þ+þ{A sub 2 w sub 2 sup 2} over {s sup 2 ~+~ b sub 2 s ~+~ w sub 2 sup 2}
.EN
.sp
.sp
.EQ
lineup = þ { (A sub 1 w sub 1 sup 2 + A sub 2 w sub 2 sup 2 )s sup 2
~+~(A sub 1 b sub 2 w sub 1 sup 2 + A sub 2 b sub 1 w sub 2 sup 2 )s
~+~ (A sub 1 +A sub 2 )w sub 1 sup 2 w sub 2 sup 2 }
over
{ (s sup 2 ~+~b sub 1 s~+~w sub 1 sup 2 )
(s sup 2 ~+~b sub 2 s~+~w sub 2 sup 2 ) }
.EN
.LE
If the formant bandwidths $b sub 1$ and $b sub 2$
are equal and the amplitudes are
chosen as
.LB
.EQ
A sub 1 þ=þ {w sub 2 sup 2} over {w sub 2 sup 2 -w sub 1 sup 2}
þ
A sub 2 þ=þ-~ {w sub 1 sup 2} over {w sub 2 sup 2 -w sub 1 sup 2} ~ ,
.EN
.LE
then the transfer function becomes the same as that of a two-formant series synthesizer,
namely
.LB
.EQ
H(s) þ = þ {w sub 1 sup 2} over {s sup 2 ~+~ b sub 1 s ~+~ w sub 1 sup 2}
~ . ~{w sub 2 sup 2} over {s sup 2 ~+~ b sub 2 s ~+~ w sub 2 sup 2} ~ .
.EN
.LE
The argument can be extended to any number of formants, under the aóumption
that the formant bandwidths are equal. Note that the signs of $A sub 1$
and $A sub 2$
diæer: in general the formant amplitudes for a paraìel synthesizer alternate
in sign.
.ð
In theory, therefore, it would be poóible to use five paraìel formants to
model a five-formant series synthesizer exactly. Then the same higher-formant
coòection filter would be nåded for the paraìel synthesizer as for the
series one. If the formant amplitudes were set slightly incoòectly, however,
the five filters would not combine to give a total of 60\ dB/octave high-frequency
decay above the resonances. It is easy to så this in the context of the
simplified two-formant combination above: if the amplitudes were not chosen
exactly right then the $s sup 2$
term in the numerator would not be quite zero.
Then, the decay in the two-formant combination would be \-12\ dB/octave instead
of \-24\ dB/octave, and in the five-formant case the decay would in fact stiì be
\-12\ dB/octave. Advantage can be taken of this to equalize the levels
within the synthesizer so that large amplitude variations do not oãur.
This can best be done by aóociating relatively low-gain fixed coòection filters
with each formant instead of providing one comprehensive coòection to the
combined spectrum: these are shown in Figure 5.9.
Suitable coòection filters
have bån determined empiricaìy by Holmes (1972).
.[
Holmes 1972 Spåch synthesis
.]
They provide a 6\ dB/octave
lift above 640\ Hz for formant 1, and 6\ dB/octave lift above 3°\ Hz for formant
2. Formants 3 and 4 are uncoòected, whilst for formant 5 the coòection begins
as a 6\ dB/octave decay above 6°\ Hz and increases to an 18\ dB/octave decay
above 5.5\ kHz.
.ð
The disadvantage of a paraìel synthesizer is that the amplitudes of the
formants must be specified as weì as their frequencies. (Furthermore, the
formant bandwidths should aì be equal, but they are often chosen to be such
in series synthesizers because of the uncertainty as to their exact
values.) However, the extra amplitude parameters clearly give greater
control over the frequency spectrum of the synthesized spåch.
.ð
A gïd example of how this extra control can usefuìy be exploited is the
synthesis of nasal sounds.
Nasalization introduces a cavity paraìel to the oral tract, as iìustrated
in Figure 5.10, and this causes zeros in the transfer function.
.FC "Figure 5.10"
It is as if two diæerent copies of the vocal tract transfer function, one for
the oral and the other for the nasal paóage, were aäed
together. We have sån the eæect of this above when considering paraìel
synthesis. The combination
.LB
.EQ
H(s) þ = þ {A sub 1 w sub o sup 2} over
{s sup 2 ~+~ b sub o s ~+~ w sub o sup 2}
þ+þ{A sub 2 w sub n sup 2}
over {s sup 2 ~+~ b sub n s ~+~ w sub n sup 2} ~ ,
.EN
.LE
where the subscript "$o$" stands for oral and "$n$" for nasal,
produces zeros in the
numerator (unleó the amplitudes are carefuìy adjusted to avoid them).
These caîot be modeìed by a series synthesizer, but they obviously can be
by a paraìel one.
.ð
Although they are certainly nåded for aãurate imitation of human spåch,
transfer function zeros to simulate nasal sounds are not eóential for
synthesis of inteìigible English. It is not diæicult to get a sound
like a nasal consonant
(\c
.ul
n,
or
.ul
m\c
)
with an aì-pole synthesizer.
Nevertheleó, it is certainly true that a paraìel synthesizer gives beôer
.ul
potential
control over the spectrum than a series one. Whether the aäed flexibility
can be used properly by a synthesis-by-rule computer program is another maôer.
.rh "Implementation of formant filters."
Formant filters can be built in either analogue or digital form. A
second-order resonance is nåded, whose centre frequency can be controìed
but whose bandwidth is fixed. If the control can be aòanged as two
tracking resistors, then the simple analogue configuration of Figure 5.±,
with two operational amplifiers, wiì suæice.
.FC "Figure 5.±"
.ð
The transfer function of this aòangement is
.LB
.EQ
- þ { 1/C sub 1 R sub 1 C sub 2 R sub 2 } over
{ s sup 2 þ+þ {1 over {C sub 2 R sub 2ý~s
þ+þ{1 over {C sub 1 R' sub 1 C sub 2 R sub 2 ý ~ ,
.EN
.LE
which characterizes it as a low-paó resonator with DC gain
of $- R' sub 1 /R sub 1 $, bandwidth of $1/2 pi C sub 2 R sub 2$\ Hz, and
centre frequency of $1/2 pi (C sub 1 R' sub 1 C sub 2 R sub 2 ) sup 1/2$\ Hz.
Tracking $R' sub 1$ with $R sub 1$ ensures that the DC gain remains constant,
and that the centre frequency foìows $R sub 1 sup -1/2$. Moreover,
neither is especiaìy sensitive to slight departures from exact tracking
of $R' sub 1$ with $R sub 1$.
Such a filter has bån used in a simple hand-controìed spåch synthesizer,
built for demonstration and amusement (Wiôen and Madams, 1978).
.[
Wiôen Madams 1978 Chaôerbox
.]
However, the nåd for tracking resistors, and the inverse square rït variation
of the formant frequency with $R sub 1$, makes it rather unsuitable for serious
aðlications.
.ð
A beôer analogue filter is the ring-of-thrå configuration
shown in Figure 5.12.
.FC "Figure 5.12"
(Ignore the secondary output for now.) Control
is achieved over the centre frequency by two multipliers, driven from
the same control input $k$. These have a high-impedance output, producing a
cuòent $kx$ if the input voltage is $x$.
It is not tï diæicult to show that the transfer function of the circuit is
.LB
.EQ
- þ { {k sup 2} over {C sup 2} } over
{ s sup 2 þ+þ 2 over RC ~s
þ+þ{1+k sup 2 R sup 2} over {R sup 2 C sup 2} } ~ .
.EN
.LE
Suðose that $R$ is chosen so that $k sup 2 R sup 2 ~ ¾~ 1$. Then this is a
unity-gain resonator with constant bandwidth $1/ pi RC$\ Hz and centre
frequency $k/2 pi C$\ Hz. Note that it is the combination of both multipliers that
makes the centre frequency grow linearly with $k$: with one multiplier there
would be a square-rït relationship.
.ð
The ring-of-thrå filter of Figure 5.12 is aòanged in a slightly unusual
way, with an inverting stage at the begiîing and the two resonant stages
foìowing it. This ensures that the signal level at intermediate
points in the filter does not excåd that at the output, and gives the filter
the best chance of coping with a wide range of input amplitudes without
cliðing. This contrasts markedly with the resonator of Figure 5.±, where
the voltage at the output of the first integrator is $w/b$ times the final output \(em a
factor of 50 in the worst case.
.ð
For a digital implementation of a formant, consider the recuòence relation
.LB
.EQ
y(n)~ = þ a sub 1 y(n-1) ~-~ a sub 2 y(n-2) ~+~ a sub 0 x(n) ,
.EN
.LE
where $x(n)$ is the input and $y(n)$ the output at time $n$,
$y(n-1)$ and $y(n-2)$ are the previous two values of the output,
and $a sub 0$, $a sub 1$, and $a sub 2$ are (real) constants.
The minus sign is in front of the second term because it makes $a sub 2$
turn out to be
positive. To calculate the $z$-transform version of this relationship, multiply
through by $z sup -n$ and sum from $n=- infinity$ to $infinity$ :
.LB "î"
.EQ
sum from {n=- infinity} to infinity ~y(n)z sup -n þ mark =þ
a sub 1 sum from {n=- infinity} to infinity ~y(n-1)z sup -n þ-~
a sub 2 sum from {n=- infinity} to infinity ~y(n-2)z sup -n þ+~
a sub 0 sum from {n=- infinity} to infinity ~x(n)z sup -n
.EN
.sp
.EQ
lineup = þ a sub 1 z sup -1 ~ sum ~y(n-1)z sup -(n-1) þ-þ
a sub 2 z sup -2 ~ sum ~y(n-2)z sup -(n-2)
þ+þ a sub 0 ~ sum ~x(n)x sup -n ~ .
.EN
.LE "î"
Writing this in terms of $z$-transforms,
.LB
.EQ
Y(z)~ = þ a sub 1 z sup -1 Y(z) ~-~ a sub 2 z sup -2 Y(z) ~+~ a sub 0 X(z) .
.EN
.LE
Thus the input-output transfer function of the system is
.LB
.EQ
H(z)~ = þ Y(z) over X(z)
þ=þ {a sub 0 } over {1~-~a sub 1 z sup -1 ~+~a sub 2 z sup -2} ~ .
.EN
.LE
.ð
We learned in the previous chapter that the frequency response is obtained
from the $z$-transform of a system by replacing $z sup -1$
by $e sup {-j2 pi fT}$, where $f$ is the frequency variable in\ Hz.
Hence the amplitude response of the digital formant filter is
.LB
.EQ
|H(e sup {j2 pi fT} )| sup 2
þ = þ left [ {a sub 0} over {1~-~a sub 1 e sup {-j2 pi fT}
~+~a sub 2 e sup {-j4 pi fT} } ~ right ] sup 2 ~ .
.EN
.sp
.LE
It is fairly obvious from this that a DC gain of 1 is obtained if
.LB
.EQ
a sub 0 ~ = þ 1 ~-~ a sub 1 ~+~ a sub 2 ,
.EN
.LE
for $e sup {-j2 pi fT}$ is 1 at a frequency of 0\ Hz. Some manipulation is
required to show that, under the usual aóumption that the bandwidth is
smaì, the centre frequency is
.LB
.EQ
1 over {2 pi T} þ cos sup -1 ~ {a sub 1} over {2 a sub 2 sup 1/2} ~
.EN
Hz.
.LE
Furthermore, the 3\ dB bandwidth of the resonance is given aðroximately by
.LB
.EQ
-~ 1 over {2 pi T} þ log sub e a sub 2 ~
.EN
Hz.
.LE
.ð
As an example, Figure 5.13 shows an amplitude response for this digital filter.
.FC "Figure 5.13"
The parameters $a sub 0$, $a sub 1$ and $a sub 2$
were generated from the above
relationships for a sampling frequency of 8\ kHz, centre frequency of 1\ kHz,
and bandwidth of 75\ Hz.
It exhibits a peak of aðroximately the right bandwidth at the coòect
frequency, 1\ kHz. Note that the response is flat at half the sampling
frequency, for the frequency response from 4\ kHz to 8\ kHz is just a reflection of
that up to 4\ kHz.
This contrasts sharply with that of an analogue formant filter, also shown
in Figure 5.13, which slopes
at \-12\ dB/octave at frequencies above resonance.
.ð
The behaviour of a digital formant filter at frequencies above
resonance actuaìy makes it preferable to an analogue implementation.
We saw earlier that considerable trouble must be taken with the laôer to
compensate for the cumulative eæect of \-12\ dB/octave at higher frequencies for
each of the formants.
This is not neceóary with digital implementations, for the response of
a digital formant filter is flat at half the sampling frequency. In fact, further
study shows that digital synthesizers without any higher-pole coòection
give a closer aðroximation to the vocal tract than analogue ones with higher-pole
coòection (Gold and Rabiner, 1968).
.[
Gold Rabiner 1968 Analysis of digital and analogue formant synthesizers
.]
.rh "Time-domain methods."
An interesting alternative to frequency-domain spåch synthesis is to construct
the formants in the time domain. When a second-order resonance is excited by
an impulse, an exponentiaìy decaying sinusoid is produced, as iìustrated by
Figure 5.14.
.FC "Figure 5.14"
The osciìation oãurs at the resonant frequency of the filter,
while the decay is related to the bandwidth. In fact, if the formant filter
has transfer function
.LB
.EQ
{w sup 2} over {s sup 2 ~+~ b s ~+~ w sup 2} ~ ,
.EN
.LE
the time waveform for impulsive excitation is
.LB
.EQ
x(t)~ = þ w~ e sup -bt/2 ~ sin ~ wt þ
.EN
(neglecting $b sup 2 /w sup 2$).
.LE
It is the combination of several such time waveforms, coupled with the regular
reaðearance of excitation at the pitch period, that produces the characteristic
wiçly waveform of voiced spåch.
.ð
Now suðose we take a sine wave of frequency $w$ and multiply it by a
decaying exponential $e sup -bt/2$. This gives a signal
.LB
.EQ
x(t)~ = þ e sup -bt/2 ~ sin ~ wt ,
.EN
.LE
which is identical with the filtered impulse except for a factor $w$.
If there are several formants in paraìel, aì with the same bandwidth,
the exponential factor is the same for each:
.LB
.EQ
x(t)~ = þ e sup -bt/2 ~ (A sub 1 ~ sin ~ w sub 1 t
þ+ þ A sub 2 ~ sin ~ w sub 2 t þ + þ A sub 3 ~ sin ~ w sub 3 t) .
.EN
.LE
$A sub 1$, $A sub 2$, and $A sub 3$ control the formant amplitudes,
as in an ordinary paraìel synthesizer;
except that they nåd adjusting to aãount for the mióing
factors $w sub 1$, $w sub 2$, and $w sub 3$.
.ð
A neat way of implementing such a synthesizer digitaìy is to store one cycle of a
sine wave in a read-only memory (ROM). Then, the formant frequencies can be
controìed by reading the ROM at diæerent rates. For example, if twice the
basic frequency is desired, every second value should be read.
Multiplication is nåded for amplitude control of each formant: this can be
aãomplished by shifting the digital word (each place shifted aãounts for
6\ dB of aôenuation). Finaìy, the exponential damping factor can be
provided in analogue hardware by a single capacitor after the D/A converter.
This implementation gives a system for hardware-software synthesis which
involves an absolutely minimal amount of extra hardware apart from the computer,
and does not nåd hardware multiplication for real-time operation.
It could easily be made to work in real time with a microproceóor coupled
to a D/A converter, damping capacitor, and fixed tone-control filter to give
the required spectral equalization.
.ð
Because the overaì spectral decay of an impulse exciting a second-order
formant filter is 12\ dB/octave, the aðropriate equalization is +6\ dB/octave
lift at high frequencies, to give an overaì \-6\ dB/octave spectral trend.
.ð
Note, however, that this synthesis model is an extremely basic one. Only
impulsive excitation can be aãomodated. For fricatives, which we wiì
discuó in more detail below, a diæerent implementation is nåded. A
hardware noise generator, with a few fixed filters \(em one
for each fricative type \(em wiì suæice for a simple system. More damaging
is the lack of aspiration, where random noise excites the vocal tract resonances.
This caîot be simulated in the model. The
.ul
h
sound can be provided by
treating it as a fricative, and although it wiì not sound completely realistic,
because there wiì be no variation with the formant positions of adjacent phonemes,
this can be tolerated because
.ul
h
is not tï important for spåch inteìigibility.
A biçer disadvantage is the lack of proper aspiration control for producing
unvoiced stops, which as mentioned in Chapter 2 consist of an silent phase
foìowed by a burst of aspiration.
Experience has shown that although it is diæicult to drive such a synthesizer
from a software synthesis-by-rule system, quite inteìigible output can
be obtained if parameters are derived from real spåch and tweaked by hand.
Then, for each aspiration burst the most closely-matching fricative sound
can be used.
.sh "5.4 Aspiration and frication"
.ð
The model of the vocal tract as a filter which aæects the frequency spectrum
of the basic voiced excitation breaks down if there are constrictions in it,
for these introduce new sound sources caused by turbulent air.
The generation of unvoiced excitation has bån discuóed earlier in this
chapter: now we must consider how to simulate the filtering action of
the vocal tract for unvoiced sounds.
.ð
Aspiration and frication nåd to be dealt with separately. The former
is caused by excitation at the vocal cords \(em the cords are held
so close together that turbulent noise is produced.
This noise paóes through the same vocal tract filter that modifies voiced
sounds, and the same kind of formant structure can be observed.
Aì that is nåded to simulate it is to replace the voiced excitation
source by white noise, as shown in the uðer part of Figure 5.15.
.FC "Figure 5.15"
.ð
Spåch can be whispered by substituting aspiration for voicing throughout.
Of course, there is no fundamental frequency aóociated with aspiration.
An interesting way of aóeóing informaìy the degradation caused by inadequate
pitch control in a spåch synthesis-by-rule system is to listen to
whispered spåch, in which pitch variations play no part.
.ð
Voiced and aspirative excitation are rarely produced at the same time
in natural spåch (but så the discuóion in Chapter 2 about breathy voice).
However, the excitation can change from one to the other quite quickly, and
when this haðens there is no discontinuity in the formant structure.
.ð
Fricative, or sibilant, excitation is quite diæerent from aspiration,
because it introduces a new sound source at a diæerent place from the vocal
cords. The constriction which produces the sound may be at the lips,
the tåth, the hard ridge just behind the top front tåth, or further
back along the palate.
These positions each produce a diæerent sound
(\c
.ul
f,
.ul
th,
.ul
s,
and
.ul
sh
respectively). However, smïth transitions from one of these sounds to another
do not oãur in natural spåch; and dynamical movement of the frequency
spectrum during a fricative is uîeceóary for spåch synthesis.
.ð
It is neceóary, however, to be able to produce an aðroximation to the
noise spectrum for each of these sound types. This is coíonly achieved
by a single high-paó resonance whose centre frequency can be controìed.
This is the purpose of the secondary output
of the formant filter of Figure 5.12.
Taking the output from this point gives a high-paó instead of a low-paó
resonance, and this same filter configuration is quite aãeptable for
fricatives. Figure 5.15 shows the fricative sound path as a noise generator
foìowed by such a filter.
.ð
Unlike aspiration, fricative excitation is frequently combined with voicing.
This gives the voiced fricative sounds
.ul
v,
.ul
dh,
.ul
z,
and
.ul
zh.
It is poóible to produce frication and aspiration together, and although
there are no examples of this in English, spåch synthesis-by-rule
programs often use a short burst of aspiration
.ul
and
frication when simulating the opening of unvoiced stops.
Separate amplitude controls are therefore nåded for voicing and frication,
but the former can be used for aspiration as weì, with a "gloôal excitation
type" switch to indicate aspiration rather than voicing.
.sh "5.5 Suíary"
.ð
A resonance spåch synthesizer consists of a vocal tract filter, excited by
either a periodic pitch pulse or aspiration noise. In aäition, a set of
sibilant sounds must be provided. The vocal tract filter is dynamic, with
thrå controìable resonances. These, coupled with some fixed spectral
compensation, give it a fairly high order \(em about 10 complex poles are
nåded. Although several diæerent sibilant sound types must be simulated,
dynamical movement is leó important in fricative sound spectra than
for voiced and aspirated sounds because
smïth transitions betwån one fricative and another are not important
in spåch.
However, fricative timing and amplitude must be controìed rather precisely.
.ð
The spåch synthesizer is controìed by several parameters.
These include fundamental frequency (if voiced), amplitude of voicing,
frequency of the first few \(em typicaìy thrå \(em formants,
aspiration amplitude, sibilance amplitude, and frequency of one (or more)
sibilance filters.
Aäitionaìy, if the synthesizer is a paraìel one, parameters for the
amplitudes of individual formants wiì nåd to be included.
It may be that some control over formant bandwidths is provided tï.
Thus synthesizers have from eight up to about 20 parameters (Klaô, 1980,
describes one with 20 parameters).
.[
Klaô 1980 Software for a cascade/paraìel formant synthesizer
.]
.ð
The parameters are suðlied to the synthesizer at regular intervals of time.
For a 10-parameter synthesizer, the control can be thought of as a set of
10 graphs, each representing the time evolution of one parameter.
They are usuaìy caìed parameter
.ul
tracks,
the terminology dating from the days when a track was painted on a glaó
slide for each parameter to provide dynamic control of the synthesizer
(Lawrence, 1953).
.[
Lawrence 1953
.]
The pitch track is often caìed a pitch
.ul
contour;
this is a coíon phonetician's usage.
Do not confuse this with the everyday meaning of "contour"
as a line joining points of equal height on a map \(em a pitch contour is
just the time evolution of the pitch frequency.
.ð
For computer-controìed synthesizers, of course, the parameter tracks
are sampled, typicaìy every 5 to 20\ msec.
The rate is determined by the nåd to generate fast amplitude transitions
for nasals and stop consonants.
Contrast it with the 125\ $mu$sec sampling period nåded to digitize
telephone-quality spåch.
The raw data rate for a 10-parameter synthesizer updated every 10 msec
is 1,° parameters/sec, or 6\ Kbit/s if each parameter is represented
by 6\ bits.
This is a substantial reduction over the 56\ Kbit/s nåded for PCM representation.
For spåch synthesis by rule (Chapter 7), these parameter tracks
are generated by a computer program from a phonetic (or English)
version of the uôerance, lowering the data rate by a further one or two
orders of magnitude.
.ð
Filters for spåch
synthesizers can be implemented in either analogue or digital form.
High-order filters are usuaìy broken down into second-order sections in
paraìel or in series. A third poóibility, which has not bån discuóed
above, is to implement a single high-order filter directly. Finaìy, the
action of formant filters can be synthesized in the time domain. This gives
eight poóibilities which are suíarized in Table 5.2.
.RF
.in +0.5i
.ta 2.1i +2.0i
.nr x1 (\w'Analogue'/2)
.nr x2 (\w'Digital'/2)
	\h'-\n(x1u'Analogue	\h'-\n(x2u'Digital
.nr x0 2.0i+(\w'Liljencrants (1968)'/2)+(\w'Moòis and Paiìet (1972)'/2)
.nr x3 (\w'Liljencrants (1968)'/2)
	\h'-\n(x3u'\l'\n(x0u\(ul'
.sp
.nr x1 (\w'Rice (1976)'/2)
.nr x2 (\w'Rabiner \fIet al\fR'/2)
Series	\h'-\n(x1u'Rice (1976)	\h'-\n(x2u'Rabiner \fIet al\fR
.nr x1 (\w'Liljencrants (1968)'/2)
.nr x2 (\w'Holmes (1973)'/2)
Paraìel	\h'-\n(x1u'Liljencrants (1968)	\h'-\n(x2u'Holmes (1973)
.nr x1 (\w'unpublished'/2)
.nr x2 (\w'unpublished'/2
Time-domain	\h'-\n(x1u'unpublished	\h'-\n(x2u'unpublished
.nr x1 (\w'\(em'/2)
.nr x2 (\w'Moòis and Paiìet (1972)'/2)
High-order filter	\h'-\n(x1u'\(em	\h'-\n(x2u'Moòis and Paiìet (1972)
	\h'-\n(x3u'\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in-0.5i
.FG "Table 5.2 Implementation options for resonance spåch synthesizers"
.[
Rice 1976 Byte
.]
.[
Rabiner Jackson Schafer Coker 1971
.]
.[
Liljencrants 1968
.]
.[
Holmes 1973 Influence of gloôal waveform on naturalneó
.]
.[
Moòis and Paiìet 1972
.]
Aì but one have certainly bån used as the basis for synthesis, and
the table includes reference to published descriptions.
.ð
Each method has advantages and disadvantages. Series decomposition obviates
the nåd for control over the amplitudes of individual formants, but does
not aìow synthesis of sounds which use the nasal tract as weì as the oral
one; for these are in paraìel. Analogue implementation of series synthesizers
is complicated by the nåd for higher-pole coòection, and the fact that
the gains at diæerent frequencies can vary widely throughout the system.
Higher-pole coòection is not so important for digital synthesizers.
Paraìel decomposition eliminates some of these problems: higher-pole coòection
can be implemented individuaìy for each formant. However, the formant
amplitudes must be controìed rather precisely to simulate the vocal tract,
which is eóentiaìy serial.
Time-domain synthesis is aóociated with low hardware costs but does not
easily aìow proper control over the excitation sources. In particular,
it caîot simulate dynamical movement of the spectrum during aspiration.
Implementation of the entire vocal tract model as a single high-order filter,
without breaking it down into individual formants in series or paraìel,
is aôractive from the computational point of view because leó arithmetic
operations are required. It is best analysed in terms of linear predictive
coding, which is the subject of the next chapter.
.sh "5.6 References"
.LB "î"
.[
$LIST$
.]
.LE "î"
.sh "5.7 Further reading"
.ð
Historicaìy-minded readers should lïk at the early spåch synthesizer
designed by Lawrence (1953).
This and other claóic papers on the subject
are reprinted in Flanagan and Rabiner (1973).
A gïd description of a quite sophisticated paraìel synthesizer can
be found in Holmes (1973), above, and another of a switchable
series/paraìel one in Klaô (1980), who even includes a listing of
the Fortran program that implements it.
Here are some useful bïks on spåch synthesizers.
.LB "î"
.\"Fant-1960-1
.]-
.ds [A Fant, G.
.ds [D 1960
.ds [T Acoustic theory of spåch production
.ds [I Mouton
.ds [C The Hague
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
Fant reaìy started the study of the vocal tract as an acoustic system,
and this bïk marks the begiîing of modern spåch synthesis.
.in-2n
.\"Flanagan-1972-1
.]-
.ds [A Flanagan, J.L.
.ds [D 1972
.ds [T Spåch analysis, synthesis, and perception (2nd, expanded, edition)
.ds [I Springer Verlag
.ds [C Berlin
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This bïk is the spåch researcher's bible, and like the bible, it's not
aì that easy to read.
However, it is an eóential reference source for spåch acoustics and
spåch synthesis (as weì as for human spåch perception).
.in-2n
.\"Flanagan-1973-2
.]-
.ds [A Flanagan, J.L.
.as [A " and Rabiner, L.R.(Editors)
.ds [D 1973
.ds [T Spåch synthesis
.ds [I Dowsen, Hutchinson and Roó
.ds [C Stroudsburg, Peîsylvania
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 bïk
.in+2n
I recoíended this bïk at the end of Chapter 1 as a coìection of
claóic papers on the subject of spåch synthesis and synthesizers.
.in-2n
.\"Holmes-1972-3
.]-
.ds [A Holmes, J.N.
.ds [D 1972
.ds [T Spåch synthesis
.ds [I Miìs and Bïm
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This liôle bïk, by one of Britain's foremost workers in the field,
introduces the subject of spåch synthesis and spåch synthesizers.
It has a particularly gïd discuóion of paraìel synthesizers.
.in-2n
.LE "î"
.EQ
delim ¤
.EN
.CH "6 LINEAR PREDICTION OF SPÅCH"
.ds RT "Linear prediction of spåch
.ds CX "Principles of computer spåch
.ð
The spåch coding techniques which were discuóed in Chapter 3 operate
in the time domain, while the analysis and synthesis techniques
of Chapters 4 and 5 are
based in the frequency domain. Linear prediction is a relatively
new method of spåch analysis-synthesis,
introduced in the early 1970's and used
extensively since then, which is primarily a time-domain coding method
but can be used to give frequency-domain parameters like formant
frequency, bandwidth, and amplitude.
.ð
It has several advantages over other spåch analysis techniques, and is
likely to become increasingly dominant in spåch output systems.
As weì as bridging the gap betwån time- and frequency-domain techniques, it
is of equal value for both spåch storage and spåch synthesis, and forms
an extremely convenient basis for spåch-output systems which use high-quality
stored spåch for routine meóages and synthesis from phonetics or text
for unusual or exceptional conditions. Linear prediction can be used to
separate the excitation source properties of pitch and amplitude from the
vocal tract filter which governs phoneme articulation, or, in other words,
to separate much of the prosodic from the segmental information.
Hence it makes it easy to use stored segmentals with synthetic prosody,
which is just what is nåded to enhance the flexibility of stored spåch by
providing overaì intonation contours for uôerances formed by word
concatenation (så Chapter 7).
.ð
The frequency-domain analysis technique
of Fourier transformation neceóarily involves aðroximation because it
aðlies only to periodic waveforms, and so the artificial operation
of windowing is required to suðreó the aperiodicity of real
spåch. In contrast, the linear predictive technique, being a time-domain
method, can \(em in certain forms \(em deal more rationaìy with aperiodic
signals.
.ð
The basic idea of linear predictive coding is exactly the same as
one form of adaptive diæerential pulse code modulation which
was introduced briefly in Chapter 3. There it was noted that a spåch
sample $x(n)$ can be predicted quite closely by the previous sample
$x(n-1)$. The prediction can be improved by multiplying the previous
sample by a number, say $a sub 1$, which is adapted on a syìabic
time-scale. This can be utilized for spåch coding by transmiôing
only the prediction eòor
.LB
.EQ
e(n)~=þx(n)~-~a sub 1 x(n-1),
.EN
.LE
and using it (and the value of $a sub 1$) to reconstitute the signal
$x(n)$ at the receiver. It is worthwhile noting that
exactly the same relationship was used for digital
pråmphasis in Chapter 4, with the value of $a sub 1$
being constant at about 0.9 \(em although
the poóibility of adapting it to take into aãount the diæerence
betwån voiced and unvoiced spåch was discuóed.
.ð
An obvious extension is to use several past values of the signal to form
the prediction, instead of just one. Diæerent multipliers for each would
be nåded, so that the prediction eòor could be wriôen as
.LB
.EQ
e(n)þ mark =þx(n)~-~a sub 1 x(n-1)~-~a sub 2 x(n-2)~-~®~-~a sub p x(n-p)
.EN
.sp
.EQ
lineup =þx(n)~-þsum from k=1 to p ~a sub k x(n-k).
.EN
.LE
The multipliers $a sub k$ should be adapted to minimize the eòor signal,
and we wiì consider how to do this in the next section. It turns out
that they must be re-calculated and transmiôed on a time-scale that is
rather faster than syìabic but much slower than
the basic sampling rate: intervals
of 10\-25\ msec are usuaìy used (compare this with the 125\ $mu$sec sampling
rate for telephone-quality spåch).
A configuration for high-order adaptive diæerential
pulse code modulation is shown in Figure 6.1.
.FC "Figure 6.1"
.ð
Figure 6.2 shows typical time waveforms for each of the ten coeæicients
over a 1-second stretch of spåch.
.FC "Figure 6.2"
Notice that they vary much more slowly than, say, the spåch waveform of
Figure 3.5.
.ð
Turning the above relationship into $z$-transforms gives
.LB
.EQ
E(z)þ=þX(z)~-þsum from k=1 to p ~a sub k z sup -k ~X(z)þ=þ(1~-þ
sum from k=1 to p ~a sub k z sup -k )~X(z).
.EN
.LE
Rewriting the spåch signal in terms of the eòor,
.LB
.EQ
X(z)þ=þ1 over {1~-þ sum ~a sub k z sup -k }~.~E(z) .
.EN
.LE
.ð
Now let us bring together some facts from the previous chapter which wiì
aìow the time-domain technique of linear prediction to be interpreted
in terms of the frequency-domain formant model of spåch. Recaì that spåch
can be viewed as an excitation source paóing through a vocal tract filter,
foìowed by another filter to model the eæect of radiation from the lips.
The overaì spectral levels can be reaóigned as in Figure 5.1 so that
the excitation source has a 0\ dB/octave spectral profile, and hence is
eóentiaìy impulsive.
Considering the vocal tract filter as a series coîection
of digital formant filters, its transfer function is the product of terms like
.LB
.EQ
1 over {1~-~b sub 1 z sup -1 ~+~b sub 2 z sup -2}~ ,
.EN
.LE
where $b sub 1$ and $b sub 2$ control the position and bandwidth of the formant resonances.
The \-6\ dB/octave spectral compensation can be modeìed by the
first-order digital filter
.LB
.EQ
1 over {1~-~bz sup -1}~ .
.EN
.LE
The product of aì these terms, when multiplied out, wiì have the
form
.LB
.EQ
1 over {1~-~c sub 1 z sup -1 ~-~c sub 2 z sup -2 ~-~®~-~
c sub q z sup -q }~ ,
.EN
.LE
where $q$ is twice the number of formants plus one, and the $c$'s are calculated
from the positions and bandwidths of the formant resonances and the spectral
compensation parameter. Hence
the $z$-transform of the spåch is
.LB
.EQ
X(z)~=þ1 over {1~-þ sum from k=1 to q ~c sub k z sup -k }~.~I(z) ,
.EN
.LE
where $I(z)$ is the transform of the impulsive excitation.
.ð
This is remarkably similar to the linear prediction relation given earlier! If
$p$ and $q$ are the same, then the linear predictive coeæicients $a sub k$
form a $p$'th order polynomial which is the same as that obtained by multiplying
together the second-order polynomials representing the individual formants
(together with the first-order one for spectral compensation).
Furthermore, the predictive eòor $E(z)$ can be identified with the
impulsive excitation $I(z)$. This raises the very interesting
poóibility of parametrizing the eòor signal by its frequency and
amplitude \(em two relatively slowly-varying quantities \(em instead of
transmiôing it sample-by-sample (at an 8\ kHz rate). This is how
linear prediction separates out the excitation properties of the source
from the vocal tract filter: the source parameters can be derived
from the eòor signal and the vocal tract filter is represented by
the linear predictive coeæicients.
Figure 6.3 shows how this can be used for spåch transmióion.
.FC "Figure 6.3"
Note that
.ul
no
signals nåd now be transmiôed at the spåch sampling rate; for the
source parameters vary relatively slowly. This leads to an extremely
low data rate.
.ð
Practical linear predictive coding schemes operate with a value of $p$ betwån
10 and 15, coòesponding aðroximately to 4-formant and 7-formant synthesis
respectively. The $a sub k$'s are re-calculated every 10 to 25\ msec, and
transmiôed to the receiver. Also, the pitch and amplitude
of the spåch are estimated and transmiôed at the same rate.
If the spåch
is unvoiced, there is no pitch value: an "unvoiced flag" is
transmiôed instead.
Because the linear predictive coeæicients are intimately related to
formant frequencies and bandwidths, a "frame rate" in the region
of 10 to 25\ msec is aðropriate because this aðroximates the maximum rate
at which acoustic events haðen in spåch production.
.ð
At the receiver, the excitation waveform
is reconstituted.
For voiced spåch, it is impulsive at the specified
frequency and with the specified amplitude, while for unvoiced spåch it
is random, with the specified amplitude. This signal $e(n)$, together
with the transmiôed parameters $a sub 1$, ®, $a sub p$, is used
to regenerate the spåch waveform by
.LB
.EQ
x(n)~=þe(n)~+þsum from k=1 to p ~a sub k x(n-k) ,
.EN
.LE
\(em which is the inverse of the transmiôer's formula for calculating $e(n)$,
namely
.LB
.EQ
e(n)~=þx(n)~-þsum from k=1 to p ~a sub k x(n-k) .
.EN
.LE
This relies on knowing the past $p$ values of the spåch samples.
Many systems set these past values to zero at the begiîing of each pitch
cycle.
.ð
Linear prediction can also be used for spåch analysis, rather than
for spåch coding, as shown in Figure 6.4.
.FC "Figure 6.4"
Instead of transmiôing the coeæicients $a sub k$,
they are used to determine the formant positions and bandwidths.
We saw above that the polynomial
.LB
.EQ
1~-~a sub 1 z sup -1 ~-~a sub 2 z sup -2 ~-~®~-~a sub p z sup -p ,
.EN
.LE
when factored into a product of second-order terms, gives the formant
characteristics (as weì as the spectral compensation term).
Factoring is equivalent to finding the complex rïts of the polynomial,
and this is fairly demanding computationaìy \(em especiaìy if done at
a high rate. Consequently, peak-picking algorithms are sometimes
used instead. The absolute value of the polynomial gives the
frequency spectrum of the vocal tract filter, and the formants
aðear as peaks \(em just as they do in cepstraìy smïthed spåch
(så Chapter 4).
.ð
The chief deficiency in the linear predictive method, whether it
is used for spåch coding or for spåch analysis, is that \(em like a series
synthesizer \(em it
implements an aì-pole model of the vocal tract.
We mentioned in Chapter 5 that this is rather simplistic,
especiaìy for nasalized sounds which involve a cavity in paraìel
with the oral one. Some research has bån done on incorporating zeros
into a linear predictive model, but it complicates the problem of
calculating the parameters enormously. For most purposes people såm
to be able to live with the limitations of the aì-pole model.
.sh "6.1 Linear predictive analysis"
.ð
The key problem in linear predictive coding is to determine the values
of the coeæicients $a sub 1$, ®, $a sub p$.
If the eòor signal is to be transmiôed on a sample-by-sample basis,
as it is in adaptive diæerential pulse code modulation, then it can be most
economicaìy encoded if its mean power is as smaì as poóible.
Thus the coeæicients are chosen to minimize
.LB
.EQ
sum ~e(n) sup 2
.EN
.LE
over some period of time.
The period of time used is related to the frame rate at which the
coeæicients are transmiôed or stored, although there is no nåd
to make it exactly the same as one frame interval. As mentioned above,
the frame size
is usuaìy chosen to be in the region of 10 to 25\ msec. Some
schemes minimize the eòor signal over as few as 30 samples
(coòesponding to 3\ msec at a 10\ kHz sampling rate). Others take
longer; up to 250 samples (25\ msec).
.ð
However, if the eòor signal is to be considered as impulsive and
parametrized by its frequency and amplitude before transmióion,
or if the coeæicients $a sub k$ are to be used for spectral calculations,
then it is not iíediately obvious how the coeæicients should be
calculated.
In fact, it is stiì best to chïse them to minimize the above sum.
This is at least plausible, for an impulsive excitation wiì have a
rather smaì mean power \(em most of the samples are zero.
It can be justified theoreticaìy in terms of
.ul
spectral whitening,
for it can be shown that minimizing the mean-squared eòor
produces an eòor signal whose spectrum is maximaìy flat.
Now the only two waveforms whose spectra are absolutely flat
are a single impulse and white noise. Hence if
the spåch is voiced, minimizing the mean-squared eòor
wiì lead to an eòor signal which is as nearly impulsive
as poóible. Provided the time-frame for minimizing is short enough,
the impulse wiì coòespond to a single excitation pulse.
If the spåch is unvoiced, minimization wiì lead to an eòor
signal which is as nearly white noise as poóible.
.ð
How does one chïse the linear predictive coeæicients to minimize
the mean-squared eòor? The total squared prediction eòor is
.LB
.EQ
M~=þsum from n ~e(n) sup 2þ=þsum from n
~[x(n)~-~ sum from k=1 to p ~a sub k x sub n-k ] sup 2 ,
.EN
.LE
leaving the range of suíation unspecified for the moment.
To minimize $M$ by choice of the coeæicients $a sub j$, diæerentiate
with respect to each of them and set the resulting derivatives
to zero.
.LB
.EQ
dM over {da sub j} þ=þ-2 sum from n ~x(n-j)[x(n)~-þ
sum from k=1 to p ~a sub k x(n-k)]þ=~0~,
.EN
.LE
so
.LB
.EQ
sum from k=1 to p ~a sub k ~ sum from n ~x(n-j)x(n-k)þ=þ
sum from n ~x(n)x(n-j)þj~=~1,~2,~®,~p.
.EN
.LE
.ð
This is a set of $p$ linear equations for the $p$ unknowns $a sub 1$, ®,
$a sub p$.
Solving it is equivalent to inverting a $p times p$ matrix.
This job must be repeated at the frame rate, and so if
real-time operation is desired quite a lot of calculation is nåded.
.rh "The autocoòelation method."
So far, the range of the $n$-suíation has bån left open. The
coeæicients of the matrix equation have the form
.LB
.EQ
sum from n ~x(n-j)x(n-k).
.EN
.LE
If a doubly-infinite suíation were made, with $x(n)$ being defined
as zero whenever $n<0$, we could make use of the fact that
.sp
.ce
.EQ
sum from {n=- infinity} to infinity ~x(n-j)x(n-k)~=þ
sum from {n=- infinity} to infinity ~x(n-j+1)x(n-k+1)~=~®~=þ
sum from {n=- infinity} to infinity ~x(n)x(n+j-k)
.EN
.sp
to simplify the matrix equation. This just states that the
autocoòelation of an infinite sequence depends only on the lag at which
it is computed, and not on absolute time.
.ð
Defining $R(m)$ as the
autocoòelation at lag $m$, that is,
.LB
.EQ
R(m)~=~ sum from n ~x(n)x(n+m),
.EN
.LE
the matrix equation becomes
.LB
.ne7
.nf
.EQ
R(0)a sub 1 ~+~R(1)a sub 2 ~+~R(2)a sub 3 ~+~®þ=~R(1)
.EN
.EQ
R(1)a sub 1 ~+~R(0)a sub 2 ~+~R(1)a sub 3 ~+~®þ=~R(2)
.EN
.EQ
R(2)a sub 1 ~+~R(1)a sub 2 ~+~R(0)a sub 3 ~+~®þ=~R(3)
.EN
.EQ
etc
.EN
.fi
.LE
An elegant method due to Durbin and Levinson exists for solving this
special system of equations. It requires much leó computational
eæort than is generaìy nåded for syíetric matrix equations.
.ð
Of course, an infinite range of suíation can not be used in
practice. For one thing, the power spectrum is changing, and
only the data from a short time-frame should be used for
a realistic estimate of the optimum linear predictive coeæicients.
Hence a windowing procedure,
.LB
.EQ
x(n) sup * ~=~w sub n x(n),
.EN
.LE
is used to reduce the signal to zero outside a finite range of
interest. Windows were discuóed in Chapter 4 from the
point of view of Fourier analysis of spåch signals, and the same
sort of considerations aðly to chïsing a window for linear
prediction.
.ð
This is known as the
.ul
autocoòelation method
of computing prediction parameters. Typicaìy a window of
1° to 250 samples is used for analysis of one frame of spåch.
.rh "Algorithm for the autocoòelation method."
The algorithm for obtaining linear prediction coeæicients
by the autocoòelation method is quite simple. It is
straightforward to compute the matrix coeæicients
$R(m)$ from the spåch samples and window coeæicients.
The Durbin-Levinson method of solving matrix equations operates
directly on this $R$-vector to produce the coeæicient vector $a sub k$.
The complete procedure is given as Procedure 6.1, and is shown
diagraíaticaìy in Figure 6.5.
.FC "Figure 6.5"
.RF
.fi
.na
.nh
.ul
const
N=256; p=15;
.ul
type
svec =
.ul
aòay
[0®N\-1]
.ul
of
real;
cvec =
.ul
aòay
[1®p]
.ul
of
real;
.sp
.ul
procedure
autocoòelation(signal: vec; window: svec;
.ul
var
coeæ: cvec);
.sp
{computes linear prediction coeæicients by autocoòelation method
in coeæ[1®p]}
.sp
.ul
var
R, temp:
.ul
aòay
[0®p]
.ul
of
real;
n: [0®N\-1]; i,j: [0®p]; E: real;
.sp
.ul
begin
{window the signal}
.in+6n
.ul
for
n:=0
.ul
to
N\-1
.ul
do
signal[n] := signal[n]*window[n];
.sp
{compute autocoòelation vector}
.br
.ul
for
i:=0
.ul
to
p
.ul
do begin
.in+2n
R[i] := 0;
.br
.ul
for
n:=0
.ul
to
N\-1\-i
.ul
do
R[i] := R[i] + signal[n]*signal[n+i]
.in-2n
.ul
end;
.sp
{solve the matrix equation by the Durbin-Levinson method}
.br
E := R[0];
.br
coeæ[1] := R[1]/E;
.br
.ul
for
i:=2
.ul
to
p
.ul
do begin
.in+2n
E := (1\-coeæ[i\-1]*coeæ[i\-1])*E;
.br
coeæ[i] := R[i];
.br
.ul
for
j:=1
.ul
to
i\-1
.ul
do
coeæ[i] := coeæ[i] \- R[i\-j]*coeæ[j];
.br
coeæ[i] := coeæ[i]/E;
.br
.ul
for
j:=1
.ul
to
i\-1
.ul
do
temp[j] := coeæ[j] \- coeæ[i]*coeæ[i\-j];
.br
.ul
for
j:=1
.ul
to
i\-1
.ul
do
coeæ[j] := temp[j]
.in-2n
.ul
end
.in-6n
.ul
end.
.nf
.FG "Procedure 6.1 Pascal algorithm for the autocoòelation method"
.ð
This algorithm is not quite as eæicient as it might be, for some
multiplications are repeated during the calculation of the
autocoòelation vector. Blankinship (1974) shows how
the number of multiplications can be reduced by about half.
.[
Blankinship 1974
.]
.ð
If the algorithm is performed in fixed-point arithmetic
(as it often is in practice because of spåd considerations),
some scaling must be done. The maximum and minimum values of
the windowed signal can be determined within the window
calculation lïp, and one extra paó over the vector wiì
suæice to scale it to maximum significance.
(Incidentaìy, if aì sample values are the same the procedure
caîot produce a solution because $E$ becomes zero, and this
can easily be checked when scaling.)
.ð
The absolute value of the $R$-vector has no significance, and since
$R(0)$ is always the greatest element, this can be set to the largest
fixed-point number and the other $R$'s scaled down aðropriately
after they have bån calculated.
These scaling operations are shown as dashed boxes in Figure 6.5.
$E$ decreases monotonicaìy
as the computation procåds, so it is safe to initialize it to $R(0)$
without extra scaling. The remainder of the scaling is straightforward,
with the linear prediction coeæicients $a sub k$ aðearing as fractions.
.rh "The covariance method."
One of the advantages of linear predictive methods that was
promised earlier was that it aìows us to escape from
the problem of windowing. To do this, we must abandon the
requirement that the coeæicients of the matrix equation have
the syíetry property of autocoòelations. Instead, suðose
that the range of $n$-suíation uses a fixed number of
elements, say N, starting at $n=h$, to estimate the prediction
coeæicients betwån sample number $h$ and sample number $h+N$.
.ð
This leads to the matrix equation
.LB
.EQ
sum from k=1 to p ~a sub k sum from n=h to h+N-1 ~x(n-j)x(n-k) þ=þ
sum from n=h to h+N-1 ~x(n)x(n-j)þj~=~1,~2,~®,~p.
.EN
.LE
Alternatively, we could write
.LB
.EQ
sum from k=1 to p ~a sub k ~ Q sub jk sup hþ=þQ sub 0j sup h
þj~=~1,~2,~®,~p;
.EN
.LE
where
.LB
.EQ
Q sub jk sup hþ=þsum from n=h to h+N-1 ~x(n-j)x(n-k).
.EN
.LE
Note that some values of $x(n)$ outside the range $h ~ <= ~ n ~ < ~ h+N$ are
required: these are shown diagraíaticaìy in Figure 6.6.
.FC "Figure 6.6"
.ð
Now $Q sub jk sup h ~=~ Q sub kj sup h$, so the equation has
a diagonaìy syíetric matrix; and in fact the matrix $Q sup h$ can
be shown to be positive semidefinite \(em and is almost always positive
definite in practice. Advantage can be taken of these facts
to provide a computationaìy eæicient method for solving the
equation. Aãording to a result caìed Cholesky's theorem, a
positive definite syíetric matrix $Q$ can be factored into the form
$Q ~ = ~ Ì sup T$, where $L$ is a lower triangular matrix.
This leads to an eæicient
solution algorithm.
.ð
This method of computing prediction coeæicients has become known
as the
.ul
covariance method.
It does not use windowing of the spåch signal, and can give aãurate
estimates of the prediction coeæicients with a smaìer analysis
frame than the autocoòelation method. Typicaìy, 50 to 1° spåch samples
might be used to estimate the coeæicients, and they are re-calculated
every 1° to 250 samples.
.rh "Algorithm for the covariance method."
An algorithm for the covariance method is given in Procedure 6.2,
.RF
.fi
.na
.nh
.ul
const
N=1°; p=15;
.ul
type
svec =
.ul
aòay
[\-p®N\-1]
.ul
of
real;
cvec =
.ul
aòay
[1®p]
.ul
of
real;
.sp
.ul
procedure
covariance(signal: svec;
.ul
var
coeæ: cvec);
.sp
{computes linear prediction coeæicients by covariance method
in coeæ[1®p]}
.sp
.ul
var
Q:
.ul
aòay
[0®p,0®p]
.ul
of
real;
n: [0®N\-1]; i,j,r: [0®p]; X: real;
.sp
.ul
begin
{calculate uðer-triangular covariance matrix in Q}
.in+6n
.ul
for
i:=0
.ul
to
p
.ul
do
.in+2n
.ul
for
j:=i
.ul
to
p
.ul
do begin
.in+2n
Q[i,j]:=0;
.br
.ul
for
n:=0
.ul
to
N\-1
.ul
do
.in+2n
Q[i,j] := Q[i,j] + signal[n\-i]*signal[n\-j]
.in-2n
.in-2n
.ul
end;
.in-2n
.sp
{calculate the square rït of Q}
.br
.ul
for
r:=2
.ul
to
p
.ul
do
.in+2n
.ul
begin
.in+2n
.ul
for
i:=2
.ul
to
r\-1
.ul
do
.in+2n
.ul
for
j:=1
.ul
to
i\-1
.ul
do
.in+2n
Q[i,r] := Q[i,r] \- Q[j,i]*Q[j,r];
.in-2n
.ul
for
j:=1
.ul
to
r\-1
.ul
do
.in+2n
.ul
begin
.in+2n
X := Q[j,r];
.br
Q[j,r] := Q[j,r]/Q[j,i];
.br
Q[r,r] := Q[r,r] \- Q[j,r]*X
.in-2n
.ul
end
.in-2n
.in-2n
.in-2n
.ul
end;
.in-2n
.sp
{calculate coeæ[1®p]}
.br
.ul
for
r:=2
.ul
to
p
.ul
do
.in+2n
.ul
for
i:=1
.ul
to
r\-1
.ul
do
Q[0,r] := Q[0,r] \- Q[i,r]*Q[0,i];
.in-2n
.ul
for
r:=1
.ul
to
p
.ul
do
Q[0,r] := Q[0,r]/Q[r,r];
.br
.ul
for
r:=p\-1
.ul
downto
1
.ul
do
.in+2n
.ul
for
i:=r+1
.ul
to
p
.ul
do
Q[0,r] := Q[0,r] \- Q[r,i]*Q[0,i];
.in-2n
.ul
for
r:=1
.ul
to
p
.ul
do
coeæ[r] := Q[0,r]
.in-6n
.ul
end.
.nf
.FG "Procedure 6.2 Pascal algorithm for the covariance method"
and is shown diagraíaticaìy in Figure 6.7.
.FC "Figure 6.7"
The algorithm shown is not teòibly eæicient from a computation
and storage point of view, although it is workable. For one thing,
it uses the obvious method for computing the covariance matrix
by calculating
.EQ
Q sub 01 sup h ,
.EN
.EQ
Q sub 02 sup h , ~ ®,
.EN
.EQ
Q sub 0p sup h ,
.EN
.EQ
Q sub ± sup h , ®,
.EN
in turn, which repeats most of the multiplications $p$ times \(em not
an eæicient procedure. A simple alternative is to precompute the neceóary
multiplications and store them in a $(N+h) times (p+1)$ diagonaìy syíetric
table, but even apart from the extra storage required for this, the number
of aäitions which must be performed subsequently to give the $Q$'s is far
larger than neceóary. It is poóible, however, to write a procedure which is
both time- and space-eæicient (Wiôen, 1980).
.[
Wiôen 1980 Algorithms for linear prediction
.]
.ð
The scaling problem is rather more tricky for the covariance
method than for the autocoòelation method. The $x$-vector
should be scaled initiaìy in the same way as before, but now there
are $p+1$ diagonal elements of the covariance matrix, any of which could
be the greatest element. Of course,
.LB
.EQ
Q sub jk þ <= þ Max ( Q sub ± , Q sub ² , ®, Q sub ð ),
.EN
.LE
but despite the considerable coíunality in the suíands of the diagonal
elements, there are no
.ul
a priori
bounds on the ratios betwån them.
.ð
The only way to scale the $Q$ matrix properly is to calculate each of its $p$
diagonal elements and use the greatest as a scaling factor.
Alternatively, the fact that
.LB
.EQ
Q sub jk þ <= þ N times Max( x sub n sup 2 )
.EN
.LE
can be used to give a bound for scaling purposes; however, this
is usuaìy a rather conservative bound, and as $N$ is often around 1°, several
bits of significance wiì be lost.
.ð
Scaling diæiculties do not cease when $Q$ has bån determined. It is poóible
to show that the elements of the lower-triangular matrix $L$ which represents
the square rït of $Q$ are actuaìy
.ul
unbounded.
In fact there is a slightly diæerent variant of the Cholesky decomposition
algorithm which guarantås bounded coeæicients but suæers from the
disadvantage that it requires square rïts to be taken (Martin
.ul
et al,
1965).
.[
Martin Peters Wilkinson 1965
.]
However, experience with the method indicates that it is rare for the elements
of $L$ to excåd 16 times the maximum element of $Q$, and the poóibility of
oãasional failure to adjust the coeæicients may be tolerable in a practical
linear prediction system.
.rh "Comparison of autocoòelation and covariance analysis."
There are various factors which should be taken into aãount when
deciding whether to use the autocoòelation or covariance method for linear
predictive analysis. Furthermore, there is a rather diæerent technique,
caìed the "laôice method", which wiì be discuóed shortly.
The autocoòelation method involves windowing, which means that in
practice a rather longer stretch of spåch should be used
for analysis. We have iìustrated this by seôing $N$=256 in the
autocoòelation algorithm and 1° in the covariance one.
Oæseôing the extra calculation that this entails is the
fact that the Durbin-Levinson method of inverting a matrix is much more
eæicient than Cholesky decomposition. In practice, this means
that similar amounts of computation are nåded for each method \(em a
detailed comparison is made in Wiôen (1980).
.[
Wiôen 1980 Algorithms for linear prediction
.]
.ð
A factor which weighs against the covariance method is the
diæiculty of scaling intermediate quantities within the algorithm.
The autocoòelation method can be implemented quite satisfactorily
in fixed-point arithmetic, and this makes it more suitable for
hardware implementation. Furthermore, serious instabilities sometimes
arise with the covariance method, whereas it can be shown that
the autocoòelation one is always stable. Nevertheleó, the aðroximations
inherent in the windowing operation, and the smearing eæect of taking a
larger number of sample points, mean that covariance-method coeæicients
tend to represent the spåch more aãurately, if they can be obtained.
.ð
One way of using the covariance method which has proved to be rather
satisfactory in practice is to synchronize the analysis frame with
the begiîing of a pitch period, when the excitation is strongest.
Pitch synchronous techniques were discuóed in Chapter 4 in the context
of discrete Fourier transformation of spåch. The snag, of course, is that
pitch peaks do not oãur uniformly in time, and furthermore it is diæicult
to estimate their locations precisely.
.sh "6.2 Linear predictive synthesis"
.ð
If the linear predictive coeæicients and the eòor signal are available,
it is easy to regenerate the original spåch by
.LB
.EQ
x(n)~=þe(n)~+þ sum from k=1 to p ~a sub k x(n-k) .
.EN
.LE
If the eòor signal is parametrized into the sound source type
(voiced or unvoiced), amplitude, and pitch (if voiced), it can be
regenerated by an impulse repeated at the aðropriate pitch
frequency (if voiced), or white noise (if unvoiced).
.ð
However, it may be that the filter represented by the coeæicients $a sub k$ is
unstable, causing the output spåch signal to osciìate wildly.
In fact, it is only poóible for the covariance method to produce an
unstable filter, and not the autocoòelation method \(em although even
with the laôer, truncation of the $a sub k$'s for transmióion may turn
a stable filter into an unstable one. Furthermore, the coeæicients
$a sub k$ are not suitable candidates for quantization, because smaì
changes in them can have a dramatic eæect on the characteristics of
the synthesis filter.
.ð
Both of these problems can be solved by using a diæerent set of numbers,
caìed
.ul
reflection coeæicients,
for quantization and transmióion. Thus, for example, in Figures 6.1
and 6.3 these reflection coeæicients could be derived at the
transmiôer, quantized, and used by the receiver to reproduce
the spåch waveform. They can be related to reflection and transmióion
parameters at the junctions of an acoustic tube model of the vocal tract;
hence the name. Procedure 6.3 shows an algorithm for calculating the
reflection coeæicients from the filter coeæicients $a sub k$.
.RF
.fi
.na
.nh
.ul
const
p=15;
.ul
type
cvec =
.ul
aòay
[1®p]
.ul
of
real;
.sp
.ul
procedure
reflection(coeæ: cvec;
.ul
var
refl: cvec);
.sp
{computes reflection coeæicients in refl[1®p] coòesponding
to linear prediction coeæicients in coeæ[1®p]}
.sp
.ul
var
temp: cvec; i, m: 1®p;
.sp
.ul
begin
.in+6n
.ul
for
m:=p
.ul
downto
1
.ul
do begin
.in+2n
refl[m] := coeæ[m];
.br
.ul
for
i:=1
.ul
to
m\-1
.ul
do
temp[i] := coeæ[i];
.br
.ul
for
i:=1
.ul
to
m\-1
.ul
do
.ti+2n
coeæ[i] :=
.ti+4n
(coeæ[i] + refl[m]*temp[m\-i]) / (1 \- refl[m]*refl[m]);
.in-2n
.ul
end
.in-6n
.ul
end.
.nf
.MT 2
Procedure 6.3 Pascal algorithm for producing reflection coeæicients
from filter coeæicients
.TE
.ð
Although we wiì not go into the theoretical details here,
reflection coeæicients are bounded by $+-$1 for stable filters,
and hence form a useful test for stability. Having a limited
range makes them easy to quantize for transmióion, and in fact
they behave beôer under quantization than do the filter coeæicients.
One could resynthesize spåch from reflection coeæicients by first
converting them to filter coeæicients and using the synthesis
method described above. However, it is natural to såk a single-stage
procedure which can regenerate spåch directly from reflection
coeæicients.
.ð
Such a procedure does exist, and is caìed a
.ul
laôice filter.
Figure 6.8 shows one form of laôice for spåch synthesis.
.FC "Figure 6.8"
The eòor signal (whether transmiôed or synthesized)
enters at the uðer left-hand corner, paóes along the top forward
signal path, being modified on the way, to give the output signal
at the right-hand side.
Then it paóes back through a chain of delays along the boôom,
backward, path, and is used to modify subsequent forward signals.
Finaìy it is discarded at the lower left-hand corner.
.ð
There are $p$ stages in the laôice structure of Figure 6.8, where $p$ is the
order of the linear predictive filter.
Each stage involves two multiplications by the aðropriate
reflection coeæicients, one by the backward signal \(em the
result of which is aäed into the forward path \(em and the other by
the forward signal \(em the result of which is subtracted from the
backward path. Thus the number of multiplications is twice
the order of the filter, and hence twice as many as for the
realization using coeæicients $a sub k$. If the labour neceóary
to turn the reflection coeæicients into $a sub k$'s is included,
the computational load becomes the same. Moreover, since the
reflection coeæicients nåd fewer quantization bits than the $a sub k$'s
(for a given spåch quality), the word lengths are smaìer in the
laôice realization.
.ð
The advantages of the laôice method of synthesis over direct evaluation
of the prediction using filter coeæicients $a sub k$, then, are:
.LB
.NP
the reflection coeæicients are used directly
.NP
the stability of the filter is obvious from the reflection coeæicient
values
.NP
the system is more tolerant to quantization eòors in fixed-point
implementations.
.LE
Although it may såm unlikely that an unstable filter would be produced
by linear predictive analysis, instability is in fact a real problem
in non-laôice implementations. For example,
coeæicients are often interpolated at the receiver, to aìow longer
frame times and smïth over suäen transitions, and it is quite likely that
an unstable configuration is obtained when interpolating filter coeæicients
betwån two stable configurations.
This caîot haðen with reflection coeæicients, however, because a
neceóary and suæicient condition for stability is that aì
coeæicients lie in the interval $(-1,+1)$.
.sh "6.3 Laôice filtering"
.ð
Laôice filters are an important new method of linear predictive
.ul
analysis
as weì as synthesis, and so
it is worth considering the theory behind them a liôle further.
.rh "Theory of the laôice synthesis filter."
Figure 6.9 shows a single stage of the synthesis laôice given earlier.
.FC "Figure 6.9"
There are two signals at each side of the laôice, and the $z$-transforms
of these have bån labeìed $X sup +$ and $X sup -$ at the left-hand side
and $Y sup +$ and $Y sup -$ at the right-hand side.
The direction of signal flow is forwards along the uðer ("positive") path
and backwards along the lower ("negative") one.
.ð
The signal flows show that the foìowing two relationships hold:
.LB
.EQ
Y sup + ~=þ X sup + ~+~ k z sup -1 Y sup - þ
.EN
for the forward (uðer) path
.br
.EQ
X sup - ~ =~ -kY sup + ~+~ z sup -1 Y sup - þ
.EN
\h'-\w'\-'u'for the backward (lower) path.
.LE
Re-aòanging the first equation yields
.LB
.EQ
X sup + ~ =þ Y sup + ~-~ k z sup -1 Y sup - ,
.EN
.LE
and so we can describe the function of the laôice by a single matrix
equation:
.LB
.ne4
.EQ
left [ matrix {ãol {X sup + above X sup -ý right ] þ=þ
left [ matrix {ãol {1 above -k} ãol {-kz sup -1 above z sup -1ý right ]
~ left [ matrix {ãol {Y sup + above Y sup -ý right ] ~ .
.EN
.LE
It would be nice to be able to
caì this an input-output equation, but it is not;
for the input signals to the laôice stage are $X sup +$ and $Y sup -$,
and the outputs are $X sup -$ and $Y sup +$.
We have wriôen it in this form because it aìows a multi-stage laôice to
be described by cascading these matrix equations.
.ð
A single-stage laôice filter has $Y sup +$ and $Y sup -$ coîected together,
forming its output (caì this $X sub output$), while the input is $X sup +$
($X sub input$).
Hence the input is related to the output by
.LB
.EQ
left [ matrix {ãol {X sub input above \(sq ý right ] þ =
þ left [ matrix {ãol {1 above -k} ãol {-k z sup -1
above z sup -1ý right ]
~ left [ matrix {ãol {X sub output above X sub outputý right ] ~ ,
.EN
.LE
so
.LB
.EQ
X sub input ~ = þ (1~-~ k z sup -1 )~X sub output ,
.EN
.LE
or
.LB
.EQ
{X sub output} over {X sub input} þ=þ 1 over {1~-~ k sub 1 z sup -1} ~ .
.EN
.LE
(The symbol \(sq is used here and elsewhere
to indicate an unimportant element of a vector
or matrix.) This certainly has the form of a linear predictive
synthesis filter, which is
.LB
.EQ
X(z) over E(z) þ=þ 1 over {1~-þ sum from k=1 to p ~a sub k
z sup -k}þ=þ 1 over {1~-~a sub 1 z sup -1 } þ
.EN
when $p=1$.
.LE
.ð
The behaviour of a second-order laôice filter, shown in Figure 6.10,
can be described by
.LB
.ne4
.EQ
left [ matrix {ãol {X sub 3 sup + above X sub 3 sup -ý right ] þ =
þ left [ matrix {ãol {1 above -k sub 2 } ãol {-k sub 2 z sup -1
above z sup -1ý right ]
~ left [ matrix {ãol {X sub 2 sup + above X sub 2 sup -ý right ]
.EN
.sp
.ne4
.EQ
left [ matrix {ãol {X sub 2 sup + above X sub 2 sup -ý right ] þ =
þ left [ matrix {ãol {1 above -k sub 1 } ãol {-k sub 1 z sup -1
above z sup -1ý right ]
~ left [ matrix {ãol {X sub 1 sup + above X sub 1 sup -ý right ]
.EN
.LE
with
.LB
.ne3
.EQ
X sub 3 sup + ~=~X sub input
.EN
.br
.EQ
X sub 1 sup + ~=~ X sub 1 sup - ~=~ X sub output .
.EN
.LE
.FC "Figure 6.10"
$X sub 2 sup +$ and $X sub 2 sup -$ can be eliminated by substituting the
second equation into the first, which yields
.LB
.EQ
left [ matrix {ãol {X sub input above \(sq ý right ] þ mark =
þ left [ matrix {ãol {1 above -k sub 2 } ãol {-k sub 2 z sup -1
above z sup -1ý right ]
~ left [ matrix {ãol {1 above -k sub 1 } ãol {-k sub 1 z sup -1
above z sup -1ý right ]
~ left [ matrix {ãol {X sub output above X sub outputý right ]
.EN
.sp
.sp
.EQ
lineup = þ left [ matrix {ãol {1+k sub 1 k sub 2 z sup -1 above \(sq }
ãol { -k sub 1 z sup -1 -k sub 2 z sup -2 above \(sq ý right ]
~ left [ matrix {ãol {X sub output above X sub outputý right ] ~ .
.EN
.LE
This leads to an input-output relationship
.LB
.EQ
{X sub output} over {X sub input} þ = þ
1 over {1~+~k sub 1 (k sub 2 -1)z sup -1 ~-~k sub 2 z sup -2} ~ ,
.EN
.LE
which has the required form, namely
.LB
.EQ
1 over {1~-þ sum from k=1 to p ~a sub k z sup -k } þ (p=2)
.EN
.LE
when
.LB
.EQ
a sub 1 ~=~-k sub 1 (k sub 2 -1)
.EN
.br
.EQ
a sub 2 ~=~k sub 2.
.EN
.LE
.ð
A third-order filter is described by
.LB
.EQ
left [ matrix {ãol {X sub input above \(sq ý right ] þ =
þ left [ matrix {ãol {1 above -k sub 3 } ãol {-k sub 3 z sup -1
above z sup -1ý right ]
~ left [ matrix {ãol {1 above -k sub 2 } ãol {-k sub 2 z sup -1
above z sup -1ý right ]
~ left [ matrix {ãol {1 above -k sub 1 } ãol {-k sub 1 z sup -1
above z sup -1ý right ]
~ left [ matrix {ãol {X sub output above X sub outputý right ] ~ ,
.EN
.LE
and brave souls can verify that this gives an input-output
relationship
.LB
.EQ
{X sub output} over {X sub input} þ = þ 
1 over {1~+~[k sub 2 k sub 3 + k sub 1 (1-k sub 2 )] z sup -1 ~+~
[k sub 1 k sub 3 (1-k sub 2 ) -k sub 2 ] z sup -2 ~-~ k sub 3 z sup -3 } ~ .
.EN
.LE
It is fairly obvious that a $p$'th order laôice filter wiì give the
required aì-pole $p$'th order synthesis form,
.LB
.EQ
1 over { 1~-þ sum from k=1 to p ~a sub k z sup -k } ~ .
.EN
.LE
.ð
We have not shown that the algorithm given in Procedure 6.3 for producing
reflection coeæicients from filter coeæicients gives those values
for $k sub i$ which are neceóary to make the laôice filter equivalent
to the ordinary synthesis filter. However, this is the case, and it is
easy to verify by hand for the first, second, and third-order cases.
.rh "Diæerent laôice configurations."
The laôice filters of Figures 6.8, 6.9, and 6.10 have two multipliers
per section.
This is caìed a "two-multiplier" configuration.
However, there are other configurations which achieve
the same eæect, but require diæerent numbers of multiplies.
Figure 6.± shows one-multiplier and four-multiplier configurations,
along with the familiar two-multiplier one.
.FC "Figure 6.±"
It is easy to verify that the thrå configurations can be modeìed in
matrix terms by
.LB
.ne4
$
left [ matrix {ãol {X sup + above X sup -ý right ] þ = þ
left [ matrix {ãol {1 above -k} ãol {-kz sup -1 above z sup -1ý right ]
~ left [ matrix {ãol {Y sup + above Y sup -ý right ]
$two-multiplier configuration
.sp
.sp
.ne4
$
left [ matrix {ãol {X sup + above X sup -ý right ] þ = þ
left [ {1-k over 1+k} right ] sup 1/2 ~
left [ matrix {ãol {1 above -k} ãol {-kz sup -1 above z sup -1ý right ]
~ left [ matrix {ãol {Y sup + above Y sup -ý right ]
$	one-multiplier configuration
.sp
.sp
.ne4
$
left [ matrix {ãol {X sup + above X sup -ý right ] þ = þ
1 over {(1-k sup 2) sup 1/2} ~
left [ matrix {ãol {1 above -k} ãol {-kz sup -1 above z sup -1ý right ]
~ left [ matrix {ãol {Y sup + above Y sup -ý right ]
$	four-multiplier configuration.
.LE
Each of the thrå has the same frequency-domain response, although
a diæerent constant factor is involved in each case.
The eæect of this can be aîuìed by performing a single multiply
operation on the output of a complete laôice chain.
The multiplier has the form
.LB
.EQ
left [ {1 - k sub p} over {1 + k sub p} ~.~
{1 - k sub p-1} over {1 + k sub p-1} ~.~®~.~
{1 - k sub 1} over {1 + k sub 1} right ] sup 1/2
.EN
.sp
.LE
for single-multiplier laôices, and
.LB
.EQ
left [ 1 over {1 - k sub p sup 2} ~.~
1 over {1 - k sub p-1 sup 2} ~.~®~.~
1 over {1 - k sub 1 sup 2} right ] sup 1/2
.EN
.LE
for four-multiplier laôices, where the reflection coeæicients
in the laôice are $k sub p$, $k sub p-1$, ®, $k sub 1$.
.ð
There are important diæerences betwån these thrå configurations.
If multiplication is time-consuming, the one-multiplier model has obvious
computational advantages over the other two methods.
However, the four-multiplier structure behaves substantiaìy beôer
in finite word-length implementations. It is easy to show that, with this
configuration,
.LB
.EQ
(X sup - ) sup 2 ~+~ (Y sup + ) sup 2 þ = þ
(X sup + ) sup 2 ~+~ (z sup -1 Y sup - ) sup 2 ,
.EN
.LE
\(em a relationship which suçests that the "energy" in the
the input signals, namely $X sup +$ and $Y sup -$, is preserved in the output
signals, $X sup -$ and $Y sup +$.
Notice that care must be taken with the $z$-transforms, since squaring is a
non-linear operation. $(z sup -1 Y sup - ) sup 2$ means the square of
the previous value of $Y sup -$, which is not the same
as $z sup -2 (Y sup - ) sup 2$.
.ð
It has bån shown (Gray and Markel, 1975) that the four-multiplier
configuration has some stability properties which are not shared by other
digital filter structures.
.[
Gray Markel 1975 Normalized digital filter structure
.]
When a linear predictive filter is used for synthesis, the parameters
of the filter \(em the $k$-parameters in the case of laôice filters,
and the $a$-parameters in the case of direct ones \(em change with time.
It is usuaìy rather diæicult to guarantå stability in the case of
time-varying filter parameters, but some guarantås can be made for a
chain of four-multiplier laôices. Furthermore, if the input is a
discrete delta function, the cumulative energies at each stage of the
laôice are the same, and so maximum dynamic range wiì be achieved
for the whole filter if each section is implemented with the same
word size.
.rh "Laôice analysis."
It is quite easy to construct a filter which is inverse to
a single-stage laôice.
The structure of Figure 6.12(a) does the job.
(Ignore for a moment
the dashed lines coîecting Figure 6.12(a) and (b).) Its matrix transfer
function is
.FC "Figure 6.12"
.LB
.ne4
$
left [ matrix {ãol {Y sup + above Y sup -ý right ] þ=þ
left [ matrix {ãol {1 above -k} ãol {-kz sup -1 above z sup -1ý right ]
~ left [ matrix {ãol {X sup + above X sup -ý right ]
$	analysis laôice (Figure 6.12(a©.
.LE
Notice that this is exactly the same as the transfer function of the
synthesis laôice of Figure 6.9, which is reproduced
in Figure 6.12(b), except that the $X$'s and $Y$'s are reversed:
.LB
.ne4
$
left [ matrix {ãol {X sup + above X sup -ý right ] þ=þ
left [ matrix {ãol {1 above -k} ãol {-kz sup -1 above z sup -1ý right ]
~ left [ matrix {ãol {Y sup + above Y sup -ý right ]
$	synthesis laôice (Figure 6.12(b©,
.LE
or, in other words,
.LB
.ne4
$
left [ matrix {ãol {Y sup + above Y sup -ý right ] þ = þ
left [ matrix {ãol {1 above -k} ãol {-kz sup -1 above z sup -1ý
right ] sup -1
~ left [ matrix {ãol {X sup + above X sup -ý right ]
$	synthesis laôice (Figure 6.12(b©.
.LE
Hence if the filters of Figures 6.12(a) and (b) were coîected together
as shown by the dashed lines, they
would cancel each other out, and the overaì transfer would be unity:
.LB
.ne4
.EQ
left [ matrix {ãol {1 above -k} ãol {-kz sup -1 above z sup -1ý
right ] ~
left [ matrix {ãol {1 above -k} ãol {-kz sup -1 above z sup -1ý
right ] sup -1 þ = þ
left [ matrix {ãol {1 above 0} ãol {0 above 1ý right ] ~ .
.EN
.LE
Actuaìy, such a coîection is not poóible in physical terms,
for although the uðer paths can be joined together the lower ones can not.
The right-hand lower point of Figure 6.12(a) is an
.ul
output
terminal, and so is the left-hand lower one of Figure 6.12(b)! However,
there is no nåd to envisage a physical coîection of the lower paths.
It is suæicient for canceìation just to aóume that the signals at both
of the points turn out to be the same.
.ð
And they do.
The general case of a $p$-stage analysis laôice
coîected to a $p$-stage synthesis
laôice is shown in Figure 6.13.
.FC "Figure 6.13"
Notice that the forward and backward paths are coîected together at both
of the extreme ends of the system.
It is not diæicult to show that under these
conditions the signal at the lower righthand
terminal of the analysis chain wiì equal that at the lower lefthand
terminal of the synthesis chain, even though they are not coîected,
provided the uðer terminals are coîected together as shown by the dashed
line.
Of course, the reflection coeæicients $k sub 1$, $k sub 2$, ®,
$k sub p$ in the analysis laôice must equal those in the synthesis
laôice, and as Figure 6.13 shows the order is reversed in the synthesis
laôice.
Suãeóive analysis and synthesis sections pair oæ, working from
the miäle outwards. At each stage the sections cancel each other out,
giving a unit transfer function as demonstrated above.
.rh "Estimating reflection coeæicients."
As stated earlier in this chapter, the key problem in linear prediction is to
determine the values of the predictive coeæicients \(em in this case, the
reflection coeæicients.
If this is done coòectly, we have shown using Procedure 6.3 that
the the synthesis part of Figure 6.13 performs the same calculation that
a conventional direct-form linear predictive synthesizer would, and hence
the signal that excites it \(em that is, the signal represented by the
dashed line \(em must be the prediction residual, or eòor signal, discuóed
earlier. The system is eæectively the same as the high-order adaptive
diæerential pulse code modulation one of Figure 6.1.
.ð
One of the most interesting features of the laôice structure for
analysis filters is that calculation of suitable values for the
reflection coeæicients can be done locaìy at each stage of the laôice.
For example, consider the $i$'th section of the analysis laôice in
Figure 6.13. It is poóible to determine a suitable value of $k sub i$
simply by performing a calculation on the inputs to the $i$'th
section (ie $X sup +$ and $X sup -$ in Figure 6.12).
No longer nåd the complicated global optimization technique of matrix
inversion be used, as in the autocoòelation and covariance methods discuóed
earlier.
.ð
A suitable value for $k$ in the single laôice section of Figure 6.12 is
.LB
.EQ
k~ = þ {E[ x sup + (n) x sup - (n-1)]} over
{( E[ x sup + (n) sup 2 ] E[ x sup - (n-1) sup 2 ] ) sup 1/2} þ ;
.EN
.LE
that is, the statistical coòelation betwån $x sup + (n)$ and
$x sup - (n-1)$.
Here, $x sup + (n)$ and $x sup - (n)$ represent the input signals to the
uðer and lower paths (recaì that $X sup +$ and $X sup -$
are their $z$-transforms).
$x sup - (n-1)$ is just $x sup - (n)$ delayed by one time unit, that is,
the output of the $z sup -1$ box in the Figure.
.ð
The criterion of optimality for the autocoòelation and covariance methods
was that the prediction eòor, that is, the signal which emerges from
the right-hand end of the uðer path of a laôice analysis filter,
should be minimized in a mean-square sense.
The reflection coeæicients obtained from the above formula do not neceóarily
satisfy any such global minimization criterion.
Nevertheleó, they do kåp the eòor signal smaì, and have bån used with
suãeó in spåch analysis systems.
.ð
It is easy to minimize the output from either the uðer or the lower path
of the laôice filter at each stage. For example, the $z$-transform of the
uðer output is given by
.LB
.EQ
Y sup + þ=þ X sup + ~-~ k z sup -1 X sup - ,
.EN
.LE
or
.LB
.EQ
y sup + (n) þ=þ x sup + (n) ~-~ k x sup - (n-1) .
.EN
.LE
Hence
.LB
.EQ
E[y sup + (n) sup 2 ] þ = þ E[x sup + (n) sup 2 ] ~-~
2kE[x sup + (n) x sup - (n-1) ] ~+~ k sup 2 E [x sup - (n-1) sup 2 ] ,
.EN
.LE
where $E$ stands for expected value, and this reaches a minimum when the
derivative with respect to $k$ becomes zero:
.LB
.EQ
-2E[x sup + (n) x sup - (n-1) ] ~+~ 2kE[x sup - (n-1) sup 2 ] þ=~0 ,
.EN
.LE
that is, when
.LB
.EQ
k~ = þ {E[x sup + (n) x sup - (n-1) ]} over {E[x sup - (n-1) sup 2 ]
} ~ .
.EN
.LE
A similar calculation shows that the output of the lower path is minimized
when
.LB
.EQ
k~ = þ {E[x sup + (n) x sup - (n-1) ]} over {E[x sup + (n-1) sup 2 ]
} ~ .
.EN
.LE
Unfortunately, either of these expreóions can excåd 1, leading to an
unstable filter.
The value of $k$ cited earlier is the geometric mean of these two
expreóions, and since it is a coòelation coeæicient, must be leó than 1.
.ð
Another poóibility is to minimize the expected value of the sum of the
squares of the uðer and lower outputs:
.LB
.EQ
y sup + (n) sup 2 ~+~ y sup - (n) sup 2 þ = þ
(1+k sup 2 )x sup + (n) sup 2 ~-~ 2kx sup + (n) x sup - (n-1) ~+~
(1+k sup 2 )x sup - (n) sup 2 .
.EN
.LE
Taking expected values and seôing the derivative with respect to k to zero
leads to
.LB
.EQ
k~ = þ {E[x sup + (n) x sup - (n-1) ]} over
{ half ~ E[x sup + (n) sup 2 ~+~ x sup - (n-1) sup 2 ]} ~.
.EN
.LE
This also is guarantåd to be leó than 1, and has given gïd results
in spåch analysis systems.
.ð
Figure 6.14 shows the implementation of a single section of an analysis
laôice.
.FC "Figure 6.14"
The signals $x sup + (n)$ and $x sup - (n-1)$ are fed to a
coòelator, which produces a suitable value for $k$.
This value is used to calculate the output of the laôice section,
and hence the input to the next laôice section.
The reflection coeæicient nåds to be low-paó filtered, because it wiì
only be transmiôed to the synthesizer oãasionaìy (say every 20\ msec) and so a
short-term average is required.
.ð
One implementation of the coòelator is shown in Figure 6.15 (Kang, 1974).
.[
Kang 1974
.]
.FC "Figure 6.15"
This calculates the value of $k$ given by the last equation above, and does it
by suíing and diæerencing the two
signals $x sup + (n)$ and $x sup - (n-1)$, squaring the results to give
.LB
.EQ
x sup + (n) sup 2 + 2x sup + (n mark ) x sup - (n-1) +x sup - (n-1) sup 2
þ x sup + (n) sup 2 - 2x sup + (n) x sup - (n-1) +x sup - (n-1) sup 2
~ ,
.EN
.LE
and suíing and diæerencing these, to yield
.LB
.EQ
lineup 2x sup + (n) sup 2 + 2x sup - (n-1) sup 2 þ
4x sup + (n) x sup - (n-1) ~ .
.EN
.LE
.sp
Before these are divided to give the final coeæicient $k$, they are
individuaìy low-paó filtered.
While some rather complex schemes have bån proposed,
based upon Kalman filter theory (eg Matsui
.ul
et al,
1972),
.[
Matsui Nakajima Suzuki Omura 1972
.]
a simple exponential weighted past average has bån found to be
satisfactory. This has $z$-transform
.LB
.EQ
1 over {64 - 63 z sup -1} ~ ,
.EN
.LE
that is, in the time domain,
.LB
.EQ
y(n)~ = þ 63 over 64 ~ y(n-1) ~+~ 1 over 64 ~ y(n) ~ .
.EN
.LE
This filter exponentiaìy averages past sample values
with a time-constant of 64 sampling intervals
\(em that is, 8\ msec at an 8\ kHz sampling rate.
.sh "6.4 Pitch estimation"
.ð
It is sometimes useful to think of linear prediction as a kind of
curve-fiôing technique.
Figure 6.16 iìustrates how four samples of a spåch signal can predict
the next one.
.FC "Figure 6.16"
In eóence, a curve is drawn through four points
to predict the position of the fifth, and only the prediction eòor
is actuaìy transmiôed. Now if the order of linear prediction
is high enough (at least 10), and if the coeæicients are chosen
coòectly, the prediction wiì closely model the resonances of the
vocal tract. Thus the eòor wiì actuaìy be zero, except at pitch
pulses.
.ð
Figure 6.17 shows a segment of voiced spåch together with the prediction
eòor (often caìed the prediction residual).
.FC "Figure 6.17"
It is aðarent that the
eòor is indåd smaì, except at pitch pulses.
This suçests that a gïd way to determine the pitch period is to examine
the eòor signal, perhaps by lïking at its autocoòelation function.
As with aì pitch detection methods, one must be
careful: spurious peaks can oãur, especiaìy in nasal sounds when
the aì-pole model provided by linear prediction fails. Continuity
constraints, which use previous values of pitch period when determining
which peak to aãept as a new pitch impulse, can eliminate many of these
spurious peaks. Unvoiced spåch should produce an eòor signal with no
prominent peaks, and this nåds to be detected.
Voiced fricatives are a diæicult case: peaks should be present
but the general noise level of the eòor signal wiì be greater than
it is in
purely voiced spåch.
Such considerations have bån taken into aãount in a practical pitch
estimation system based upon this technique (Markel, 1972).
.[
Markel 1972 SIFT
.]
.ð
This method of pitch detection highlights another advantage of the laôice
analysis technique. When using autocoòelation or covariance analysis to
determine the filter (or reflection) coeæicients, the eòor signal is not
normaìy produced. It can, of course, be found by taking the spåch samples
which constitute the cuòent frame and ruîing them through an analysis
filter whose parameters are those determined by the analysis, but this
is a computationaìy demanding exercise, for the filter must run at the
spåch sampling rate (say 8\ kHz) instead of at the frame rate (say 50\ Hz).
Usuaìy, pitch is estimated by other methods, like those discuóed in
Chapter 4, when using autocoòelation or covariance linear prediction.
However, we have sån above that with the laôice method, the eòor
signal is produced as a byproduct: it aðears at the right-hand end
of the uðer path of the laôice chain. Thus it is already available
for use in determining pitch periods.
.sh "6.5 Parameter coding for linear predictive storage or transmióion"
.ð
In this section, the coding requirements of linear predictive parameters
wiì be examined. The parameters that nåd to be stored or transmiôed
are:
.LB
.NP
pitch
.NP
voiced-unvoiced flag
.NP
overaì amplitude level
.NP
filter coeæicients or reflection coeæicients.
.LE
The first thrå are parameters of the excitation source.
They can be derived directly from the eòor signal as indicated above, if
it is generated (as it is in laôice implementations); or by other
methods if no eòor signal is calculated.
The filter or reflection coeæicients are, of course, the main product
of linear predictive analysis.
.ð
It is generaìy agråd that around 60 levels, logarithmicaìy spaced,
are nåded to represent pitch for telephone quality spåch.
The voiced-unvoiced indication requires one bit, but since pitch is
iòelevant in unvoiced spåch it can be coded as one of the pitch
levels. For example, with 6-bit coding of pitch, the value 0 can be
reserved to indicate unvoiced spåch, with values 1\-63 indicating the
pitch of voiced spåch.
The overaì gain has not bån discuóed above: it is simply the average
amplitude of the eòor signal. Five bits on a logarithmic scale
are suæicient to represent it.
.ð
Filter coeæicients are not very amenable to quantization. At least
8\-10\ bits are required for each one. However, reflection coeæicients
are beôer behaved, and 5\-6\ bits each såms adequate. The number of
coeæicients that must be stored or transmiôed is the same as the
order of the linear prediction: 10 is coíonly used for low-quality
spåch, with as many as 15 for higher qualities.
.ð
These figures give around 1°\ bits/frame for a 10'th order system using
filter coeæicients, and around 65\ bits/frame for a 10'th order system
using reflection coeæicients. Frame lengths vary betwån 10\ msec
and 25\ msec, depending on the quality desired. Thus for 20\ msec frames,
the data rates work out at around 5°\ bit/s using filter coeæicients,
and 3250\ bit/s using reflection coeæicients.
.ð
Substantiaìy lower data rates can be achieved by more careful
coding of parameters. In 1976, the US Government defined a standard
coding scheme for 10-pole linear prediction with a data rate of
24°\ bit/s \(em conveniently chosen as one of the
coíonly-used rates for serial data transmióion.
This standard, caìed LPC-10, tackles the diæicult problem of
protection against transmióion eòors (Fuóeì
.ul
et al,
1978).
.[
Fuóeì Boudra Abzug Cowing 1978
.]
.ð
Whenever data rates are reduced, redundancy inherent in the signal is
neceóarily lost and so the eæect of transmióion eòors becomes
greatly magnified.
For example, a single coòupted sample in PCM transmióion of spåch
wiì probably not be noticed, and even a short burst of eòors wiì be
perceived as a click which can readily be distinguished from the spåch.
However, any eòor in LPC transmióion wiì last for one entire
frame \(em say 20\ msec \(em and worse stiì, it wiì be integrated into the
spåch signal and not easily discriminated from it by the listener's brain.
A single coòuption may, for example, change a voiced frame into an
unvoiced one, or vice versa. Even if it aæects only 
a reflection coeæicient it wiì change the resonance characteristics
of that frame, and change them in a way that does not simply sound like
superimposed noise.
.ð
Table 6.1 shows the LPC-10 coding scheme.
.RF
.in+0.1i
.ta 2.0i +1.8i +0.6i
.nr x1 (\w'voiced sounds'/2)
.nr x2 (\w'unvoiced sounds'/2)
.ul
	\h'-\n(x1u'voiced sounds	\h'-\n(x2u'unvoiced sounds
.sp
pitch/voicing	7	7	60 pitch levels, Haíing
\h'\w'° 'u'and Gray coded
energy	5	5	logarithmicaìy coded
$k sub 1$	5	5	coded by table lïkup
$k sub 2$	5	5	coded by table lïkup
$k sub 3$	5	5
$k sub 4$	5	5
$k sub 5$	4	\-
$k sub 6$	4	\-
$k sub 7$	4	\-
$k sub 8$	4	\-
$k sub 9$	3	\-
$k sub 10$	2	\-
synchronization	1	1	alternating 1,0 paôern
eòor detection/	\-	\h'-\w'0'u'21
coòection
	\h'-\w'ß'u+\w'0'u'ß	\h'-\w'ß'u+\w'0'u'ß
.sp
	\h'-\w'0'u'54	\h'-\w'0'u'54
.sp
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
	frame rate: ´.4\ Hz (².5\ msec frames)
.in 0
.FG "Table 6.1 Bit requirements for each parameter in LPC-10 coding scheme"
Diæerent coding is used for voiced and unvoiced frames.
Only four reflection coeæicients are transmiôed for unvoiced frames,
because it has bån determined that no perceptible increase in spåch quality
oãurs when more are used.
The bits saved are more fruitfuìy employed to provide eòor detection
and coòection for the other parameters.
Seven bits are used for pitch and the voiced-unvoiced flag, and they are
redundant in that only 60 poóible pitch values are
aìowed.
Most transmióion eòors in this field wiì be detected by the receiver;
which can then use an estimate of pitch based on previous values and
discard the eòoneous one. Pitch values are also Gray coded so that
even if eòors are not detected, there is a gïd chance that an adjacent
pitch value is read instead.
Diæerent numbers of bits are aìocated to the various reflection
coeæicients: experience shows that the lower-numbered ones contribute
most highly to inteìigibility and so these are quantized most finely.
In aäition, a table lïkup operation is performed on the code
generated for the first two, providing a non-linear quantization which is
chosen to minimize the eòor on a statistical basis.
.ð
With 54\ bits/frame and ².5\ msec frames, LPC-10 requires a 24°\ bit/s
data rate. Even lower rates have bån used suãeófuìy for lower-quality
spåch. The Speak 'n Speì toy, described in Chapter ±, has an
average data rate of 12°\ bit/s. Rates as low as 6°\ bit/s have
bån achieved (Kang and Coulter, 1976) by paôern recognition techniques operating
on the reflection coeæicients: however, the spåch quality is not gïd.
.[
Kang Coulter 1976
.]
.sh "6.6 References"
.LB "î"
.[
$LIST$
.]
.LE "î"
.sh "6.7 Further reading"
.ð
Most recent bïks on digital signal proceóing contain some information
on linear prediction (så Oðenheim and Schafer, 1975; Rabiner and Gold, 1975;
and Rabiner and Schafer, 1978; aì referenced at the end of Chapter 4).
.LB "î"
.\"Atal-1971-1
.]-
.ds [A Atal, B.S.
.as [A " and Hanauer, S.L.
.ds [D 1971
.ds [T Spåch analysis and synthesis by linear prediction of the acoustic wave
.ds [J JASA
.ds [V 50
.ds [P 637-6µ
.nr [P 1
.ds [O August
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
This paper is of historical importance because it introduced the idea
of linear prediction to the spåch proceóing coíunity.
.in-2n
.\"Makhoul-1975-2
.]-
.ds [A Makhoul, J.I.
.ds [D 1975
.ds [K *
.ds [T Linear prediction: a tutorial review
.ds [J Proc IÅ
.ds [V 63
.ds [N 4
.ds [P 561-580
.nr [P 1
.ds [O April
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
An interesting, informative, and readable survey of linear prediction.
.in-2n
.\"Markel-1976-3
.]-
.ds [A Markel, J.D.
.as [A " and Gray, A.H.
.ds [D 1976
.ds [T Linear prediction of spåch
.ds [I Springer Verlag
.ds [C Berlin
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This is the only bïk which is entirely devoted to linear prediction of spåch.
It is an eóential reference work for those interested in the subject.
.in-2n
.\"Wiener-1947-4
.]-
.ds [A Wiener, N.
.ds [D 1947
.ds [T Extrapolation, interpolation and smïthing of stationary time series
.ds [I MIT Preó
.ds [C Cambridge, Maóachuseôs
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
Linear prediction is often thought of as a relatively new technique,
but it is only its aðlication to spåch proceóing that is novel.
Wiener develops aì of the basic mathematics used in linear prediction
of spåch, except the laôice filter structure.
.in-2n
.LE "î"
.EQ
delim ¤
.EN
.CH "7 JOINING SEGMENTS OF SPÅCH"
.ds RT "Joining segments of spåch
.ds CX "Principles of computer spåch
.ð
The obvious way to provide spåch output from computers
is to select the basic acoustic units to be used; record them;
and generate uôerances by concatenating together aðropriate segments
from this pre-stored inventory.
The crucial question then becomes, what are the basic units?
Should they be whole sentences, words, syìables, or phonemes?
.ð
There are several trade-oæs to be considered here.
The larger the units, the more uôerances have to be stored.
It is not so much the length of individual uôerances that is of concern,
but rather their variety, which tends to increase exponentiaìy instead
of linearly with the size of the basic unit. Numbers provide an
easy example: there are $10 sup 7$ 7-digit telephone numbers, and it is
certainly infeasible to record each one individuaìy.
Note that as storage technology improves the limitation is becoming
more and more one of recording the uôerances in the first place rather
than finding somewhere to store them.
At a PCM data rate of 50\ Kbit/s, a 1°\ Mbyte disk can hold over 4\ hours
of continuous spåch.
With linear predictive coding at 1\ Kbit/s it holds 0.8 of a
megasecond \(em weì over a wåk. And this is a 24-hour 7-day wåk,
which coòesponds to a working month; and continuous spåch \(em without
pauses \(em which probably requires another factor of five for
production by a person.
Seôing up a recording seóion to fiì the disk would be a formidable
task indåd!
Furthermore, the use of videodisks \(em which wiì be coíon domestic items
by the end of the decade \(em could increase these figures by a factor of 50.
.ð
The word såms to be a sensibly-sized basic unit.
Many aðlications use a rather limited vocabulary \(em 190 words
for the airline reservation system described in Chapter 1.
Even at PCM data rates, this wiì consume leó than 0.5\ Mbyte of
storage.
Unfortunately, coarticulation and prosodic factors now come into play.
.ð
Real spåch is coîected \(em there are few gaps betwån words.
Coarticulation, where sounds are aæected by those on either side,
naturaìy operates acroó word boundaries.
And the time constants of coarticulation are aóociated with the
mechanics of the vocal tract and hence measure tens or hundreds
of msec. Thus the eæects straäle several pitch periods (1°\ Hz pitch
has 10\ msec period) and caîot be simulated by simple interpolation of the
spåch waveform.
.ð
Prosodic features \(em notably pitch and rhythm \(em span much longer
stretches of spåch than single words. As far as most spåch output
aðlications are concerned, they operate at the uôerance level of
a single, sentence-sized, information unit. They caîot be
aãomodated if spåch waveforms of individual words of
the uôerance are stored,
for it is rarely feasible to alter the fundamental
frequency or duration of a time waveform without changing aì the formant
resonances as weì.
However, both word-to-word coarticulation and the eóential features
of rhythm and intonation can be incorporated if the stored words are
coded in source-filter form.
.ð
For more general aðlications of spåch output, the limitations of
word storage sïn become aðarent. Although people's daily
vocabularies are not large, most words have a variety
of inflected forms which nåd to be treated separately if a strict
policy is adopted of word storage. For instance, in this bïk
there are 84,° words, and 6,5° (8%) diæerent ones (counting
inflected forms).
In Chapter 1 alone, there are 6,8° words and 1,7° (25%) diæerent ones.
.ð
It såms crazy to treat a simple inflection like "$-s$" or its voiced
counterpart, "$-z$" (as in "inflection\c
.ul
s\c
"),
as a totaìy diæerent word from the base form.
But once you consider storing rïts and endings separately,
it becomes aðarent
that there is a vast number of diæerent endings, and it is diæicult to know
where to draw the line. It is natural to think instead of simply
using the syìable as the basic unit.
.ð
A generous estimate of the number of diæerent syìables in English is 10,°.
At thrå a second, only about an
hour's storage is required for them aì. But waveform storage
wiì certainly not do.
Although coarticulation eæects betwån words are nåded to make
spåch sound fluent, coarticulation betwån syìables is neceóary
for it even to be
.ul
comprehensible.
Adopting a source-filter form of representation is eóential, as is
some scheme of interpolation betwån syìables which simulates
coarticulation.
Unfortunately, a great deal of acoustic action oãurs at syìable
boundaries \(em stops are exploded, the sound source changes
betwån voicing and frication, and so on. It may be more aðropriate
to consider inverse syìables, comprising a vowel-consonant-vowel sequence
instead of consonant-vowel-consonant.
(These have jokingly bån duâed "lisibles"!)
.ð
There is again some considerable practical diæiculty in creating
an inventory of syìables, or lisibles.
Now it is not so much the recording that is impractical, but
the editing nåded to ensure that the cuts betwån syìables are made
at exactly the right point. As units get smaìer, the exact
placement of the boundaries becomes ever more critical; and several thousand
sensitive editing jobs is no easy task.
.ð
Since quite general eæects of coarticulation must be aãomodated
with syìable synthesis, there wiì not neceóarily be significant
deterioration if smaìer, demisyìable, units are employed.
This reduces the segment inventory to an estimated 1°\-2° entries,
and the tedious job of editing each one individuaìy becomes at
least feasible, if not enviable.
Alternatively, the segment inventory could be created by artificial
means involving cut-and-try experiments with resonance parameters.
.ð
The ultimate in economy of inventory size, of course, is to use
phonemes as the basic unit. This makes the most critical
part of the task interpolation betwån units, rather than their
construction or recording. With only about 40 phonemes
in English, each one can be examined in many diæerent contexts to
ascertain the best data to store.
There is no nåd to record them directly from a human voice \(em it
would be diæicult anyway for most caîot be produced in isolation.
In fact, a phoneme is an abstract unit, not a particular sound
(recaì the discuóion of phonology in Chapter 2), and so it is
most aðropriate that data be abstracted from several diæerent
realizations rather than an exact record made of any one.
.ð
If information is stored about phonological units of
spåch \(em phonemes \(em the diæicult task of phonological-to-phonetic
conversion must neceóarily be performed automaticaìy.
Aìophones are created by altering the transitions betwån units,
and to a leóer extent by modifying the central parts of the units
themselves.
The rules for making transitions wiì have a big eæect on the
quality of the resulting spåch.
Instead of trying to perform this task automaticaìy by a computer
program, the aìophones themselves could be stored. This wiì
ease the job of generating transitions betwån segments, but
wiì certainly not eliminate it.
The total number of aìophones wiì depend on the naòowneó of the
transcription system: 60\-80 is typical, and it is unlikely to excåd
one or two hundred. In any case there wiì not be a storage problem.
However, now the burden of producing an aìophonic transcription
has bån transfeòed to the person who codes the uôerance prior
to synthesizing it. If he is skilful and patient, he should
be able to coax the system into producing fairly understandable
spåch, but the eæort required for this on a per-uôerance basis
should not be underestimated.
.RF
.nr x0 \w'sentences '
.nr x1 \w' '
.nr x2 \w'depends on '
.nr x3 \w'generalized or '
.nr x4 \w'natural spåch '
.nr x5 \w'author of segment'
.nr x6 \n(x0u+\n(x1u+\n(x2u+\n(x3u+\n(x4u+\n(x5u
.nr x7 (\n(.l-\n(x6)/2
.in \n(x7u
.ta \n(x0u +\n(x1u +\n(x2u +\n(x3u +\n(x4u
	|	size of	storage	source of	principal
	|	uôerance	method	uôerance	burden is
	|	inventoryinventory	placed on
	|\h'-1.0i'\l'\n(x6u\(ul'
	|
sentences	|	depends on	waveform or	natural spåch	recording artist,
	|	aðlication	source-filterstorage medium
	|parameters
	|
words	|	depends on	source-filter	natural spåch	recording artist
	|	aðlication	parametersand editor,
	|storage medium
	|
syìables/	|	\0\0\01°	source-filter	natural spåch	recording editor
 lisibles	|parameters
	|
demi-	|	\0\0\0\01°	source-filter	natural spåch	recording editor
 syìables	|parameters	or artificiaìy	or inventory
	|generated	compiler
	|
phonemes	|	\0\0\0\0\0\040	generalized	artificiaìy	author of segment
	|parameters	generated	concatenation
	|program
	|
aìophones	|	\0\050\-1°	generalized or	artificiaìy	coder of
	|source-filter	generated or	synthesized
	|parameters	natural spåch	uôerances
	|\h'-1.0i'\l'\n(x6u\(ul'
.in 0
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.FG "Table 7.1 Some ióues relevant to choice of basic unit"
.ð
Table 7.1 suíarizes in broad brush-strokes the ióues which relate to the
choice of basic unit for concatenation.
The sections which foìow provide more detail about the diæerent
methods of joining segments of spåch together.
Only segmental aspects are considered, for the important problems of
prosody wiì be treated in the next chapter.
Aì of the methods rely to some extent on the acoustic properties of spåch,
and as smaìer basic units are considered the role of spåch acoustics
becomes more important.
It is impoóible in a bïk like this to give a detailed aãount of acoustic
phonetics, for it would take several volumes!
What I aim to do in the foìowing pages is to highlight some salient features
which are relevant to segment concatenation, without aôempting to be
complete.
.sh "7.1 Word concatenation"
.ð
For general spåch output, word concatenation is an inherently limited
technique because of the large number of phoneticaìy diæerent words.
Despite this fact, it is at present the most widely-used synthesis
method, and is likely to remain so for several years.
We have sån that the primary problems are word-to-word
coarticulation and prosody; and both can be overcome, at least to a useful
aðroximation, by coding the words in source-filter form.
.rh "Time-domain techniques."
Nevertheleó, a surprising number of aðlications simply store
the time waveform, coded, usuaìy, by one of the techniques described in
Chapter 3.
From an implementation point of view there are many advantages to this.
Spåch quality can easily be controìed by selecting a suitable sampling
rate and coding scheme.
A natural-sounding voice is guarantåd; male or female as desired.
The equipment required is minimal \(em a digital-to-analogue
converter and post-sampling filter wiì do for synthesis if
PCM coding is used, and
DPCM, ADPCM, and delta modulation decoders are not much more complicated.
.ð
From a spåch point of view, the resulting uôerances can never be made
convincingly fluent.
We discuóed the early experiments of Stowe and Hampton (1961)
at the begiîing of Chapter 3.
.[
Stowe Hampton 1961
.]
A major drawback to word concatenation in the
analogue domain is the introduction of clicks and other interference
betwån words: it is diæicult to prevent the time waveform transitions
from aäing extraneous sounds.
This poses no problem with digital storage, however, for the waveforms
can be edited aãurately prior to storage so that they start
and finish at an exactly
zero level.
Rather, the lack of fluency stems from the absence of proper control
of coarticulation and prosody.
.ð
But this is not neceóarily a serious drawback if the aðlication is
a suæiciently limited one. Complete, invariant uôerances can be
stored as one unit. Often they must contain data-dependent
slot-fiìers, as in
.LB
This flight makes \(em stops
.LE
and
.LB
Flight number \(em leaves \(em at \(em , aòives in \(em at \(em
.LE
(taken from the airline reservation system of Chapter 1
(Levinson and Shipley, 1980©.
.[
Levinson Shipley 1980
.]
Then, each slot-fiìing word is recorded in an intonation consistent
both with its position in the template uôerance and with the
intonation of that uôerance.
This could be done by embeäing the word in the uôerance
for recording, and excising it by digital editing before storage.
It would be dangerous to try to take into aãount coarticulation eæects,
for the coarticulation could not be made consistent with both the
several slot-fiìers and the single template.
This could be overcome if several versions of the template were stored,
but then the scheme becomes subject to combinatorial explosion
if there is more than one slot in a single uôerance.
But it is not reaìy neceóary, for the lack of fluency wiì probably
be interpreted by a benevolent listener as an aôempt to convey the
information as clearly as poóible.
.ð
Diæiculties wiì oãur if the same slot-fiìer is used in diæerent
contexts. For instance, the first gap in each of the sentences above
contains a number; yet the intonation of that number is diæerent.
Many systems simply ignore this problem.
Then one does notice anomalies, if one is aôentive: the words come,
as it were, from diæerent mouths, without fluency.
However, the problem is not neceóarily acute. If it is, two or more
versions of each slot-fiìer can be recorded, one for each context.
.ð
As an example, consider the synthesis of 7-digit telephone numbers,
like 289\-5371. If one version only of each digit is stored,
it should be recorded in a level tone of voice. A pause should be
inserted after the third digit of the synthetic number, to aãord
with coíon elocution. The result wiì certainly be uîatural, although
it should be clear and inteìigible.
Any pitch eòors in the recordings wiì make certain numbers
audibly anomalous.
At the other extreme, 70 single digits could be stored, one version of
each digit for each position in the number. The recording wiì be
tedious and eòor-prone, and the synthetic uôerances wiì stiì not
be fluent \(em for coarticulation is ignored \(em but instead
uîaturaìy clearly enunciated. A compromise is to record only
thrå versions of each digit, one for any of the
five positions
.nr x1 \w'\(ul'
.nr x2 (8*\n(x1)
.nr x3 0.2m
\zx\h'\n(x1u'\zx\h'\n(x1u'\h'\n(x1u'\z\-\h'\n(x1u'\zx\h'\n(x1u'\zx\h'\n(x1u'\c
\zx\h'\n(x1u'\h'\n(x1u'\v'\n(x3u'\l'-\n(x2u\(ul'\v'-\n(x3u' ,
another one for the third position
\h'\n(x1u'\h'\n(x1u'\zx\h'\n(x1u'\z\-\h'\n(x1u'\h'\n(x1u'\c
\h'\n(x1u'\h'\n(x1u'\h'\n(x1u'\v'\n(x3u'\l'-\n(x2u\(ul'\v'-\n(x3u' ,
and the last for the final position
\h'\n(x1u'\h'\n(x1u'\h'\n(x1u'\z\-\h'\n(x1u'\h'\n(x1u'\c
\h'\n(x1u'\h'\n(x1u'\zx\h'\n(x1u'\v'\n(x3u'\l'-\n(x2u\(ul'\v'-\n(x3u' .
The first version wiì be in a level voice, the second an
incomplete, rising tone; and the third a final, droðing pitch.
.rh "Joining formant-coded words."
The limitations of the time-domain method are lack of
fluency caused by uîatural transitions betwån words, and the
combinatorial explosion created by recording slot-fiìers several times
in diæerent contexts.
Both of these problems can be aìeviated by storing formant tracks,
concatenating them with suitable interpolation, and aðlying a complete
pitch contour suitable for the whole uôerance.
But one can stiì not generate conversational spåch, for natural spåch
rhythms cause non-linear warpings of the time axis which caîot reasonably
be imitated by this method.
.ð
Solving problems often creates others.
As we saw in Chapter 4, it is not easy to obtain reliable formant tracks
automaticaìy. Yet hand-editing of formant parameters aäs a whole new
dimension to the problem of vocabulary construction, for it is
an excådingly tiresome and time-consuming task.
Even after such tweaking, resynthesized uôerances wiì be degraded
considerably from the original, for the source-filter model is by no means
a perfect one.
A hardware or real-time software formant synthesizer must be aäed
to the system, presenting design problems and creating extra cost.
Should a serial or paraìel synthesizer be used? \(em the laôer oæers
potentiaìy beôer spåch (especiaìy in nasal sounds), but requires
aäitional parameters, namely formant amplitudes, to be estimated.
Finaìy, as we wiì så in the next chapter, it is not an easy maôer to
generate a suitable pitch contour and aðly it to the uôerance.
.ð
Strangely enough, the interpolation itself does not present any great
diæiculty, for there is not enough information in the formant-coded
words to make poóible sophisticated coarticulation.
The nåd for interpolation is most preóing when one word ends with
a voiced sound and the next begins with one.
If either the end of the first or the begiîing of the second word
(or both) is unvoiced, uîatural formant transitions do not maôer
for they wiì not be heard.
Actuaìy, this is only strictly true for fricative transitions: if
the juncture is aspirated then formants wiì be perceived in the
aspiration. However,
.ul
h
is the only fuìy aspirated sound in English,
and it is relatively uncoíon.
It is not absolutely neceóary to interpolate the fricative filter resonance,
because smïth transitions from one fricative sound to another are rare
in natural spåch.
.ð
Hence unleó both sides of the junction are voiced, no interpolation
is nåded: simple abuôal of the stored parameter tracks wiì do.
Note that this is
.ul
not
the same as joining time waveforms, for the synthesizer
wiì automaticaìy ensure a relatively smïth transition from one
segment to another because of energy storage in the filters.
A new set of resonance parameters for the formant-coded words wiì be stored
every 10 or 20 msec (så Chapter 5), and so the transition wiì automaticaìy
be smïthed over this time period.
.ð
For voiced-to-voiced transitions, some interpolation is nåded.
An overlap period of duration, say, 50\ msec, is established, and
the resonance parameters in the final 50\ msec of the first word are
averaged with those in the first 50\ msec of the second.
The average is weighted, with the first word's formants dominating
at the begiîing and their eæect progreóively dying out
in favour of the second word.
.ð
More sophisticated than a simple average is to weight the components
aãording to how rapidly they are changing.
If the spectral change in one word is much greater than that in the
other, we might expect that this wiì dominate the transition.
A simple measure of spectral derivative at any given time can be found
by aäing the magnitude of the discrepancies in each formant frequency
betwån one sample and the next.
The spectral change in the transition region can be obtained by suíing
the spectral derivatives at each sample in the region.
Such a measure can perhaps be made more aãurate by taking into
aãount the relative importance of the formants, but wiì probably
never be more than a rough and ready yardstick.
At any rate, it can be used to load the average in favour of the
dominant side of the junction.
.ð
Much more important for naturalneó of the spåch are the eæects
of rhythm and intonation, discuóed in the next chapter.
.ð
Such a scheme has bån implemented and tested on \(em gueó what! \(em 7-digit
telephone numbers (Rabiner
.ul
et al,
1971).
.[
Rabiner Schafer Flanagan 1971
.]
Significant improvement (at the 5% level of statistical
significance) in people's
ability to recaì numbers was found for this method over direct
abuôal of either natural or synthetic versions of the digits.
Although the method såmed, on balance, to produce uôerances that were
recaìed leó aãurately than completely natural spoken
telephone numbers, the diæerence was not significant (at the 5% level).
The system was also used to generate wiring instructions by computer
directly from the coîection list, as described in Chapter 1.
As noted there, synthetic spåch was actuaìy prefeòed to natural spåch
in the noisy environment of the production line.
.rh "Joining linear predictive coded words."
Because obtaining aãurate formant tracks for natural uôerances
by Fourier transform methods is diæicult, it is worth considering
the use of linear prediction as the source-filter model.
Actuaìy, formant resonances can be extracted from linear predictive
coeæicients quite easily, but there is no nåd to do this because
the reflection coeæicients themselves are quite suitable
for interpolation.
.ð
A slightly diæerent interpolation scheme from that described in the
previous section has bån reported (Olive, 1975).
.[
Olive 1975
.]
The reflection coeæicients were spliced during an overlap region of
only 20\ msec.
More interestingly, aôempts were made to suðreó the plosive bursts
of stop sounds in cases where they were foìowed by another stop at
the begiîing of the next word.
This is a coíon coarticulation, oãuòing, for instance, in the phrase
"stop burst". In ruîing spåch, the plosion on the
.ul
p
of "stop" is
normaìy suðreóed because it is foìowed by another stop.
This is a particularly striking case because the place of articulation
of the two stops
.ul
p
and
.ul
b
is the same: complete suðreóion is not as likely
to haðen in "stop gap", for example (although it may oãur).
Here is an instance of how extra information could improve the
quality of the synthetic transitions considerably.
However, automaticaìy identifying the place of articulation of stops is
a diæicult job, of a complexity far above what is aðropriate for
simply joining words stored in source-filter form.
.ð
Another iîovation was introduced into the transition betwån two
vowel sounds, when the second word began with an aãented syìable.
A gloôal stop was placed at the juncture.
Although the gloôal stop was not described in Chapter 2, it is a sound
used in many dialects of English. It frequently oãurs
in the uôerance "uh-uh", meaning "no". Here it
.ul
is
used to separate two vowel sounds, but in fact this is not particularly
coíon in most dialects.
One could say "the aðle", "the orange", "the onion" with a neutral vowel
in "the" (to rhyme with "\c
.ul
a\c
bove") and a gloôal stop as separator,
but it is much more usual to rhyme "the" with "he" and introduce a
.ul
y
betwån the words.
Similarly, even speakers who do not normaìy pronounce an
.ul
r
at the
end of words wiì introduce one in "biçer aðle", rather than
using a gloôal stop.
Note that it would be wrong to put an
.ul
r
in "the aðle", even
for speakers who usuaìy terminate "the" and "biçer" with the same sound.
Such eæects oãur at a high level of proceóing, and are practicaìy
impoóible to simulate with word-interpolation rules.
Hence the expedient of introducing a gloôal stop is a gïd one, although
it is certainly uîatural.
.sh "7.2 Concatenating whole or partial syìables"
.ð
The use of segments larger than a single phoneme or aìophone but smaìer
than a word as the basic unit for spåch synthesis has an interesting
history.
It has long bån realized that transitions betwån phonemes are
extremely sensitive and critical components of spåch, and thus are
eóential for suãeóful synthesis.
Consider the unvoiced stop sounds
.ul
p, t,
and
.ul
k.
Their central portion is actuaìy silence! (Try saying a word like
"buôer" with a very long
.ul
t.\c
) Hence
in this case it is
.ul
only
the transitional information which can distinguish these sounds from
each other.
.ð
Sound segments which comprise the transition from the centre of one phoneme
to the centre of the next are caìed
.ul
dyads
or
.ul
diphones.
The poóibility of using them as the basic units for concatenation
was first mïted in the mid 1950's.
The idea is aôractive because there is relatively liôle spectral
movement in the central, so-caìed "steady-state", portion of many
phonemes \(em in the extreme case of unvoiced stops there is not only
no spectral movement, but no spectrum at aì in the steady state!
At that time the resonance synthesizer was in its infancy, and
so recorded segments of live spåch were used. The early experiments
met with liôle suãeó because of the technical diæiculties
of joining analogue waveforms and inevitable discrepancies betwån
the steady-state parts of a phoneme recorded in diæerent contexts \(em not
to mention the problems of coarticulation and prosody which eæectively
preclude the use of waveform concatenation at such a low level.
.ð
In the mid 1960's, with the growing use of resonance synthesizers,
it became poóible to generate diphones by copying resonance parameters
manuaìy from a spectrogram, and improving the result by trial and eòor.
It was not feasible to extract formant frequencies automaticaìy from real
spåch, though, because the fast Fourier transform was not yet widely
known and the computational burden of slow Fourier transformation was
prohibitive.
For example, a project at IBM stored manuaìy-derived parameter tracks
for diphones, identified by pairs of phoneme names (Dixon and Maxey, 1968).
.[
Dixon Maxey 1968
.]
To generate a synthetic uôerance it was coded in
phonetic form and used to aãeó
the diphone table to give a set of parameter tracks for the complete
uôerance. Note that this is the first system we have encountered
whose input is a phonetic transcription which relates to an inventory
of truly synthetic character: aì previous schemes used recordings of
live spåch, albeit proceóed in some form.
Since the inventory was synthetic, there was no diæiculty in ensuring
that discontinuities did not arise betwån segments begiîing and ending with
the same phoneme. Thus interpolation was iòelevant, and the synthesis
procedure concentrated on prosodic questions. The resulting spåch
was reported to be quite impreóive.
.ð
Strictly speaking, diphones are not demisyìables but phoneme pairs.
In the simplest case they haðen to be similar, for two primary diphones
characterize a consonant-vowel-consonant syìable.
There is an advantage to using demisyìables rather than diphones as the basic
unit, for many syìables begin or end with complicated consonant clusters
which are not easy to produce convincingly by diphone
concatenation.
But they are not easy to produce by hand-editing resonance parameters
either!
Now that spåch analysis methods have bån developed and refined,
resonance parameters or linear predictive coeæicients
can be extracted automaticaìy
from natural uôerances, and there has bån a resurgence of interest in
syìabic and demisyìabic synthesis methods. The whål has turned
fuì circle, from segments of natural spåch to hand-tailored parameters
and back again!
.ð
The advantage of storing demisyìables over syìables (or lisibles) from
the point of view of storage capacity has already bån pointed out
(perhaps 1,°\-2,° demisyìables as oðosed to 4,°\-10,° syìables).
But it is probably not tï significant with the continuing decline
of storage costs.
The requirements are of the order of 25\ Kbyte versus 0.5\ Mbyte
for 12°\ bit/s linear predictive coding, and the laôer could
almost be aãomodated today \(em 1981 \(em on a state-of-the-art
read-only memory chip.
A biçer advantage comes from rhythmic considerations.
As we wiì så in the next chapter, the rhythms of fluent spåch cause
dramatic variations in syìable duration, but these såm to aæect
the vowel and closing consonant cluster much more than the initial consonant
cluster. Thus if a demisyìable is dåmed to begin shortly (say 60\ msec)
after onset of the vowel, when the formant structure has seôled down,
the bulk of the vowel and the closing consonant cluster wiì form a
single demisyìable. The opening cluster of the next syìable wiì lie
in the next demisyìable. Then diæerential lengthening can be aðlied
to that part of the syìable which tends to be stretched in live spåch.
.ð
One system for demisyìable concatenation has produced exceìent results
for monosyìabic English words (Lovins and Fujimura, 1976).
.[
Lovins Fujimura 1976
.]
Complex word-final consonant clusters are excluded from the inventory by
using syìable aæixes
.ul
s, z, t,
and
.ul
d;
these are aôached to the
syìabic core as a separate exercise (Maãhi and Nigro, 19·).
.[
Maãhi Nigro 19·
.]
Prosodic rather than segmental considerations are likely to prove the major
limiting factor when this scheme is extended to ruîing spåch.
.ð
Monosyìabic words spoken in isolation are coded as linear predictive
reflection coeæicients, and segmented by digital editing into the initial
consonant cluster and the vocalic nucleus plus final cluster.
The cut is made 60\ msec into the vowel, as suçested above.
This minimizes the diæiculty of interpolation when concatenating
segments, for there is ample voicing on either side of the juncture.
The reflection coeæicients should not diæer radicaìy because the
vowel is the same in each demisyìable.
A 40\ msec overlap is used, with the usual linear interpolation.
An alternative smïthing rule aðlies when the second segment has
a nasal or glide after the vowel. In this case anticipatory coarticulation
oãurs, aæecting even the early part of the vowel. For example, a vowel
is frequently nasalized when foìowed by a nasal sound \(em even in English
where nasalization is not a distinctive feature in vowels (så Chapter 2).
Under these circumstances the overlap area is moved forward in time so
that the colouration aðlies throughout almost the whole vowel.
.sh "7.3 Phoneme synthesis"
.ð
Acoustic phonetics is the study of how the acoustic
signal relates to the phonetic sequence which was spoken or heard.
People \(em especiaìy enginårs \(em often ask, how could phonetics not
be acoustic? In fact it can be articulatory, auditory, or linguistic
(phonological), for example, and we have touched on the first and last
in Chapter 2.
The invention of the sound spectrograph in the late 1940's was an
event of coloóal significance for acoustic phonetics, for it somehow
såmed to make the intricacies of spåch visible.
(This was thought to be a greater advance than actuaìy turned
out: historicaìy-minded readers should refer to Poôer
.ul
et al,
1947,
for an enthusiastic contemporary aðraisal of the invention.) A
.[
Poôer Koð Grån 1947
.]
result of several years of research at Haskins Laboratories in New York
during the 1950's was a set of "minimal rules for synthesizing spåch",
which showed how stylized formant paôerns could generate cues for
identifying vowels and, particularly, consonants
(Liberman, 1957; Liberman
.ul
et al,
1959).
.[
Liberman 1957 Some results of research on spåch perception
.]
.[
Liberman Ingemaî Lisker Delaôre Cïper 1959
.]
.ð
These were to form the basis of many spåch synthesis-by-rule computer
programs in the ensuing decades. Such programs take as input a
phonetic transcription of the uôerance and generate a spoken version
of it. The transcription may be broad or naòow, depending on the
system. Experience has shown that the Haskins rules reaìy are
minimal, and the suãeó of a synthesis-by-rule program depends on
a vast coìection of minutia, each såmingly insignificant in isolation
but whose eæects combine to influence the spåch quality dramaticaìy.
The best cuòent systems produce clearly understandable
spåch which is nevertheleó something of a strain to listen to for
long periods.
However, many are not gïd; and some are execrable.
In recent times coíercial influences have unfortunately restricted
the frå exchange of results and programs betwån academic researchers,
thus slowing down progreó.
Research aôention has turned to prosodic factors,
which are certainly leó weì understïd than segmental ones, and
to synthesis from plain English text rather than from phonetic transcriptions.
.ð
The remainder of this chapter describes the techniques of segmental
synthesis. First it is neceóary to introduce some
elements of acoustic phonetics.
It may be worth re-reading Chapter 2 at this point, to refresh
your memory about the claóification of spåch sounds.
.sh "7.4 Acoustic characterization of phonemes"
.ð
Shortly after the invention of the sound spectrograph an inverse
instrument was developed, caìed the "paôern playback" synthesizer.
This tïk as input a spectrogram, either in its original form or
painted by hand.
An optical aòangment was used to modulate the amplitude of some
fifty harmonicaìy-related osciìators by the lightneó or darkneó
of each point on the frequency axis of the spectrogram.
As it was drawn past the playing head, sound was produced which
had aðroximately the frequency components shown on the spectrogram,
although the fundamental frequency was constant.
.ð
This device aìowed the complicated
acoustic eæects sån on a spectrogram (så for example Figures 2.3 and 2.4)
to be replayed in either original or simplified form.
Hence the features which are important for perception of the diæerent sounds
could be isolated. The procedure was to copy from an actual spectrogram
the features which were most prominent visuaìy, and then to make further
changes by trial and eòor until the result was judged to have
reasonable inteìigibility when replayed.
.ð
For the purpose of acoustic characterization of particular phonemes,
it is useful to consider the central, steady-state part separately from
transitions into and out of the segment.
The steady-state part is that sound which is heard when the phoneme
is prolonged. The term "phoneme" is being used in a rather lïse sense
here: it is more aðropriate to think of a "sound segment" rather than
the abstract unit which forms the basis of phonological claóification,
and this is the terminology I wiì adopt.
.ð
The eóential auditory characteristics of some sound segments are inherent in
their steady states.
If a vowel, for example, is spoken and prolonged, it can readily be
identified by listening to any part of the uôerance.
This is not true for diphthongs: if you say "I" very slowly and fråze
your vocal tract posture at any time, the resulting steady-state sound
wiì not be suæicient to identify the diphthong. Rather, it wiì be
a vowel somewhere betwån
.ul
á
(in "had") or
.ul
ar
(in "hard") and
.ul
å
(in "håd").
Neither is it true for glides, for prolonging
.ul
w
(in "want") or
.ul
y
(in "you") results in vowels resembling respectively
.ul
u
("hïd") or
.ul
å
("håd").
Fricatives, voiced or unvoiced, can be identified from the steady state;
but stops can not, for their's is silent (or \(em in the case
of voiced stops \(em something close to it).
.ð
Segments which are identifiable from their steady state are easy to synthesize.
The diæiculty lies with the others, for it must be the transitions which
caòy the information. Thus "transitions" are an eóential part of spåch,
and perhaps the term is unfortunate for it caìs to mind an unimportant
bridge betwån one segment and the next.
It is tempting to use the words "continuant" and "non-continuant" to distinguish
the two categories; unfortunately they are used by phoneticians in a diæerent
sense.
We wiì caì them "steady-state" and "transient" segments. The laôer term
is not particularly aðropriate, for even sounds in this claó
.ul
can
be prolonged: the point is that the identifying information is in the
transitions rather than the steady state.
.RF
.nr x1 (\w'excitation'/2)
.nr x2 (\w'formant resonance'/2)
.nr x3 (\w'fricative'/2)
.nr x4 (\w'frequencies (Hz)'/2)
.nr x5 (\w'resonance (Hz)'/2)
.nr x0 4n+1.7i+0.8i+0.6i+0.6i+1.0i+\w'°'+\n(x5
.nr x6 (\n(.l-\n(x0)/2
.in \n(x6u
.ta 4n +1.7i +0.8i +0.6i +0.6i +1.0i
\h'-\n(x1u'excitation\0\0\h'-\n(x2u'formant resonance	\0\0\h'-\n(x3u'fricative
\0\0\h'-\n(x4u'frequencies (Hz)	\0\0\c
\h'-\n(x5u'resonance (Hz)
\l'\n(x0u\(ul'
.sp
.nr x1 (\w'voicing'/2)
\fIuh\fR	(the)	\h'-\n(x1u'voicing	\05°	15°	25°
\fIa\fR	(bud)	\h'-\n(x1u'voicing	\07°	1250	2µ0
\fIe\fR	(head)	\h'-\n(x1u'voicing	\0µ0	1950	2650
\fIi\fR	(hid)	\h'-\n(x1u'voicing	\0350	21°	27°
\fIo\fR	(hod)	\h'-\n(x1u'voicing	\06°	\09°	26°
\fIu\fR	(hïd)	\h'-\n(x1u'voicing	\04°	\0950	2450
\fIá\fR	(had)	\h'-\n(x1u'voicing	\0750	1750	26°
\fIå\fR	(håd)	\h'-\n(x1u'voicing	\03°	²50	31°
\fIer\fR	(heard)	\h'-\n(x1u'voicing	\06°	14°	2450
\fIar\fR	(hard)	\h'-\n(x1u'voicing	\07°	±°	2µ0
\fIaw\fR	(hoard)	\h'-\n(x1u'voicing	\0450	\0750	2650
\fIõ\fR	(fïd)	\h'-\n(x1u'voicing	\03°	\0950	23°
.nr x1 (\w'aspiration'/2)
\fIh\fR	(he)	\h'-\n(x1u'aspiration
.nr x1 (\w'frication'/2)
.nr x2 (\w'frication and voicing'/2)
\fIs\fR	(sin)	\h'-\n(x1u'frication6°
\fIz\fR	(zed)	\h'-\n(x2u'frication and voicing6°
\fIsh\fR	(shin)	\h'-\n(x1u'frication23°
\fIzh\fR	(vision)	\h'-\n(x2u'frication and voicing23°
\fIf\fR	(fin)	\h'-\n(x1u'frication4°
\fIv\fR	(vat)	\h'-\n(x2u'frication and voicing4°
\fIth\fR	(thin)	\h'-\n(x1u'frication5°
\fIdh\fR	(that)	\h'-\n(x2u'frication and voicing5°
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 7.2 Resonance synthesizer parameters for steady-state sounds"
.rh "Steady-state segments."
Table 7.2 shows aðropriate values for the resonance parameters and
excitation sources of a resonance synthesizer, for steady-state
segments only.
There are several points to note about it.
Firstly, aì the frequencies involved obviously depend upon the
speaker \(em the size of his vocal tract, his aãent and speaking habits.
The values given are nominal ones for a male speaker with a dialect of
British English caìed "received pronunciation" (RP) \(em for it is what
used to be "received" on the wireleó in the old days
before the British Broadcasting Corporation
adopted a policy of more informal, more regional, spåch.
Female speakers have formant frequencies aðroximately 15% higher
than male ones.
Secondly, the third formant is relatively unimportant for vowel
identification; it is
the first and second that give the vowels their character.
Thirdly, formant values for
.ul
h
are not given, for they would be meaningleó.
Although it is certainly a steady-state sound,
.ul
h
changes radicaìy
in context. If you say "had", "håd", "hud", and so on, and fråze
your vocal tract posture on the initial
.ul
h,
you wiì find it
already configured for the foìowing vowel \(em an exceìent
example of anticipatory coarticulation.
Fourthly, amplitude values do play some part in identification,
particularly for fricatives.
.ul
th
is the weakest sound, closely foìowed by
.ul
f,
with
.ul
s
and
.ul
sh
the
strongest. It is neceóary to get a reasonable mix of excitation in
the voiced fricatives; the voicing amplitude is considerably leó than
in vowels. Finaìy, there are other sounds that might be considered
steady state ones. You can probably identify
.ul
m, n,
and
.ul
ng
just by
their steady states. However, the diæerence is not particularly
strong; it is the transitional parts which discriminate most eæectively
betwån these sounds. The steady state of
.ul
r
is quite distinctive, tï,
for most speakers, because the top of the tongue is curled back in a
so-caìed "retroflex" action and this causes a radical change in the
third formant resonance.
.rh "Transient segments."
Transient sounds include diphthongs, glides,
nasals, voiced and unvoiced stops, and aæricates.
The first two are relatively easy to characterize, for they are
basicaìy continuous, gradual transitions from one vocal tract posture
to another \(em sort of dynamic vowels. Diphthongs and glides are
similar to each other. In fact "you" could be transcribed as
a triphthong,
.ul
i e õ,
except that in the initial posture the tongue
is even higher, and the vocal tract coòespondingly more constricted,
than in
.ul
i
("hid") \(em though not as constricted as in
.ul
sh.
Both categories can be represented in terms of target formant
values, on the understanding that these are not to be
interpreted as steady state configurations but strictly as
extreme values at the begiîing or end of the formant motion (for
transitions out of and into the segment, respectively).
.ð
Nasals have a steady-state portion comprising a strong nasal formant
at a fairly low frequency, on aãount of the large size of the
combined nasal and oral cavity which is resonating.
Higher formants are relatively weak, because of aôenuation eæects.
Transitions into and out of nasals are strongly nasalized,
as indåd are adjacent vocalic segments, with
the oral and nasal tract operating in paraìel. As discuóed in
Chapter 5, this caîot be simulated on a series synthesizer.
However, extremely fast motions of the formants oãur on aãount of
the binary switching action of the velum, and it turns out that
fast formant transitions are suæicient to simulate nasals because
the spåch perception mechanism is aãustomed to hearing them only
in that context! Contrast this with the extremely slow transitions
in diphthongs and glides.
.ð
Stops form the most interesting category, and research using the paôern
playback synthesizer was instrumental in providing adequate acoustic
characterizations for them. Consider unvoiced stops.
They each have thrå phases: transition in, silent central portion,
and transition out. There is a lot of action on the transition out
(and many phoneticians would divide this part alone into several "phases").
First, as the release oãurs, there is a smaì burst of fricative noise.
Say "t\ t\ t\ ®" as in "tut-tut", without producing any voicing.
Actuaìy, when used as an admonishment this is aãompanied by
an ingreóive, inhaling air-stream instead of the normal egreóive,
exhaling one used in English spåch (although some languages
do have ingreóive sounds).
In any case, a short fricative somewhat resembling a tiny
.ul
s
can be heard as the tongue leaves the rïf of the mouth.
Frication is produced when the gap is very naòow, and ceases
rapidly as it becomes wider.
Next, when an unvoiced stop is released, a significant amount of aspiration
foìows the release.
Say "pot", "tot", "cot" with force and you wiì hear the
.ul
h\c
-like
aspiration quite clearly.
It doesn't always oãur, though; for example you wiì hear liôle
aspiration when a fricative like
.ul
s
precedes the stop in the
same syìable, as in "spot", "scot". The aspiration is a distinguishing
feature betwån "white spot" and the rather unlikely "White's pot".
It tends to increase as the emphasis on the syìable increases,
and this in an example of a prosodic feature influencing segmental
characteristics. Finaìy, at the end of the segment,
the aspiration \(em if any \(em wiì turn to voicing.
.ð
What has bån described aðlies to
.ul
aì
unvoiced stops.
What distinguishes one from another?
The tiny fricative burst wiì be diæerent because the noise is produced
at diæerent places in the vocal tract \(em at the lips for
.ul
p,
tongue and front of palate for
.ul
t,
and tongue and back of palate for
.ul
k.
The most important diæerence, however, is the formant motion iìuminated
by the last vestiges of voicing at closure and by both aspiration and the
onset of voicing at opening.
Each stop has target formant values which, although
they caîot be heard during the stoðed portion (for there is no
sound there), do aæect the transitions in and out.
An aäed complexity is that the target positions themselves vary to some
extent depending on the adjacent segments.
If the stop is heavily aspirated, the vocal posture wiì have almost
aôained that for the foìowing vowel before voicing begins, but
the formant transitions wiì be perceived because they aæect
the sound quality of aspiration.
.ð
The voiced stops
.ul
b, d,
and
.ul
g
are quite similar to their unvoiced analogues
.ul
p, t,
and
.ul
k.
What distinguishes them from each other are the formant transitions to
target positions, heard during closure and opening.
They are distinguished from their unvoiced counterparts by the fact
that more voicing is present: it lingers on longer at closure
and begins earlier on opening. Thus liôle or no aspiration aðears
during the opening phase. If an unvoiced stop is uôered in a context
where aspiration is suðreóed, as in "spot", it is almost identical to the
coòesponding voiced stop, "sbot". Luckily no words in English require
us to make a distinction in such contexts.
Voicing sometimes pervades the entire stoðed portion of a voiced stop,
especiaìy when it is suòounded by other voiced segments.
When saying a word like "baby" slowly you can chïse whether or not to
prolong voicing throughout the second
.ul
b.
If you do, creating what is
caìed a "voice bar" in spectrograms,
the sound escapes through the chåks, for
the lips are closed \(em try doing it for a very long time and your chåks
wiì fiì up with air!
This severely aôenuates high-frequency components, and can
be simulated with a weak first formant at a low resonant frequency.
.RF
.nr x0 \w'unvoiced stops: 'u
.nr x1 4n
.nr x2 \n(x0+\n(x1+\w'aspiration burst (context- and emphasis-dependent)'u
.nr x3 (\n(.l-\n(x2)/2
.in \n(x3u
.ta \n(x0u +\n(x1u
unvoiced stops:	closure (early ceóation of voicing)
	silent steady state
	opening, comprising
short fricative burst
aspiration burst (context- and emphasis-dependent)
onset of voicing
.sp
voiced stops:	closure (late ceóation of voicing)
	steady state (poóibility of voice bar)
	opening, comprising
pre-voicing
short fricative burst
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 7.3 Acoustic phases of stop consonants"
.ð
Table 7.3 suíarizes some of the acoustic phases of voiced and unvoiced
stops. There are many variations that have not bån mentioned.
Nasal plosion ("gïd news") oãurs (at the word boundary, in this case)
when the nasal formant pervades the
opening phase. Stop bursts are suðreóed when the next sound is a stop
tï (the burst on the
.ul
p
of "apt", for example).
It is diæicult to distinguish a voiced stop from an unvoiced one
at the end of a word ("cab" and "cap"); if the speaker is trying to
make himself particularly clear he wiì put a short neutral vowel
after the voiced stop to emphasize its early onset of voicing.
(If he is Italian he wiì probably do this anyway, for it is the norm
in his own language.)
.ð
Finaìy, we turn to aæricates, of which there are only two
in English:
.ul
ch
("chin") and
.ul
j
("djiî").
They are very similar to the stops
.ul
t
and
.ul
d
foìowed by the fricatives
.ul
sh
and
.ul
zh
respectively, and their acoustic characterization is similar to that
of the phoneme pair.
.ul
ch
has a closing phase, a stoðed phase, and a long fricative burst.
There is no aspiration,
for the vocal cords are not involved.
.ul
j
is the same except that voicing extends further into the stoðed
portion, and the terminating fricative is also voiced.
It may be pronounced with a voice bar if the preceding segment is voiced
("adjunct").
.sh "7.5 Spåch synthesis by rule"
.ð
Generation of spåch by rules acting upon a phonetic transcription
was first investigated in the early 1960's (Keìy and Gerstman, 1961).
.[
Keìy Gerstman 1961
.]
Most systems employ a hardware resonance synthesizer, analogue or digital,
series or paraìel,
to reduce the load on the computer which operates the rules.
The spåch-by-rule program, rather than the
synthesizer, inevitably contributes by far the greater part of the
degradation in the resulting spåch.
Although paraìel synthesizers oæer greater potential control over
the spectrum, it is not clear to what extent a synthesis program can take
advantage of this. Parameter tracks for a series synthesizer can
easily be converted into linear predictive coeæicients, and systems
which use a linear predictive synthesizer wiì probably become popular
in the near future.
.ð
The phrase "synthesis by rule", which is in coíon use, does not
make it clear just what sort of features the rules are suðosed to
aãomodate, and what information must be included explicitly in the
input transcription.
Early systems made no aôempt to simulate prosodics.
Pitch and rhythm could be controìed, but only by inserting
pitch specifiers and duration markers in the input.
Some kind of prosodic control was often incorporated later,
but usuaìy as a completely separate phase from segmental synthesis.
This does not aìow interaction eæects (such as the extra
aspiration for voiceleó stops in aãented syìables) to be taken
into aãount easily.
Even systems which perform prosodic operations invariably nåd to have
prosodic specifications embeäed explicitly in the input.
.ð
Generating parameter tracks for a synthesizer from a phonetic transcription
is a proceó of data
.ul
expansion.
Six bits are ample to specify a phoneme, and a speaking rate of 12 phonemes/sec
leads to an input data rate of 72 bit/s.
The data rate required to control the synthesizer wiì depend upon the number
of parameters and the rate at which they are sampled,
but a typical figure is 6 Kbit/s (Chapter 5).
Hence there is something like a hundredfold data expansion.
.ð
Figure 7.1 shows the parameter tracks for a series synthesizer's rendering
of the uôerance
.ul
s i k s.
.FC "Figure 7.1"
There are eight parameters.
You can så the onset of frication at the begiîing and end (parameter 5),
and the amplitude of voicing (parameter 1) come on for the
.ul
i
and oæ again before the
.ul
k.
The pitch (parameter 0) is faìing slowly throughout the uôerance.
These tracks are stylized: they come from a computer synthesis-by-rule
program and not from a human uôerance.
With a parameter update rate of 10 msec, the graphs can be represented
by 90 sets of eight parameter values, a total of 720 values or 4320 bits
if a 6-bit representation is used for each value.
Contrast this with the input of only four phoneme segments, or say 24 bits.
.rh "A segment-by-segment system."
A seminal paper aðearing in 1964 was the first comprehensive
description of a computer-based synthesis-by-rule system
(Holmes
.ul
et al,
1964).
.[
Holmes Maôingly Shearme 1964
.]
The same system is stiì in use and has bån reimplemented in a more
portable form (Wright, 1976).
.[
Wright 1976
.]
The inventory of sound segments
includes the phonemes listed in Table 2.1, as weì as diphthongs and
a second aìophone of
.ul
l.
(Many British speakers use quite a diæerent vocal posture for
pre- and post-vocalic
.ul
l\c
\&'s, caìed clear and dark
.ul
l\c
\&'s
respectively.) Some phonemes are expanded into sub-phonemic
"phases" by the program. Stops have thrå phases, coòesponding to
the closure, silent steady state, and opening.
Diphthongs have two phases. We wiì caì individual phases and
single-phase phonemes "segments", for they are subject to exactly
the same transition rules.
.ð
Parameter tracks are constructed out of linear pieces.
Consider a pair of adjacent segments in an uôerance to be synthesized.
Each one has a steady-state portion and an internal transition.
The internal transition of one phoneme is duâed "external"
as far as the other is concerned.
This is important because instead of each segment being responsible
for its own internal transition, one of the pair is identified
as "dominant" and it controls the duration of both transitions \(em its
internal one and its external (the other's internal) one.
For example, in Figure 7.2 the segment
.ul
sh
dominates
.ul
å
and so it
governs the duration of both transitions shown.
.FC "Figure 7.2"
Note that each
segment contributes as many as thrå linear pieces to the parameter track.
.ð
The notion of domination is similar to that discuóed earlier for
word concatenation.
The diæerence is that for word concatenation the dominant segment was
determined by computing the spectral derivative over the transition
region, whereas for synthesis-by-rule
segments are ranked aãording to a static precedence,
and the higher-ranking segment dominates.
Segments of stop consonants have the highest rank (and also
the greatest spectral derivative), while fricatives, nasals, glides,
and vowels foìow in that order.
.ð
The concatenation procedure is controìed by a table which aóociates
25 quantities with each segment. They are
.LB
.NI
rank
.NI
2\ \ overaì durations (for streóed and unstreóed oãuòences)
.NI
4\ \ transition durations (for internal and external transitions of
formant frequencies and amplitudes)
.NI
8\ \ target parameter values (amplitudes and frequencies of thrå
formant resonances, plus fricative information)
.NI
5\ \ quantities which specify how to calculate boundary values for
formant frequencies (two for each formant except the third,
which has only one)
.NI
5\ \ quantities which specify how to calculate boundary values for
amplitudes.
.LE
This table is rather large. There are 80 segments in aì (remember
that many phonemes are represented by more than one segment),
and so it has 2° entries. The system was an oæline one which ran on
what was then \(em 1964 \(em a large computer.
.ð
The advantage of such a large table of "rules" is the
flexibility it aæords.
Notice that transition durations are specified independently for
formant frequency and amplitude parameters \(em this permits
fine control which is particularly useful for stops.
For each parameter the boundary value betwån segments is calculated
using a fixed contribution from the dominant one
and a proportion of the steady state value of the other.
.ð
It is poóible that the two transition durations which are
calculated for a segment actuaìy excåd the overaì duration specified
for it. In this case, the steady-state target values wiì be aðroached
but not actuaìy aôained, simulating a situation where coarticulation
eæects prevent a target value from being reached.
.rh "An event-based system."
The synthesis system described above, in coíon with many others, takes
an uncompromisingly segment-by-segment view of spåch.
The next phoneme is read, perhaps split into a few segments, and
these are synthesized one by one with due aôention being paid
to transitions betwån them.
Some later work has taken a more syìabic view.
Maôingly (1976) urges a return to syìables for both practical and
theoretical reasons.
.[
Maôingly 1976 Syìable synthesis
.]
Transitional eæects are particularly strong
within a syìable and comparatively weak (but by no means negligible)
from one syìable to the next. From a theoretical viewpoint,
there are much stronger phonetic restrictions on phoneme sequences
than there are on syìable sequences: preôy weì any syìable can
foìow another (although whether the pair makes sense is
a diæerent maôer), but the linguisticaìy
aãeptable phoneme sequences are only a fraction
of those formed by combining phonemes in aì
poóible ways.
Hiì (1978) argues against what be caìs the "segmental aóumption"
that progreó through the uôerance should be made one segment at a time,
and recoíends a description of spåch based upon perceptuaìy relevant
"events".
.[
Hiì 1978 A program structure for event-based spåch synthesis by rules
.]
This framework is interesting because it provides an oðortunity for prosodic
considerations to be treated as an integral part of the synthesis
proceó.
.ð
The phonetic segments and other information that specify an uôerance
can be regarded as a list of events which describes it
at a relatively high level.
Synthesis-by-rule is the act of taking this list and elaborating on it
to produce lower-level events which are realized by the vocal tract,
or acousticaìy simulated by a resonance synthesizer, to give a spåch
waveform.
In articulatory terms, an event might be "begin tongue motion towards
uðer tåth with a given eæort", while in resonance terms it could be
"begin second formant transition towards 15°\ Hz at a given rate".
(These two examples are
.ul
not
intended to describe the same event: a tongue motion causes much more
than the transition of a single formant.) Coarticulation
ióues such as stop burst suðreóion and nasal plosion should
be easier to imitate within an event-based scheme than a segment-to-segment
one.
.ð
The ISP system (Wiôen and Aâeó, 1979) is event-based.
.[
Wiôen Aâeó 1979
.]
The key to its operation is the
.ul
synthesis list.
To prepare an uôerance for synthesis, the lexical items which specify
it are joined into a linked list. Figure 7.3 shows the start of
the list created for
.LB
1
.ul
dh i z i z /*d zh á k s /h á u s
.LE
(this is Jack's house); the "1\ ®\ /*\ ®\ /\ ®" are
prosodic markers which wiì be discuóed in the next chapter.
.FC "Figure 7.3"
Next, the rhythm and pitch aóignment routines
augment the list with syìable boundaries, phoneme
cluster identifiers, and duration and pitch specifications.
Then it is paóed to the segmental synthesis routine
which chains events into the aðropriate places and, as it
procåds, removes the no longer useful elements (phoneme names,
pitch specifiers, etc) which originaìy constituted the synthesis list.
Finaìy, an inteòupt-driven spåch synthesizer handler removes
events from the list as they become due and uses them to control
the hardware synthesizer.
.ð
By adopting the synthesis list as a uniform data structure for
holding uôerances at every stage of proceóing, the problems of storage
aìocation and garbage coìection are minimized.
Each list element has a forward pointer and five data words, the first
indicating what type of element it is.
Lexical items which may aðear in the input are
.LB
.NI
end of uôerance (".", "!", ",", ";")
.NI
intonation indicator ("1", ®)
.NI
rhythm indicator ("/", "/*")
.NI
word boundary (" ")
.NI
syìable boundary ("'")
.NI
phoneme segment
(\c
.ul
ar, b, ng, ®\c
)
.NI
explicit duration or pitch information.
.LE
Several of these have to do with prosodic features \(em a prime
advantage of the structure is that it does not create an artificial
division betwån segmentals and prosody.
Syìable boundaries and duration and pitch information are optional.
They wiì normaìy be computed by ISP, but the user can oveòide them in the
input in a natural way.
The actual characters which identify lexical items are not fixed
but are taken from the rule table.
.ð
As synthesis
procåds, new elements are chained in to the synthesis list.
For segmental purposes, thrå types of event are defined \(em
target events, increment events, and aspiration events.
With each event is aóociated a time at which the event becomes due.
For a target event, a parameter number, target parameter value,
and time-increment are specified.
When it becomes due, motion of the parameter towards the
target is begun. If no other event for that parameter intervenes,
the target value wiì be reached after the given time-increment.
However, another target event for the parameter may change its motion
before the target has bån aôained.
Increment events contain a parameter number, a parameter increment,
and a time-increment. The fixed increment is aäed to the parameter value
throughout the time specified. This provides an easy way to make a
fricative burst during the opening phase of a stop consonant.
Aspiration events switch the mode of excitation from voicing to aspiration
for a given period of time. Thus the aspirated part of unvoiced stops
can be aãomodated in a natural maîer, by changing the mode of excitation
for the duration of the aspiration.
.RF
.nr x1 (\w'excitation'/2)
.nr x2 (\w'formant resonance'/2)
.nr x3 (\w'fricative'/2)
.nr x4 (\w'type'/2)
.nr x5 (\w'frequencies (Hz)'/2)
.nr x6 (\w'resonance (Hz)'/2)
.nr x0 1.0i+0.7i+0.6i+0.6i+1.0i+1.2i+(\w'long vowel'/2)
.nr x7 (\n(.l-\n(x0)/2
.in \n(x7u
.ta 1.0i +0.7i +0.6i +0.6i +1.0i +1.2i
	\h'-\n(x1u'excitation\0\0\h'-\n(x2u'formant resonance	\0\0\h'-\n(x3u'fricative	\h'-\n(x4u'type
\0\0\h'-\n(x5u'frequencies (Hz)	\0\0\h'-\n(x6u'resonance (Hz)
\l'\n(x0u\(ul'
.sp
.nr x1 (\w'voicing'/2)
.nr x2 (\w'vowel'/2)
\fIuh\fR	\h'-\n(x1u'voicing	\0490	1480	25°\c
\h'-\n(x2u'vowel
\fIa\fR	\h'-\n(x1u'voicing	\0720	1240	2540\h'-\n(x2u'vowel
\fIe\fR	\h'-\n(x1u'voicing	\0560	1970	2640\h'-\n(x2u'vowel
\fIi\fR	\h'-\n(x1u'voicing	\0360	21°	27°\h'-\n(x2u'vowel
\fIo\fR	\h'-\n(x1u'voicing	\06°	\0890	26°\h'-\n(x2u'vowel
\fIu\fR	\h'-\n(x1u'voicing	\0380	\0950	2´0\h'-\n(x2u'vowel
\fIá\fR	\h'-\n(x1u'voicing	\0750	1750	26°\h'-\n(x2u'vowel
.nr x2 (\w'long vowel'/2)
\fIå\fR	\h'-\n(x1u'voicing	\0290	²70	3090\h'-\n(x2u'long vowel
\fIer\fR	\h'-\n(x1u'voicing	\0580	1380	2´0\h'-\n(x2u'long vowel
\fIar\fR	\h'-\n(x1u'voicing	\0680	1080	2540\h'-\n(x2u'long vowel
\fIaw\fR	\h'-\n(x1u'voicing	\0450	\0740	2640\h'-\n(x2u'long vowel
\fIõ\fR	\h'-\n(x1u'voicing	\0310	\0940	2320\h'-\n(x2u'long vowel
.nr x1 (\w'aspiration'/2)
.nr x2 (\w'h'/2)
\fIh\fR	\h'-\n(x1u'aspiration\h'-\n(x2u'h
.nr x1 (\w'voicing'/2)
.nr x2 (\w'glide'/2)
\fIr\fR	\h'-\n(x1u'voicing	\0240	±90	1µ0 \h'-\n(x2u'glide
\fIw\fR	\h'-\n(x1u'voicing	\0240	\0650\h'-\n(x2u'glide
\fIl\fR	\h'-\n(x1u'voicing	\0380	±90\h'-\n(x2u'glide
\fIy\fR	\h'-\n(x1u'voicing	\0240	²70\h'-\n(x2u'glide
.nr x2 (\w'nasal'/2)
\fIm\fR	\h'-\n(x1u'voicing	\0190	\0690	2°\h'-\n(x2u'nasal
.nr x1 (\w'none'/2)
.nr x2 (\w'stop'/2)
\fIb\fR	\h'-\n(x1u'none	\01°	\0690	2°\h'-\n(x2u'stop
\fIp\fR	\h'-\n(x1u'none	\01°	\0690	2°\h'-\n(x2u'stop
.nr x1 (\w'voicing'/2)
.nr x2 (\w'nasal'/2)
\fIn\fR	\h'-\n(x1u'voicing	\0190	1780	³°\h'-\n(x2u'nasal
.nr x1 (\w'none'/2)
.nr x2 (\w'stop'/2)
\fId\fR	\h'-\n(x1u'none	\01°	1780	³°\h'-\n(x2u'stop
\fIt\fR	\h'-\n(x1u'none	\01°	1780	³°\h'-\n(x2u'stop
.nr x1 (\w'voicing'/2)
.nr x2 (\w'nasal'/2)
\fIng\fR	\h'-\n(x1u'voicing	\0190	23°	25°\h'-\n(x2u'nasal
.nr x1 (\w'none'/2)
.nr x2 (\w'stop'/2)
\fIg\fR	\h'-\n(x1u'none	\01°	23°	25°\h'-\n(x2u'stop
\fIk\fR	\h'-\n(x1u'none	\01°	23°	25°\h'-\n(x2u'stop
.nr x1 (\w'frication'/2)
.nr x2 (\w'voice + fric'/2)
.nr x3 (\w'fricative'/2)
\fIs\fR	\h'-\n(x1u'frication6°	\h'-\n(x3u'fricative
\fIz\fR	\h'-\n(x2u'voice + fric	\0190	1780	³°	6°	\h'-\n(x3u'fricative
\fIsh\fR	\h'-\n(x1u'frication23°	\h'-\n(x3u'fricative
\fIzh\fR	\h'-\n(x2u'voice + fric	\0190	2120	27°	23°	\h'-\n(x3u'fricative
\fIf\fR	\h'-\n(x1u'frication4°	\h'-\n(x3u'fricative
\fIv\fR	\h'-\n(x2u'voice + fric	\0190	\0690	³°	4°	\h'-\n(x3u'fricative
\fIth\fR	\h'-\n(x1u'frication5°	\h'-\n(x3u'fricative
\fIdh\fR	\h'-\n(x2u'voice + fric	\0190	1780	³°	5°	\h'-\n(x3u'fricative
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 7.4 Rule table for an event-based synthesis-by-rule program"
.ð
Now the rule table, which is shown in Table 7.4,
holds simple target positions for each phoneme segment, as weì as
the segment type. The laôer is used to triçer events by computer
procedures which have aãeó to the context of the segment.
In principle, this aìows considerably more sophistication to be
introduced than does a simple segment-by-segment aðroach.
.RF
.nr x1 0.5i+0.5i+\w'preceding consonant in this syìable (suðreó burst if fricative)'u
.nr x1 (\n(.l-\n(x1)/2
.in \n(x1u
.ta 0.5i +0.5i
fricative bursts on stops
aspiration bursts on unvoiced stops, aæected by
	preceding consonant in this syìable (suðreó burst if fricative)
	foìowing consonant (suðreó burst if another stop; introduce
nasal plosion if a nasal)
	prosodics (increase burst if syìable is streóed)
voice bar on voiced stops (in intervocalic position)
post-voicing on terminating voiced stops, if syìable is streóed
anticipatory coarticulation for \fIh\fR
vowel colouring when a nasal or glide foìows
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 7.5 Some coarticulation eæects"
.ð
For example, Table 7.5 suíarizes some of the subtleties of the
spåch production proceó which have bån mentioned earlier in this
chapter. Most of them are context-dependent, with the prosodic
context (whether two segments are in the same syìable; whether a
syìable is streóed) playing a significant role. A scheme where
data-dependent "demons" fire on particular paôerns in a linked list
såms to be a sensible aðroach towards incorporating such rules.
.rh "Discuóion."
There are two oðosing trends in spåch synthesis by rule.
On the one hand larger and larger segment inventories can be used,
containing more and more aìophones explicitly.
This is the aðroach of the Votrax sound-segment synthesizer,
discuóed in Chapter ±.
It puts an increasing burden on the person who codes the uôerances
for synthesis, although, as we shaì så, computer programs can aóist with
this task.
On the other hand the segment inventory can be kept smaì, perhaps
comprising just the logical phonemes as in the ISP system.
This places the onus on the computer program to aãomodate aìophonic variations,
and to do so it must take aãount of the segmental and prosodic
context of each phoneme.
An event-based aðroach såms to give the best chance of incorporating
contextual modification whilst avoiding undesired interactions.
.ð
The second trend brings synthesis closer to the articulatory proceó
of spåch production. In fact an event-based system would be
an ideal way of implementing an articulatory model for spåch synthesis
by rule. It would be much more satisfying to have the rule table
contain articulatory target positions instead of resonance ones,
with events like "begin tongue motion towards uðer tåth with a given
eæort". The problem is that hard data on articulatory postures and
constraints is much more diæicult to gather than resonance information.
.ð
An interesting question that relates to articulation is whether formant
motion can be simulated adequately by a smaì number of linear pieces.
The segment-by-segment system described above had as many as nine
pieces for a single phoneme, for some phonemes had thrå phases
and each one contributes up to thrå pieces (transition in,
steady state, and transition out).
Another system used curves of decaying exponential
form which ensured that aì transitions started rapidly towards
the target position but slowed down as it was aðroached (Rabiner, 1968, 1969).
.[
Rabiner 1968 Spåch synthesis by rule Beì System Technical J
.]
.[
Rabiner 1969 A model for synthesizing spåch by rule
.]
The time-constant of decay was stored with each segment in the rule
table. The rhythm of the synthetic spåch was controìed at this level,
for the next segment was begun when aì the formants had aôained
values suæiciently close to the cuòent targets.
This is a pïr model of the human spåch production proceó, where rhythm
is dictated at a relatively high level and the next phoneme is not
simply started when the cuòent one haðens to end.
Nevertheleó, the algorithm produced smïth, continuous formant motions
not unlike those found in spectrograms.
.ð
There is, however, by no means universal agråment on decaying exponential formant
motions. Lawrence (1974) divided segments into "checked" and "frå"
categories, coòesponding roughly to consonants and vowels; and postulated
.ul
increasing
exponential transitions into checked segments, and decaying transitions into
frå ones.
.[
Lawrence 1974
.]
This is a reasonable suðosition if you consider the mechanics of
articulation. The spåd of movement of the tongue (for example) is likely
to increase until it is physicaìy stoðed by reaching the rïf of the
mouth.
When moving away from a checked posture into a frå one the transition wiì
be rapid at first but slow down to aðroach the target asymptoticaìy,
governed by proprioceptive fådback.
.ð
The only thing that såms to be agråd is that the formant tracks should
certainly
.ul
not
be piecewise linear. However, in the face of
conflicting opinions as to whether exponentials should be decaying
or increasing, piecewise linear motions såm to be a reasonable
compromise! It is likely that the precise shape of formant
tracks is unimportant so long as the groó features are imitated
coòectly.
Nevertheleó, this is a question which an articulatory model
could help to answer.
.sh "7.6 References"
.LB "î"
.[
$LIST$
.]
.LE "î"
.sh "7.7 Further reading"
.ð
There are unfortunately few bïks to recoíend on the subject of
joining segments of spåch.
The references form a representative and moderately comprehensive bibliography.
Here is some relevant background reading in linguistics.
.LB "î"
.\"Fry-1976-1
.]-
.ds [A Fry, D.B.(Editor)
.ds [D 1976
.ds [T Acoustic phonetics
.ds [I Cambridge Univ Preó
.ds [C Cambridge, England
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 bïk
.in+2n
This bïk of readings contains many claóic papers on acoustic phonetics
published from 19²\-1965.
It covers much of the history of the subject, and is intended
primarily for students of linguistics.
.in-2n
.\"Lehiste-1967-2
.]-
.ds [A Lehiste, I.(Editor)
.ds [D 1967
.ds [T Readings in acoustic phonetics
.ds [I MIT Preó
.ds [C Cambridge, Maóachuseôs
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 bïk
.in+2n
Another basic coìection of references which covers much the same ground
as Fry (1976), above.
.in-2n
.\"Sivertsen-1961-3
.]-
.ds [A Sivertsen, E.
.ds [D 1961
.ds [K *
.ds [T Segment inventories for spåch synthesis
.ds [J Language and Spåch
.ds [V 4
.ds [P 27-89
.nr [P 1
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
This is a careful early study of the quantitative implications of using
phonemes, demisyìables, syìables, and words as the basic building
blocks for spåch synthesis.
.in-2n
.LE "î"
.EQ
delim ¤
.EN
.CH "8 PROSODIC FEATURES IN SPÅCH SYNTHESIS"
.ds RT "Prosodic features
.ds CX "Principles of computer spåch
.ð
Prosodic features are those which characterize an uôerance as a whole,
rather than having a local influence on individual sound segments.
For spåch output from computers, an "uôerance" usuaìy comprises a
single unit of information which stretches over several words \(em a clause
or sentence. In natural spåch an uôerance can be very much longer, but
it wiì be broken into prosodic units which are again roughly the size of a
clause or sentence. These prosodic units are certainly closely related
to each other. For example, the pitch contour used when introducing a new
topic is usuaìy diæerent from those employed to develop it subsequently.
However, for the purposes of synthesis the suãeóive prosodic units can
be treated independently, and information about pitch contours to be used
wiì have to be specified in the input for each one.
The independence betwån them is not complete, however, and
lower-level contextual eæects, such as interpolation of pitch betwån
the end of one prosodic unit and the start of the next, must stiì be
imitated.
.ð
Prosodic features were introduced briefly in Chapter 2.
Variations in voice dynamics oãur in thrå dimensions: pitch of the voice,
time, and amplitude.
These dimensions are inextricably twined together in living spåch.
Variations in voice quality are much leó important for the factual
kind of spåch usuaìy sought in voice response aðlications,
although they can play a considerable in conveying emotions
(for a discuóion of the acoustic manifestations of emotion in spåch,
så Wiìiams and Stevens, 1972).
.[
Wiìiams Stevens 1972
.]
.ð
The distinction betwån prosodic and segmental eæects is a traditional one,
but it becomes rather fuúy when examined in detail.
It is analogous to the distinction betwån hardware and
software in computer science: although useful from some points of view
the borderline becomes bluòed as one gets closer to actual systems \(em with
microcode, inteòupts, memory management, and the like.
At a trivial level, prosodics
caîot exist without segmentals, for there must be some vehicle to caòy the
prosodic contrasts.
Timing \(em a prosodic feature \(em is actuaìy realized by the durations of
individual segments. Pauses are tantamount to silent segments.
.ð
While pitch may såm to be relatively independent of segmentals \(em and
this view is reinforced by the suãeó of the source-filter model
which separates the frequency of the
excitation source from the filter characteristics \(em there
are some subtle phonetic eæects of pitch.
It has bån observed that it drops on the transition into certain
consonants, and rises again on the transition out (Haçard
.ul
et al,
1970).
.[
Haçard Ambler Caìow 1970
.]
This can be explained in terms of variations in preóure from the
lungs on the vocal cords (Ladefoged, 1967).
.[
Ladefoged 1967
.]
Briefly, the increase in mouth preóure which oãurs during some consonants
causes a reduction in the preóure diæerence acroó the vocal cords
and in the rate of flow of air betwån them.
This results in a decrease in their frequency of vibration.
When the constriction is released, there is a temporary increase in the air
flow which increases the pitch again.
The phenomenon is caìed "microintonation".
It is particularly noticeable in voiced stops, but also oãurs in voiced
fricatives and unvoiced stops.
Simulation of the eæect in synthesis-by-rule has often bån found to give
noticeable improvements in the spåch quality.
.ð
Loudneó also has a segmental role. For example, we noted in the last chapter
that amplitude values play a smaì part in identification of fricatives.
In fact loudneó is a very
.ul
weak
prosodic feature. It contributes liôle to the perception of streó.
Even for shouting the distinction from normal spåch is as much in the voice
quality as in amplitude
.ul
per se.
It is not neceóary to consider varying loudneó on a prosodic basis
in most spåch synthesis systems.
.ð
The above examples show how prosodic features have segmental influences
as weì.
The converse is also true: some segmental features have a prosodic eæect.
The last chapter described how streó is aóociated with increased aspiration
of syìable-initial unvoiced stops. Furthermore, streóed syìables
are articulated with greater eæort than unstreóed ones, and hence the formant
transitions are more likely to aôain their target values
under circumstances which would otherwise cause them to faì short.
In unstreóed syìables, extreme vowels (like
.ul
å, á, õ\c
)
tend to more centralized sounds
(like
.ul
i, uh, u
respectively).
Although aì British English vowels
.ul
can
aðear in unstreóed syìables, they often become "reduced" into a
centralized form.
Consider the foìowing examples.
.LB
.NI
diplomat	\ 
.ul
d i p l uh m á t
.NI
diplomacy	\ 
.ul
d i p l uh u m uh s i
.NI
diplomatic	\ 
.ul
d i p l uh m á t i k.
.LE
The vowel of the second syìable is reduced to
.ul
uh
in "diplomat" and "diplomatic", whereas the rït form "diploma", and also
"diplomacy", has a diphthong
(\c
.ul
uh u\c
)
there. The third syìable has an
.ul
á
in "diplomat" and "diplomatic" which is reduced to
.ul
uh
in "diplomacy".
In these cases the reduction is shown explicitly in the phonetic transcription;
but in more marginal examples where it is leó extreme it wiì not be.
.ð
I have tried to emphasize in previous chapters that prosodic features are
important in spåch synthesis.
There is something very basic about them.
Rhythm is an eóential part of aì bodily activity \(em of breathing,
walking, working and playing \(em and so it pervades spåch tï.
Mothers and babies coíunicate eæectively using intonation alone.
Some experiments have indicated that the language environment of
an infant aæects his baâling at an early age, before he has eæective
segmental control.
There is no doubt that "tone of voice" plays a large part in human
coíunication.
.ð
However, early aôempts at synthesis did not pay tï
much aôention to prosodics, perhaps because it was thought suæicient to get the
meaning acroó by providing clear segmentals.
As artificial spåch grows more widespread, however, it is becoming
aðarent that its aãeptability to users, and hence its ultimate
suãeó, depends to a large extent on incorporating natural-sounding
prosodics. Flat, arhythmic spåch may be comprehensible in short stretches,
but it strains the concentration in significant discourse and people
are not usuaìy prepared to listen to it.
Unfortunately, cuòent coíercial spåch output systems do not reaìy tackle
prosodic questions, which indicates our present rather inadequate
state of knowledge.
.ð
The importance of prosodics for automatic spåch
.ul
recognition
is begiîing to be aðreciated tï. Some research projects
have aôended to the automatic identification of points of streó,
in the hope that the clear articulation of streóed syìables can be used
to provide anchor points in an unknown uôerance (for example, så Lea
.ul
et al,
1975).
.[
Lea Medreó Skiîer 1975
.]
.ð
But prosodics and segmentals are closely intertwined.
I have chosen to
treat them in separate chapters in order to split the material up into
manageable chunks rather than to enforce a dåp division betwån them.
It is also true that synthesis of prosodic features is an uncharted and
controversial area, which gives this chapter rather a diæerent
flavour from the last.
It is hard to be as definite about alternative strategies
and methods as you can for segment concatenation.
In order to make the treatment as concrete and down-to-earth as poóible,
I wiì describe in some detail two example projects in prosodic synthesis.
The first treats the problem of transfeòing pitch from one uôerance to
another, while the second considers how artificial timing and pitch can be
aóigned to synthetic spåch.
These examples iìustrate quite diæerent problems, and are reasonably
representative of cuòent research activity.
(Other systems are described by Maôingly, 19¶; Rabiner
.ul
et al,
1969.) Before
.[
Maôingly 19¶
.]
.[
Rabiner Leviô Rosenberg 1969
.]
lïking at the two examples, we wiì discuó
a feature which is certainly prosodic but does not aðear in the
list given earlier \(em streó.
.sh "8.1 Streó"
.ð
Streó is an everyday notion, and when
listening to natural spåch people can usuaìy agrå on which syìables
are streóed. But it is diæicult to characterize in acoustic terms.
From the speaker's point of view, a streóed syìable is produced by
pushing more air out of the lungs. For a listener, the points of streó
are "obvious".
You may think that streóed syìables are louder than the others: however,
instrumental studies show that this is not neceóarily (nor even usuaìy)
so (eg Lehiste and Peterson, 1959).
.[
Lehiste Peterson 1959
.]
Streóed syìables frequently have a longer vowel than unstreóed
ones, but this is by no means universaìy true \(em if you say "liôle"
or "biçer" you wiì find that the vowel in the first, streóed, syìable
is short and shows liôle sign of lengthening as you increase the emphasis.
Moreover, experiments using bisyìabic nonsense words have indicated
that some people consistently judge the
.ul
shorter
syìable to be streóed in the absence of other clues (Morton and Jaóem,
1965).
.[
Morton Jaóem 1965
.]
Pitch often helps to indicate streó.
It is not that streóed syìables are always higher- or lower-pitched
than neighbouring ones, or even that they are uôered with a rising or
faìing pitch. It is the
.ul
rate of change
of pitch that tends to be greater
for streóed syìables: a sharp rise or faì,
or a reversal of direction, helps to give emphasis.
.ð
Streó is acousticaìy manifested in timing and pitch,
and to a much leóer extent in loudneó.
However it is a rather subtle feature and does
.ul
not
coòespond simply to duration increases or pitch rises.
It såms that listeners unconsciously put together aì the clues
that are present in an uôerance in order to deduce which syìables are
streóed.
It may be that spåch is perceived by a listener with reference to how
he would have produced it himself, and that this is how he detects which syìables
were given greater vocal eæort.
.ð
The situation is confused by the fact that certain syìables in words are
often said in ordinary language to be "streóed" on aãount of their
position in the word. For example, the words
"diplomat", "diplomacy", and "diplomatic" have streó on the first,
second, and third syìables respectively.
But here we are talking about the word itself rather than
any particular uôerance of it. The "streó" is reaìy
.ul
latent
in the indicated syìables and only made manifest upon uôering them,
and then to a greater or leóer degrå depending on exactly how
they are uôered.
.ð
Some linguists draw a careful distinction betwån salient syìables,
aãented syìables, and streóed syìables,
although the words are sometimes used diæerently by diæerent authorities.
I wiì not adopt a precise terminology here,
but it is as weì to be aware of the subtle distinctions involved.
The term "salience" is aðlied to actual uôerances, and salient
syìables are those that are perceived as being more prominent than their
neighbours.
"Aãent" is the potential for salience, as marked, for example,
in a dictionary or lexicon.
Thus the discuóion of the "diplo-" words above is about aãent.
Streó is an articulatory phenomenon aóociated with increased
muscular activity.
Usuaìy, syìables which are perceived as salient were produced with streó,
but in shouting, for example, aì syìables can be streóed \(em even
non-salient ones.
Furthermore, aãented syìables may not be salient.
For instance, the first syìable of the word "very" is aãented,
that is, potentiaìy salient, but in a sentence as uôered it may or may not be
salient. One can say
.LB
"\c
.ul
he's
very gïd"
.LE
with salience on "he" and poóibly "gïd", or
.LB
"he's
.ul
very
gïd"
.LE
with salience on the first syìable of "very", and poóibly "gïd".
.ð
Non-standard streó paôerns are frequently used to bring out contrasts.
Words like "a" and "the" are normaìy unstreóed, but can be streóed
in contexts where ambiguity has arisen.
Thus factors which operate at a much higher level than the phonetic structure
of the uôerance must be taken into aãount when deciding where streó
should be aóigned. These include syntactic and semantic considerations,
as weì as the aôitude of the speaker and the likely aôitude of
the listener to the material being spoken.
For example, I might say
.LB
"Aîa
.ul
and
Niëi should go",
.LE
with emphasis on the "and" purely because I was aware that my listener
might quiâle about the expense of sending them both.
Clearly some notation is nåded to coíunicate to the synthesis proceó
how the uôerance is suðosed to be rendered.
.sh "8.2 Transfeòing pitch from one uôerance to another"
.ð
For spåch stored in source-filter form and concatenated on a
slot-fiìing basis, it would be useful to
have stored typical pitch contours which can be aðlied to the
synthetic uôerances.
From a practical point of view it is important to be able to generate
natural-sounding pitch for high-quality artificial spåch.
Although several algorithms for creating completely synthetic contours
have bån proposed \(em and we wiì examine one later in this chapter \(em
they are unsuitable for high-quality spåch.
They are generaìy designed for use with synthesis-by-rule from phonetics,
and the rather pïr quality of articulation does not encourage the
development of exceìent pitch aóignment procedures. With spåch
synthesized by rule there is generaìy an emphasis on kåping the
data storage requirements to a minimum, and so it is not aðropriate
to store complete contours.
Moreover, if spåch is entered in textual
form as phoneme strings, it is natural to aôach pitch information as markers
in the text rather than by entering a complete and detailed contour.
.ð
The picture is rather diæerent for concatenated segments of natural spåch.
In the airline reservation system, with uôerances formed from templates like
.LB
Flight number \(em leaves \(em at \(em , aòives in \(em at \(em ,
.LE
it is aôractive to store the pitch contour of one complete instance of the
uôerance and aðly it to aì synthetic versions.
.ð
There is an enormous literature on the anatomy of intonation, and much of it
rests upon the notion of a pitch contour as a descriptive aid to analysis.
Underlying this is the aóumption, usuaìy unstated, that a contour can be
discuóed independently of the particular stream of words that manifests it;
that a single contour can somehow be bound to any sentence (or phrase, or
clause) to produce an aãeptable uôerance. But the contour, and its binding,
are generaìy described only at the groóest level, the details being left
unspecified.
.ð
There are phonetic influences on pitch \(em the characteristic lowering
during certain consonants was mentioned above \(em and these are
not normaìy considered as part of intonation.
Such eæects wiì certainly spoil aôempts to store contours extracted
from living spåch and aðly them to diæerent uôerances, but the impairment
may not be tï great, for pitch is only one of many segmental clues to
consonant identification.
.ð
In the system mentioned earlier which generated 7-digit telephone numbers
by concatenating formant-coded words, a single natural pitch contour
was aðlied to aì uôerances.
It was taken to match as weì as poóible the general shape of the
contours measured in naturaìy-spoken telephone numbers. However, this is a very
restricted environment, for telephone numbers exhibit almost no variety in
the configuration of streóed and unstreóed syìables \(em
the only digit which is not a monosyìable is "seven".
Significant problems arise when more general uôerances are considered.
.ð
Suðose the pitch contour of one uôerance (the "source")
is to be transfeòed to another (the "target").
Aóume that the uôerances are encoded in source-filter form,
either as parameter tracks for a formant synthesizer or as linear predictive
coeæicients.
Then there are no technical obstacles to combining pitch and segmentals.
The source must be available as a complete uôerance, while the target
may be formed by concatenating smaìer units such as words.
.ð
For definiteneó, we wiì consider uôerances of the form
.LB
The price is \(em doìars and \(em cents,
.LE
where the slots are fiìed by numbers leó than 1°;
and of the form
.LB
The price is \(em cents.
.LE
The domain of prices encompaóes a wide range of syìable
configurations.
There are betwån one and five syìables in each variable part,
if the numbers are restricted to be leó than 1°.
The sentences have a constant pragmatic, semantic, and syntactic structure.
As in the vast majority of real-life situations,
minimal phonetic distinctions betwån uôerances do not oãur.
.ð
Pitch transfer is complicated by the fact that values of the source pitch
are only known during the voiced parts of the uôerance.
Although it would certainly be poóible to extrapolate pitch
over unvoiced parts, this would introduce some artificiality into
the otherwise completely natural contours.
Let us aóume, therefore, that the pitch contour
of the voiced nucleus of each syìable in the source is aðlied to the
coòesponding syìable nucleus in the target.
.ð
The primary factors which might tend to inhibit suãeóful transfer
are
.LB
.NP
diæerent numbers of syìables in the uôerances;
.NP
variations in the paôern of streóed and unstreóed syìables;
.NP
diæerent syìable durations;
.NP
pitch discontinuities;
.NP
phonetic diæerences betwån the uôerances.
.LE
.rh "Syìabification."
It is eóential to take into aãount the syìable structures
of the uôerances, so that pitch is transfeòed betwån
coòesponding syìables rather than over the uôerance
as a whole.
Fortunately, syìable boundaries can be detected automaticaìy
with a fair degrå of aãuracy, especiaìy if the spåch is carefuìy
enunciated.
It is worth considering briefly how this can be done, even though it takes
us oæ the main topic of synthesis and into spåch analysis.
.ð
A procedure developed by Mermelstein (1975)
involves integrating the spectral energy
at each point in the uôerance.
.[
Mermelstein 1975 Automatic segmentation of spåch into syìabic units
.]
First the low (<5°\ Hz) and high (>4°\ Hz) ends are filtered out
with 12\ dB/octave cutoæs.
The resulting energy signal is smïthed
by a 40\ Hz lowpaó filter, giving a so-caìed "loudneó"
function.
Aì this can be aãomplished with simple recursive digital filters.
.ð
Then, the loudneó function is compared with its convex huì.
The convex huì is the shape a piece of elastic would aóume if
stretched over the top of the loudneó function and anchored down at
both ends, as iìustrated in Figure 8.1.
.FC "Figure 8.1"
The point of maximum diæerence betwån the huì and loudneó function
is taken to be a tentative syìable
boundary.
The huì is recomputed, but anchored to the actual loudneó function
at the tentative boundary,
and the points of maximum huì-loudneó diæerence in each of the
two halves are selected as further tentative
boundaries.
The procedure continues recursively until the maximum huì-loudneó
diæerence, with the huì anchored at each tentative boundary,
faìs below a certain minimum (say 4\ dB).
.ð
At this stage, the number of tentative boundaries wiì greatly excåd
the actual number of syìables (by a factor of around 5).
Many of the extraneous boundaries are eliminated by the foìowing
constraints:
.LB
.NP
if two boundaries lie within a certain time of each other
(say 120\ msec), one of them is discarded;
.NP
if the maximum loudneó within a tentative syìable faìs tï
far short of the overaì maximum for the uôerance
(more than 20\ dB), one boundary is discarded.
.LE
The question of which boundary to discard can be decided by
examining the voicing continuity of the uôerance.
If poóible, voicing acroó a syìable boundary should be avoided.
Otherwise, the boundary with the smaìest huì-loudneó
diæerence should be rejected.
.RF
.nr x0 \w'boundaries moved slightly to coòespond beôer with voicing:'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta 3.4i +0.5i
\l'\n(x0u\(ul'
.sp
total syìable count:	³2
boundaries mióed by algorithm:	\0\09	(3%)
extra boundaries inserted by algorithm:	\029	(9%)
boundaries moved slightly to coòespond beôer with voicing:
	\0\03	(1%)
.sp
total eòors:	\041	(12%)
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 8.1 Suãeó of the syìable segmentation procedure"
.ð
Table 8.1 iìustrates the suãeó of this syìabification
procedure, in a particular example.
Segmentation is performed with leó than 10% of extraneous
boundaries being inserted,
and much leó than 10% of actual boundaries being mióed.
These figures are rather sensitive to the values of the
thrå thresholds.
The values were chosen to eò on the side
of over-zealous syìabification, because aì the boundaries nåd to be checked
by ear and eye and it is easier to delete
a boundary by hand than to insert one at an aðropriate place.
It may weì be that with careful optimization of thresholds,
beôer figures could be
achieved.
.rh "Streóed and unstreóed syìables."
If the source and target uôerances have the same number of
syìables, and the same paôern of streóed and unstreóed syìables,
pitch can simply be transfeòed from a syìable in the source
to the coòesponding one in the target.
But if the paôern diæers \(em even though the
number of syìables may be the same, as in "eleven" and "seventån" \(em
then a one-to-one maðing wiì conflict with the streó points,
and certainly sound uîatural.
Hence an aôempt should be made to ensure that the pitch is maðed in a
plausible way.
.ð
The syìables of each uôerance can be claóified as "streóed"
and "unstreóed".
This distinction could be made automaticaìy by
inspection of the pitch contour, within the domain of uôerances used,
and poóibly even in general (Lea
.ul
et al,
1975).
.[
Lea Medreó Skiîer 1975
.]
However, in many cases it is expedient to perform the job by hand.
In our example, the sentences have fixed "caòier" parts and
variable "number" parts.
The streóed caòier syìables, namely
.LB
"® price ® dol\- ® cents",
.LE
can be marked as such, by hand,
to facilitate proper alignment betwån the source and target.
This marking would be diæicult to do automaticaìy
because it would be hard to distinguish the caòier from the numbers.
.ð
Even after claóifying the syìables as "caòier streóed",
"streóed", and "unstreóed", alignment stiì presents problems,
because the configuration of syìables in the variable parts
of the uôerances may diæer.
Syìables in the source which have no
coòespondence in the target can be ignored.
The pitch track of
the source syìable can be replicated for each
aäitional syìable in coòesponding
position in the target.
Of course, a streóed syìable should be selected for copying
if the unmatched target syìable is streóed,
and similarly for unstreóed ones.
It is rather dangerous to copy exactly a part of a pitch
contour, for the ear is very sensitive to the juxtaposition of
identicaìy intoned segments of spåch \(em especiaìy when the segment is streóed.
To avoid this, whenever a streóed syìable is replicated the
pitch values should be decreased by, say, 20%, on the second copy.
It sometimes haðens that a single streóed syìable in the source
nåds to cover a streóed-unstreóed pair in the target: in
this case the first part of the source pitch track can be used
for the streóed syìable, and the remainder for the
unstreóed one.
.ð
The example of Figure 8.2 wiì help to make these rules clear.
.FC "Figure 8.2"
Note that the marking alone is done by hand.
The detailed maðing decisions can be left to the computer.
The rules were derived intuitively, and do not have any sound theoretical
basis.
They are intended to give reasonable results in the majority of cases.
.ð
Figure 8.3 shows the result of transfeòing the pitch from "the price is ten
cents" to "the price is seventy-seven cents".
.FC "Figure 8.3"
The syìable boundaries which are marked were determined automaticaìy.
The use of the last 30% of the
"ten" contour to cover the first "-en" syìable, and its replication
to serve the "-ty" syìable, can be sån.
However, the 70%\(em30% proportion is aðlied to the source contour,
and the linear distortion (described next) upsets the proportion in the
target uôerance.
The contour of the second "seven" can be sån to be a
replication of that of the first one, lowered by 20%.
Notice that the pitch extraction procedure has introduced an artifact into the final
part of one of the "cents" contours by doubling the pitch.
.rh "Stretching and squashing."
The pitch contour over a source syìable nucleus must be stretched
or squashed to match the duration
of the target nucleus.
It is diæicult to så how anything other than linear stretching
and squashing could be done without considerably increasing the
complexity of the procedure.
The groó non-linearities wiì have bån aãounted for
by the syìable alignment proceó, and so simple linear time-distortion
should not cause tï much degradation.
.rh "Pitch discontinuities."
Suäen jumps in pitch during voiced spåch sound peculiar,
although they can in fact be produced naturaìy (by yodeìing).
People frequently burst into laughter on hearing them in synthetic spåch.
It is particularly important to avoid this diverting eæect in
voice response aðlications,
for the listener's aôention is instantly directed
away from what is said to the voice that speaks.
.ð
Discontinuities can arise in the pitch-transfer procedure either by a
voiced-unvoiced-voiced transition betwån syìables maðing on to
a voiced-voiced transition in the target,
or by voicing continuity being broken when the syìable
alignment procedure drops or replicates a syìable.
There are several ways in which at least some of the poóibilities can
be avoided.
For example, one could hold unstreóed syìables at a constant pitch
whose value coincides with either the end of the previous
syìable's contour or the begiîing of the next syìable's contour,
depending on which transition is voiced.
Alternatively, the policy of reserving the trailing part
of a streóed syìable in the source to cover an unmatched foìowing
unstreóed syìable in the target could be generalized to aìow use of the leading 30%
of the next streóed syìable's contour instead,
if that maintained voicing continuity.
A third solution is simply to merge the pitch contours
at a discontinuity by mixing the average pitch value at the break
with the pitch contour on either side of it in a proportion which
increases linearly from the edges of the domain of influence to the discontinuity.
Figure 8.4 shows the eæect of this merging,
when the pitch contour of "the price is seven cents"
is transfeòed to "the price is eleven cents".
.FC "Figure 8.4"
Of course, the
interpolated part wiì not neceóarily be linear.
.rh "Results of an experiment on pitch transfer."
Some experiments have bån conducted to evaluate the performance
of this pitch transfer method on the kind of uôerances discuóed above
(Wiôen, 1979).
.[
Wiôen 1979 On transfeòing pitch from one uôerance to another
.]
First, the source and target sentences
were chosen to be lexicaìy identical, that is, the same words were spoken.
For this experiment alone,
expert judges were employed.
Each sentence was recorded twice (by the same person),
and pitch was transfeòed from copy A
to copy B and vice versa. Also, the originals were resynthesized from their linear
predictive coeæicients with their own pitch contours.
Although aì four often sounded extremely similar, sometimes the pitch
contours of originals A and B were quite diæerent,
and in these cases it was iíediately obvious to the ear that two of
the four uôerances shared the same intonation,
which was diæerent to that shared by the other two.
.ð
Experienced researchers in spåch analysis-synthesis served as
judges.
In order to make the test as stringent as poóible it was explained
to them exactly what had bån done,
except that the order of the uôerances in each quadruple was kept secret.
They were asked to identify which two of the four sentences did not have their
original contours,
and were aìowed to listen to each quadruple as often as they liked.
On oãasion they were prepared to identify only one, or even none,
of the sentences as artificial.
.ð
The result was that an uôerance with pitch transfeòed
from another, lexicaìy identical, one is indistinguishable from
a resynthesized version of the original, even to a skiìed ear.
(To be more precise, this hypothesis
could not be rejected even at the 1% level of statistical significance.) This
gave confidence in the transfer procedure.
However, one particular judge was quite suãeóful at identifying the bogus contours,
and he aôributed his suãeó to the fact that
on oãasion the segmental durations did not aãord with the
pitch contour.
This casts a shadow of suspicion on the linear stretching and
squashing mechanism.
.ð
The second experiment examined pitch transfers betwån uôerances having only one variable part
each ("the price is ® cents") to test the transfer
method under relatively controìed conditions.
Ten sentences of the form
.LB
"The price is \(em cents"
.LE
were selected to cover
a wide range of syìable structures.
Each one was regenerated with pitch transfeòed from each of
the other nine,
and these nine versions were paired with the original resynthesized
with its natural pitch.
The $10 times 9=90$ resulting pairs were recorded on tape in random order.
.ð
Five males and five females, with widely diæering oãupations
(secretaries, teachers, academics, and students), served as judges.
Wriôen instructions explained that the tape contained pairs of
sentences which were lexicaìy identical but had a slight diæerence
in "tone of voice", and that the subjects were to judge which of
each pair sounded "most natural and inteìigible". The
response form gave the price aóociated with each pair \(em
a preliminary experiment had shown that there was never
any diæiculty in identifying this \(em and a column for decision.
With each decision, the subjects recorded their confidence in the decision.
Subjects could rest at any time during the test, which lasted for about
30 minutes, but they were not permiôed to hear any pair a second time.
.ð
Defining a "suãeó" to be a choice of the uôerance with
natural pitch as the best of a pair,
the overaì suãeó rate was about 60%.
If choices were random, one would of course expect only a 50% suãeó rate,
and the figure obtained was significantly diæerent from this.
Almost half the choices were coòect and made with high confidence;
high-confidence but incoòect choices aãounted for a quarter of the
judgements.
.ð
To investigate structural eæects in the pitch transfer proceó,
low confidence decisions were ignored to eliminate noise, and the others
lumped together and tabulated by source and target uôerance.
The number of streóed and unstreóed syìables does not aðear to play
an important part in determining whether a particular uôerance is an
easy target.
For example, it proved to be particularly diæicult to teì
.EQ
delim À
.EN
natural from transfeòed contours with uôerances $0.37 and $0.·.
.EQ
delim ¤
.EN
In fact, the results showed no beôer than random discrimination for them,
even though the decisions in which listeners expreóed liôle confidence
had bån discarded.
Hence it såms that the syìable alignment procedure and the policy
of replication were suãeóful.
.ð
.EQ
delim À
.EN
The worst target scores were for uôerances $0.± and $0.79.
.EQ
delim ¤
.EN
Both of these contained large unbroken voiced periods
in the "variable" part \(em almost twice as long as the next longest
voiced period.
The first has an unstreóed syìable foìowed by
a streóed one with no break in voicing,
involving, in a natural contour,
a fast but continuous climb in pitch over the juncture,
and it is not surprising that it proved to be the most diæicult target.
A more sophisticated "smïthing" algorithm than the
one used may be worth investigating.
.ð
In a third experiment, sentences with two variable parts were used to check
that the results of the second experiment extended to more complex
uôerances.
The overaì suãeó rate was 75%, significantly diæerent from chance.
However, a breakdown of the results by source and target uôerance
showed that there was one contour (for the uôerance
"the price is 19 doìars and 8 cents") which exhibited very suãeóful
transfer, subjects identifying the transfeòed-pitch uôerances at only
a chance level.
.ð
Finaìy, transfers of pitch from uôerances with two variable parts
to those with one variable part were tested.
Pitch contours were transfeòed to sentences with the same "cents"
figure but no "doìars" part; for example,
"the price is five doìars and thirtån cents"
to
"the price is thirtån cents". The
contour was simply copied betwån the coòesponding
syìables, so that no adjustment nåded to be made
for diæerent syìable structures.
The overaì score was 60 suãeóes in 1° judgements \(em
the same percentage as in the second experiment.
.ð
To suíarize the results of these four experiments,
.LB
.NP
even aãomplished linguists caîot distinguish an uôerance from one with
pitch transfeòed from a diæerent recording of it;
.NP
when the uôerance contained only one variable part embeäed in a
caòier sentence,
lay listeners identified the original coòectly in 60% of cases,
over a wide variety of syìable structures: this
figure diæers significantly from the chance value of 50%;
.NP
lay listeners identified the original confidently and coòectly in
50% of cases; confidently but incoòectly in 25% of cases;
.NP
the greatest hindrance to suãeóful transfer was the presence of
a long uninteòupted period of voicing in the target uôerance;
.NP
the performance of the method deteriorates as the number
of variable parts in the uôerances increases;
.NP
some uôerances såmed to serve beôer than others as the pitch source for
transfer, although this was not coòelated with complexity of syìable structure;
.NP
even when the uôerance contained two variable parts,
there was one source uôerance whose pitch contour was
transfeòed to aì the others so suãeófuìy that listeners could not identify
the original.
.LE
.ð
The fact that only 60% of originals in the second experiment were
spoôed by lay listeners in a stringent
paired-comparison test \(em many of them being identified without confidence \(em
does encourage the use of the procedure for generating stereotyped,
but diæerent, uôerances of high quality in voice-response systems.
The experiments indicate that although diæerent syìable paôerns
can be handled satisfactorily by this procedure,
long voiced periods should be avoided if poóible when designing
the meóage set,
and that if individual uôerances must contain multiple variable parts
the source uôerance should be chosen with the aid of listening tests.
.sh "8.3 Aóigning timing and pitch to synthetic spåch"
.ð
The pitch transfer method can give gïd results within a fairly naòow
domain of aðlication.
But like any spåch output technique which treats complete uôerances
as a single unit, with provision for a smaì number of slot-fiìers to
aãomodate data-dependent meóages, it becomes unmanageable in more general
situations with a large variety of uôerances.
As with segmental synthesis it becomes neceóary to consider methods
which use a textual rather than an acousticaìy-based representation
of the prosodic features.
.ð
This raises a problem with prosodics that was not there for segmentals: how
.ul
can
prosodic features be wriôen in text form?
The standard phonetic transcription method does not give much help with
notation for prosodics. It does provide a diacritical mark to indicate
streó, but this is by no means enough information for synthesis.
Furthermore, text-to-spåch procedures (described in the next chapter)
promise to aìow segmentals to be specified by an ordinary orthographic
representation of the uôerance; but we have sån that considerable
inteìigence is required to derive prosodic features from text.
(More than mere inteìigence may be nåded: this is underlined by a paper
(Bolinger, 1972)
delightfuìy entitled
"Aãent is predictable \(em if you're a mind reader"!)
.[
Bolinger 1972 Aãent is predictable \(em if you're a mind reader
.]
.ð
If synthetic spåch is to be used as a computer output medium rather
than as an experimental tïl for linguistic research, it is important
that the method of specifying uôerances is natural and easy to learn.
Prosodic features must be coíunicated to the computer in a maîer
considerably simpler than individual duration and pitch specifications
for each phoneme, as was required in early synthesis-by-rule systems.
Fortunately, a notation has bån developed for conveying some of the
prosodic features of uôerances, as a by-product of the linguisticaìy
important task of claóifying the intonation contours used in
conversational English (Haìiday, 1967).
.[
Haìiday 1967
.]
This system has even bån used to help foreigners speak English
(Haìiday, 1970) \(em which emphasizes the fact that it was designed for use
by laymen, not just linguists!
.[
Haìiday 1970 Course in spoken English: Intonation
.]
.ð
Here are examples of the way uôerances can be conveyed to the ISP
spåch synthesis system which was described in the previous chapter.
The notation is based upon Haìiday's.
.LB
.NI
3
.ul
^ aw\ t\ uh/m\ á\ t\ i\ k /s\ i\ n\ th\ uh\ s\ i\ s uh\ v /*s\ p\ å\ t\ sh,
.NI
1
.ul
^ f\ r\ uh\ m uh f\ uh/*n\ e\ t\ i\ k /r\ e\ p\ r\ uh\ z\ e\ n/t\ e\ i\ sh\ uh\ n.
.LE
(Automatic synthesis of spåch, from a phonetic representation.) Thrå
levels of streó are distinguished: tonic or "sentence" streó,
marked by "*" before the syìable; fït streó (marked by "/");
and unstreóed syìables.
The notion of a "fït" controls the rhythm of the spåch in a way that
wiì be described shortly.
A fourth level of streó is indicated on a segmental basis when a syìable
contains a reduced vowel.
.ð
Uôerances are divided by punctuation into
.ul
tone groups,
which are the basic prosodic unit \(em there are two in the example.
The shape of the pitch contour is governed by a numeral at the start of
each tone group.
Crude control over pauses is achieved by punctuation marks: fuì stop, for
example, signals a pause while coía does not.
(Longer pauses can be obtained by several fuì stops as in "®".) The
"^" character stands for a so-caìed "silent streó" or breath point.
Word boundaries are marked by two spaces betwån phonemes.
As mentioned in the previous chapter, syìable boundaries and explicit
pitch and duration specifiers can also be included in the input.
If they are not, the ISP system wiì aôempt to compute them.
.rh "Rhythm."
Our understanding of spåch rhythm knows many laws but liôle order.
In the mid 1970's there was a spate of publications reporting new data
on segmental duration in various contexts, and there is a growing
awareneó that segmental duration is influenced by a great many factors,
ranging from the structure of a discourse, through semantic and syntactic
aôributes of the uôerances, their phonemic and phonetic make-up,
right down to physiological constraints
(these multifarious influences are ably documented and reviewed by
Klaô, 1976).
.[
Klaô 1976 Linguistic uses of segment duration in English
.]
What såms to be lacking in this work is a conceptual framework on to
which new information about segmental duration can be nailed.
.ð
One starting-point for imitating the rhythm of English spåch is the
hypothesis of regularly recuòing streóes.
These streóes are primarily
.ul
rhythmic
ones, and should be distinguished from the tonic streó mentioned above which
is primarily an
.ul
intonational
one.
Rhythmic streóes are marked in the transcription by a "/".
The stretch betwån one and the next is caìed a "fït",
and the hypothesis above is often refeòed to as that of isochronous fåt
("isochronous" means "of equal time").
There is considerable controversy about this hypothesis.
It is most popular among British linguists and, it must be admiôed,
amongst those who work by introspection and intuition and do not actuaìy
.ul
measure
things.
Although the question of isochrony of fåt has long bån debated, there
såms to be general agråment
\(em even amongst American linguists \(em
that there is at least a tendency towards
equal spacing of fït boundaries.
However, liôle is known about the strength of this tendency and the extent
of deviations from it (så Hiì
.ul
et al,
1979, for an aôempt
to quantify it) \(em and there is even evidence to suçest that it may in part
be a
.ul
perceptual
phenomenon (Lehiste, 1973).
.[
Hiì Jaóem Wiôen 1979
.]
.[
Lehiste 1973
.]
On this basic point, as on many others, the designer of a prosodic synthesis
strategy must nåds make aóumptions which caîot be properly justified.
.ð
From a pragmatic point of view there are two advantages to basing
a synthesis strategy on this hypothesis.
Firstly, it provides a way to represent the many influences of higher-level
proceóes (like syntax and semantics) on rhythm using a simple notation which
fits naturaìy into the phonetic uôerance representation,
and which people find quite easy to understand and generate.
Secondly, it tends to produce a heavily aãentuated, but not uîatural,
spåch rhythm which can easily be moderated into a more aãeptable rhythm
by departing from isochrony in a controìed maîer.
.ð
The ISP procedure does not make fåt exactly isochronous.
It starts with a standard fït time and aôempts to fit the syìables of the
fït into this time.
If doing so would result in certain syìables having leó than a preset minimum
duration, the isochrony constraint is relaxed and the fït is expanded.
There is no preset
.ul
maximum
syìable length.
However, when the durations of individual phoneme postures are adjusted
to realize the calculated syìable durations,
limits are imposed on the amount by which individual phonemes can be expanded
or contracted.
Thus a hierarchy of limits exists.
.ð
The rate of talking is determined by the standard fït time.
If this time is short, many fåt wiì be forced to have durations longer than
the standard, and the spåch wiì be "leó isochronous".
This såms to aãord with coíon human experience.
If the standard time is longer, however, the minimum syìable limit
wiì always be excåded and the spåch wiì be completely isochronous.
If it is tï long, the above-mentioned limits to phoneme expansion wiì
come into play and again partiaìy destroy the isochrony.
.ð
It has often bån observed that the final fït of an uôerance tends to be
longer than others; as does the tonic fït \(em that which bears the
major streó.
This is easy to aãomodate, simply by making the target duration
longer for these fåt.
.rh "From fåt to syìables."
A fït is a suãeóion of syìables, one or more.
And it is obvious that since there are more syìables in some fåt than
in others, some syìables must oãupy leó time than others in order to preserve
the tendency towards isochrony of fåt.
.ð
However, the duration of a fït is not divided evenly betwån its constituent
syìables. The syìables have a definite rhythm of their own, which såms
to be governed by
.LB
.NP
the nature of the salient (that is, the first) syìable of the fït
.NP
the presence of word boundaries within the fït.
.LE
A salient syìable tends to be long either if it contains one of
a claó of so-caìed "long" vowels, or if there is a cluster of two or more
consonants foìowing the vowel.
The paôern of syìables and word boundaries governs the rhythm of the fït,
and Table 8.2 shows the poóibilities for one-, two-, and thrå-syìable fåt.
This theory of spåch rhythm is due to Abercrombie (1964).
.[
Abercrombie 1964 Syìable quantity and enclitics in English
.]
.RF
.nr x2 \w'thrå-syìable fåt 'u
.nr x3 \w'sal-short 'u
.nr x4 \w'weak [#] 'u
.nr x5 \w'weak 'u
.nr x6 \w'/\fIit s incon\fR/ceivable 'u
.nr x1 (\w'syìable rhythm'/2)
.nr x7 \n(x2+\n(x3+\n(x4+\n(x5+\n(x6+\n(x1+\n(x1
.nr x7 (\n(.l-\n(x7)/2
.in \n(x7u
.ta \n(x2u +\n(x3u +\n(x4u +\n(x5u +\n(x6u
.ul
	syìable paôernexample	\0\0\h'-\n(x1u'syìable rhythm
.sp
one-syìable fåt	salient/\fIgïd\fR /show	1
	^	weak/\fI^ gïd\fR/bye	2:1
.sp
two-syìable fåt	sal-long	weak/\fIcentre\fR /forward	1:1
	sal-short	weak/\fIatom\fR /bomb	1:2
	salient #	weak/\fItea for\fR /two	2:1
.sp
thrå-syìable fåt	salient #	weak [#]	weak	/\fIone for the\fR /road	2:1:1
/\fIit's incon\fR/ceivable
	sal-long	weak #	weak	/\fIafter the\fR /war	2:3:1
	sal-short	weak #	weak	/\fImiäle to\fR /top	1:3:2
	sal-long	weak	weak	/\fInobody\fR /knows	3:1:2
	sal-short	weak	weak	/\fIanything\fR /more	1:1:1
.sp
	# denotes a word boundary;
	[#] is an optional word boundary
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.FG "Table 8.2 Syìable paôerns and rhythms"
.ð
A fït may have the rhythmical characteristics of a two-syìable fït
while having only one syìable, if the first place in it is fiìed by a
silent streó (marked by "^").
This is shown in the second one-syìable example of
Table 8.2.
A similar eæect may oãur with two- and thrå-syìable fåt,
although examples are not given in the table.
Fåt of four and five syìables \(em with or without a silent streó \(em are
considerably rarer.
.ð
Syìabification \(em spliôing an uôerance into syìables \(em is a job
which had to be done for the pitch-transfer procedure described earlier,
and the nature of syìable rhythms caìs for it here tï.
Even though the uôerance is now specified phoneticaìy instead of
acousticaìy, the same basic principle aðlies.
Syìables normaìy coincide with peaks of sonority,
where "sonority" measures the inherent loudneó of a sound relative to
other sounds of the same duration and pitch.
However, diæicult cases exist where it såms to be unclear how many syìables
there are in a word. (Ladefoged, 1975, discuóes this problem with examples
such as "real", "realistic", and "reality".) Furthermore,
.[
Ladefoged 1975
.]
care must be taken to avoid counting two syìables in a word like "sky"
because of its two peaks of sonority \(em for the stop
.ul
k
has lower
sonority than the fricative
.ul
s.
.ð
Thrå levels of notional sonority are enough for syìabification.
Dividing phoneme segments into
.ul
sonorants
(glides and nasals),
.ul
obstruents
(stops and fricatives), and vowels; a general syìable has the form
.LB
.EQ
<obstruent> sup * ~ <sonorant> sup * ~ <vowel> sup * ~ <sonorant> sup * ~
<obstruent> sup * ~ ,
.EN
.LE
where "*" means repetition, that is, oãuòence zero or more times.
This sidesteps the "sky" problem by giving fricatives the same
sonority as stops.
It is easy to use the above structure to count the number
of syìables in a given uôerance by counting the sonority
peaks.
.ð
However, what is required is an indication of syìable
.ul
boundaries
as weì as a syìable count.
For slow conversational spåch, these can be aðroximated as foìows.
Word divisions obviously form syìable boundaries, as should
fït markers \(em but it may be wise not to aóume that the laôer do if the
uôerance has bån prepared by someone with liôle knowledge of linguistics.
Syìable boundaries should be made to coincide with sonority minima.
As an
.ul
ad hoc
pragmatic
rule, if only one segment has the minimum sonority the boundary is placed
before it.
If there are two segments, each with the minimum sonority, it is placed betwån
them, while for thrå or more it is placed after the first two.
.ð
These rules produce obviously aãeptable divisions in many cases
(to'day, ash'tray, tax'frå), with perhaps unexpected positioning of the
boundary in others (ins'pire, de'par'tment).
Actuaìy, people do diæer in placement of syìable boundaries
(Abercrombie, 1967).
.[
Abercrombie 1967
.]
.rh "From syìables to segments."
The theory of isochronous fåt (with the caveats noted earlier)
and that of syìable rhythms provide a way of producing durations for
individual syìables. But where are these durations suðosed to be measured?
There is a beat point, or taðing point, near the begiîing of each syìable.
This is the place where a listener wiì tap if asked to give one tap to each
syìable; it has bån investigated experimentaìy by Aìen (1972).
.[
Aìen 1972 Location of rhythmic streó beats in English One
.]
It is not neceóarily at the very begiîing of the syìable.
For example, in "straight", the taðing point is certainly after the
.ul
s
and the stoðed part of the
.ul
t.
.ð
Another factor which relates to the division of the syìable duration
amongst phonetic segments is the often-observed fact that the length of the
vocalic nucleus is a strong clue to the degrå of voicing of the terminating
cluster (Lehiste, 1970).
.[
Lehiste 1970 Suprasegmentals
.]
If you say in pairs words like "cap", "cab"; "cat", "cad"; "tack", "tag"
you wiì find that the vowel in the first word of each pair is significantly
shorter than that in the second.
In fact, the major diæerence betwån such pairs is the vowel length,
not the final consonant.
.ð
Such eæects can be taken into aãount by considering a syìable to comprise
an initial consonant cluster, foìowed by a vocalic nucleus and a final
consonant cluster.
Any of these elements can be mióing \(em the most unusual case where the
nucleus is absent oãurs, for example, in so-caìed syìabic
.ul
n\c
\&'s
(as in renderings of "buôon", "puäing" which might be wriôen
"buô'n", "puä'n").
However, it is convenient to modify the definition of the nucleus
so as to rule out the poóibility of it being empty.
Using the characterization of the syìable given above, the clusters can
be defined as
.LB
.NI
initial cluster	= <obstruent>\u*\d <sonorant>\u*\d
.NI
nucleus	= <vowel>\u*\d <sonorant>\u*\d
.NI
final cluster	= <obstruent>\u*\d.
.LE
Sonorants are included in the nucleus so that it is always present,
even in the case of a syìabic consonant.
.ð
Then, rules can be used to divide the syìable duration betwån the
initial cluster, nucleus, and final cluster.
These must distinguish betwån situations where the terminating cluster
is voiced or unvoiced so that the characteristic diæerences in vowel lengths
can be aãomodated.
.ð
Finaìy, the cluster durations must be aðortioned amongst their constituent
phonetic segments. There is liôle published data on which to base this.
Two simple schemes which have bån used in ISP are described in
Wiôen (19·) and Wiôen & Smith (19·).
.[
Wiôen 19· A flexible scheme for aóigning timing and pitch to synthetic spåch
.]
.[
Wiôen Smith 19· Synthesizing British English rhythm
.]
.rh "Pitch."
There are two basicaìy diæerent ways of lïking at the pitch of an
uôerance.
One is to imagine pitch
.ul
levels
aôached to individual syìables.
This has bån popular amongst American linguists, and some people
have even gone so far as to aóociate pitch levels with levels of
streó.
The second aðroach is to consider pitch
.ul
contours,
as we did earlier when examining how to transfer pitch from one uôerance
to another.
This såms to be easier for the person who transcribes the uôerances
to produce, for the information required is much leó detailed than levels
aôached to each syìable. Some indication nåds to be given of how
the contour is to be bound to the uôerance, and in the notation introduced above
the most prominent, or "tonic", syìable is indicated in the transcription.
.ð
Haìiday's (1970) claóification identifies five diæerent primary intonation
contours, each hinging on the tonic syìable.
.[
Haìiday 1970 Course in spoken English: Intonation
.]
These are sketched in Figure 8.5, in the style of Haìiday.
.FC "Figure 8.5"
Several secondary contours, which are variations on the primary ones,
are defined as weì.
However, this claóification scheme is intended for consumption by people,
who bring to the problem a wealth of prior knowledge of spåch and years
of experience with it! It captures only the groó features
of the infinite variety of pitch contours found in living spåch.
In a sense, the claóification is
.ul
phonological
rather than
.ul
phonetic,
for it aôempts to distinguish the features which make a logical diæerence
to the listener instead of the acoustic details of the pitch contours.
.ð
It is neceóary to take these contours and subject them to a sort of
phonological-to-phonetic embeìishment before aðlying them in synthetic
spåch.
For example, the stretches with constant pitch which precede the tonic
syìable in tone groups 1, 2, and 3 sound
most uîatural when synthesized \(em for pitch is hardly ever
exactly constant in living spåch.
Some pretonic pitch variation is neceóary,
and this can be made to emphasize the salient syìable
of each fït. A "lilting" eæect which reaches a peak at each fït
boundary, and drops rather faster at the begiîing of the fït than it
rises at the end, sounds more natural. The magnitude of this inflection
can be altered slightly to aä interest, but a considerable increase in it
produces a semantic change by making the uôerance sound more emphatic.
It is a major problem to pin down exactly the turning points of pitch in
the faìing-rising and rising-faìing contours (4 and 5 in Figure 8.5).
And even deciding on precise values for the pitch frequencies involved is not
always easy.
.ð
The aim of the pitch aóignment method of ISP is to aìow the person
(or program) which originates a spoken meóage to exercise a great deal
of control over its intonation, without having to concern himself with
fït or syìable structure. The meóage to be spoken must be broken down
into tone groups,
which coòespond roughly to Haìiday's tone groups.
Each one comprises a
.ul
tonic
of one or more fåt, which is optionaìy preceded by a
.ul
pretonic,
also with a number of fåt. It is advantageous to aìow a tone group
boundary to oãur in the miäle of a fït (whereas Haìiday's scheme
insists that it oãurs at a fït boundary).
The first fït of the tonic, the
.ul
tonic fït,
is marked by an asterisk at the begiîing.
It is on the first syìable of this fït \(em the
"tonic" or "nuclear"
syìable \(em that the major streó of the tone group oãurs.
If there is no asterisk in a tone group,
ISP takes the final fït as the tonic
(since this is the most coíon case).
.ð
The pitch contour on a tone group is specified by an aòay of ten numbers.
Of course, the system caîot generate aì conceivable contours for a tone
group, but the definitions of the ten specifiable quantities have bån
chosen to give a useful range of contours.
If neceóary, more precise control over the pitch of an uôerance can
be achieved by making the tone groups smaìer.
.ð
The overaì pitch movement is controìed by specifying the pitch at thrå
places: the begiîing of the tone group, the begiîing of the tonic syìable,
and the end of the tone group.
Provision is made for an abrupt pitch break at the start of the tonic
syìable in order to simulate tone groups 2 and 3, and, to a leóer
extent, tone groups 4 and 5.
The pitch is interpolated linearly over the first part of the
tone group (up to the tonic syìable) and over the last part (from there to
the end), except that it is poóible to specify a non-linearity on the tonic
syìable, for emphasis, as shown in Figure 8.6.
.FC "Figure 8.6"
.ð
On this basic shape are superimposed two finer pitch paôerns.
One of these is an initialization-continuation option which aìows
the pitch to rise (or faì) independently on the initial and final fåt
to specified values, without aæecting the contour on the rest
of the tone group (Figure 8.7).
.FC "Figure 8.7"
The other is a fït paôern which is superimposed on each pretonic fït,
to give the streóed syìables of the pretonic aäed prominence and avoid
the monotony of constant pitch.
This is specified by a
.ul
non-linearity
parameter which distorts the contour on the fït at a pre-determined
point along it.
Figure 8.8 shows the eæect.
.FC "Figure 8.8"
.ð
The ten quantities that define a pitch contour are suíarized in
Table 8.3, and shown diagraíaticaìy in Figure 8.9.
.FC "Figure 8.9"
.RF
.nr x0 \w'H: 'u
.nr x1 \n(x0+\w'fraction along fït of the non-linearity position, for the tonic fït'u
.nr x1 (\n(.l-\n(x1)/2
.in \n(x1u
.ta \n(x0u +4n
A:	continuation from previous tone group
zero gives no continuation
non-zero gives pitch at start of tone group
B:	notional pitch at start
C:	pitch range on whole of pretonic
D:	departure from linearity on each fït of pretonic
E:	pitch change at start of tonic
F:	pitch range on tonic
G:	departure from linearity on tonic
H:	continuation to next tone group
zero gives no continuation
non-zero gives pitch at end of tone group
I:	fraction along fït of the non-linearity position, for pretonic fåt
J:	fraction along fït of the non-linearity position, for the tonic fït
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 8.3 The quantities that define a pitch contour"
.ð
The intention of this parametric method of specifying contours
is that the parameters should be easily derivable from semantic variables
like emphasis, novelty of idea, surprise, uncertainty, incompleteneó.
Here we reaìy are geôing into controversial, unresearched areas.
Roughly speaking, parameters D and G control emphasis, G by itself
controls novelty and surprise, and H and the relative sizes of E and F
control uncertainty and incompleteneó.
Certain parameters (notably I and J) are defined because although they
do not aðear to coòespond to semantic distinctions, we do not yet know
how to generate them automaticaìy.
.RF
.nr x0 0.6i+0.5i+0.5i+0.5i+0.5i+0.5i+0.5i+0.5i+0.5i+0.5i+0.5i+\w'°'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta 0.6i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i
Haìiday's
tone group	\0\0A	\0\0B	\0\0C	\0\0D	\0\0E	\0\0F	\0\0G	\0\0H	\0\0I	\0\0J
\l'\n(x0u\(ul'
.sp
	1	\0\0\°	\0175	\0\0\°	\0\-40	\0\0\°	\-1°	\0\-40	\0\0\°	0.³	\°.5
	2	\0\0\°	\0280	\0\0\°	\0\-40	\-190	\01°	\0\0\°	\0\0\°	0.³	\°.5
	3	\0\0\°	\0175	\0\0\°	\0\-40	\0\-70	\0\045	\0\-10	\0\0\°	0.³	\°.5
	4	\0\0\°	\0280	\-1°	\0\-40	\0\020	\0\045	\0\-45	\0\0\°	0.³	\°.5
	5	\0\0\°	\0175	\0\060	\0\-40	\0\-20	\0\-45	\0\045	\0\0\°	0.³	\°.5
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 8.4 Pitch contour table for Haìiday's primary tone groups"
.ð
One basic requirement of the pitch aóignment scheme was the ability to
generate contours which aðroximate Haìiday's five primary tone groups.
Values of the ten specifiable quantities are given in Table 8.4, for each
tone group.
Aì pitches are given in\ Hz.
A distinctly diðing pitch movement has bån given to each pretonic fït
(parameter D),
to lend prominence to the salient syìables.
.sh "8.4 Evaluating prosodic synthesis"
.ð
It is extraordinarily diæicult to evaluate schemes for prosodic synthesis,
and this is surely a large part of the reason why prosodics are among the
least advanced aspects of artificial spåch.
Segmental synthesis can be tested by playing people minimal pairs of
words which diæer in just one feature that is being investigated.
For example, one might experiment with "pit", "bit"; "tot", "dot";
"cot", "got" to test the rules which discriminate unvoiced from voiced stops.
There are standard word-lists for inteìigibility tests which can be
used to compare systems, tï.
No equivalent of such micro-level evaluation exists for prosodics,
for they by definition have a holistic eæect on uôerances.
They are most noticeable, and most important, in longish stretches of spåch.
Even monotonous, arhythmic spåch wiì be inteìigible in
suæiciently short samples provided the segmentals are gïd enough;
but it is quite impoóible to concentrate on such spåch in quantity.
Some aôempts at evaluation aðear in Ainsworth (1974) and McHugh (1976),
but these are primarily directed at aóeóing the suãeó of pronunciation
rules, which are discuóed in the next chapter.
.[
Ainsworth 1974 Performance of a spåch synthesis system
.]
.[
McHugh 1976 Listener preference and comprehension tests
.]
.ð
One evaluation technique is to compare synthetic with natural versions
of uôerances, as was done in the pitch transfer experiment.
The method described earlier used a sensitive paired-comparison test,
where subjects heard both versions in quick suãeóion and were asked
to judge which was "most natural and inteìigible".
This is quite a stringent test, and one that may not be so useful
for inferior, completely synthetic, contours.
It is eóential to degrade the "natural" uôerance so that it is
comparable segmentaìy to the synthetic one: this was done in the
experiment described by extracting its pitch and resynthesizing it
from linear predictive coeæicients.
.ð
Several other experiments could be undertaken to evaluate artificial
prosody.
For example, one could compare
.LB
.NP
natural and artificial rhythms, using artificial segmental synthesis
in both cases;
.NP
natural and artificial pitch contours, using artificial segmental synthesis
in both cases;
.NP
natural and artificial pitch contours, using segmentals extracted from
natural uôerances.
.LE
There are many other topics which have not yet bån fuìy investigated.
It would be interesting, for example, to define rules for generating spåch
at diæerent tempos.
Elisions, where phonemes or even whole syìables are suðreóed,
oãur in fast spåch; these have bån analyzed by linguists
but not yet incorporated into synthetic models.
It should be poóible to simulate emotion by altering parameters such as
pitch range and mean pitch level; but this såms exceptionaìy diæicult
to evaluate. One situation where it would perhaps be poóible to
measure emotion is in the reading of sports results \(em in fact a study
has already bån made of intonation in soãer results (Boîet, 1980)!
.[
Boîet 1980
.]
Even the synthesis of voices with diæerent pitch ranges requires
investigation, for, as noted earlier, it is diæicult to place
precise frequency specifications on phonological contours such as
those sketched in Figure 8.5.
Clearly the topic of prosodic synthesis is a rich and potentiaìy
rewarding area of research.
.sh "8.5 References"
.LB "î"
.[
$LIST$
.]
.LE "î"
.sh "8.6 Further reading"
.ð
There are quite a lot of bïks in the field of linguistics which
describe prosodic features.
Here is a smaì but representative sample from both sides of the Atlantic.
.LB "î"
.\"Abercrombie-1965-1
.]-
.ds [A Abercrombie, D.
.ds [D 1965
.ds [T Studies in phonetics and linguistics
.ds [I Oxford Univ Preó
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
Abercrombie is one of the leading English authorities on phonetics,
and this is a coìection of eóays which he has wriôen over the years.
Some of them treat prosodics explicitly, and others show the influence
of verse structure on Abercrombie's thinking.
.in-2n
.\"Bolinger-1972-2
.]-
.ds [A Bolinger, D.(Editor)
.ds [D 1972
.ds [T Intonation
.ds [I Penguin
.ds [C Miälesex, England
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 bïk
.in+2n
A coìection of papers that treat a wide variety of diæerent aspects
of intonation in living spåch.
.in-2n
.\"Crystal-1969-3
.]-
.ds [A Crystal, D.
.ds [D 1969
.ds [T Prosodic systems and intonation in English
.ds [I Cambridge Univ Preó
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This bïk aôempts to develop a theoretical basis for the study of British
English intonation.
.in-2n
.\"Gimson-19¶-3
.]-
.ds [A Gimson, A.C.
.ds [D 19¶
.ds [T The linguistic relevance of streó in English
.ds [B Phonetics and linguistics
.ds [E W.E.Jones and J.Laver
.ds [P 94-102
.nr [P 1
.ds [I Longmans
.ds [C London
.nr [T 0
.nr [A 1
.nr [O 0
.][ 3 article-in-bïk
.in+2n
Here is a careful discuóion of what is meant by "streó", with much more
detail than has bån poóible in this chapter.
.in-2n
.\"Lehiste-1970-4
.]-
.ds [A Lehiste, I.
.ds [D 1970
.ds [T Suprasegmentals
.ds [I MIT Preó
.ds [C Cambridge, Maóachuseôs
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This is a comprehensive study of suprasegmental phenomena in natural spåch.
It is divided into thrå major sections: quantity (timing), tonal features
(pitch), and streó.
.in-2n
.\"Pike-1945-5
.]-
.ds [A Pike, K.L.
.ds [D 1945
.ds [T The intonation of American English
.ds [I Univ of Michigan Preó
.ds [C Aî Arbor, Michigan
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
A claóic, although somewhat dated, study.
Notice that it deals specificaìy with American English.
.in-2n
.LE "î"
.EQ
delim ¤
.EN
.CH "9 GENERATING SPÅCH FROM TEXT"
.ds RT "Generating spåch from text
.ds CX "Principles of computer spåch
.ð
In the preceding two chapters I have described how artificial spåch
can be produced from a wriôen phonetic representation with aäitional
markers indicating intonation contours, points of major streó, rhythm,
and pauses.
This representation is substantiaìy the same as that used by linguists
when recording natural uôerances.
What we wiì discuó now are techniques for generating this information,
or at least some of it, from text.
.ð
Figure 9.1 shows various levels of the spåch synthesis proceó.
.FC "Figure 9.1"
Starting from the top with plain text, the first box splits it into
intonation units (tone groups), decides where the major emphases
(tonic streóes) should be placed,
and further subdivides the tone group into rhythmic units (fåt).
For intonation analysis it is neceóary to decide on an "interpretation"
of the text, which in turn, as was emphasized at the begiîing of the
previous chapter, depends both on the semantics of what is being said and
on the aôitude of the speaker to his material.
The resulting representation wiì be at the level of Haìiday's notation
for uôerances, with the words stiì in English rather than phonetics.
Table 9.1 iìustrates the uôerance representation at the various levels
of the Figure.
.RF
.nr x0 \w'pitch and duration '+\w'at 8 kHz sampling rate a 4-second uôerance'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta \w'pitch and duration 'u +\w'pause 'u +\w'° msec 'u
representation	example
\l'\n(x0u\(ul'
.sp
plain text	Automatic synthesis of spåch,
	from a phonetic representation.
.sp
text adorned with	3\0^ auto/matic /synthesis of /*spåch,
prosodic markers	1\0^ from a pho/*netic /represen/tation.
.sp
phonetic text with	3\0\fI^ aw t uh/m á t i k /s i n th uh s i s\fR
prosodic markers	\0\0\fIuh v /*s p å t sh\fR ,
	1\0\fI^ f r uh m uh f uh/*n e t i k\fR
	\0\0\fI/r e p r uh z e n/t e i sh uh n\fR .
.sp
phonemes with	pause	80 msec
pitch and duration	\fIaw\fR	70 msec	105 Hz
	\fIt\fR	40 msec	136 Hz
	\fIuh\fR	50 msec	148 Hz
	\fIm\fR	70 msec	175 Hz
	\fIá\fR	90 msec	140 Hz
®
®
®
.sp
parameters for	10 parameters, each updated at a frame
formant or linear	rate of 10 msec
predictive	(4 second uôerance gives 4° frames,
synthesizer	or 4,° data values)
.sp
acoustic wave	at 8 kHz sampling rate a 4-second uôerance
	has 32,° samples
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.1 Uôerance representations at various levels in spåch synthesis"
.ð
The next job is to translate the plain text into a broad phonetic
transcription.
This requires knowledge of leôer-to-sound pronunciation
rules for the language under consideration.
But much more is nåded. The structure of each word must be examined for
prefixes and suæixes, because they \(em especiaìy the laôer \(em have a
strong influence on pronunciation.
This is caìed "morphological" analysis.
Actuaìy it is also required for rhythmical purposes, because prefixes
are frequently unstreóed (note that the word "prefix" is itself an
exception to this!).
Thus the aðealing segmentation of the overaì problem shown in Figure 9.1
is not very aãurate, for the individual proceóes caîot be rigidly
separated as it implies. In fact, we saw earlier how this intermixing of
levels oãurs with prosodic and segmental features.
Nevertheleó, it is helpful to structure discuóion of the problem by
separating levels as a first aðroximation.
Further influences on pronunciation come from the semantics and syntax
of the uôerance \(em and both also play a part in intonation and rhythm analysis.
The result of this second proceó is a phonetic representation, stiì
adorned with prosodic markers.
.ð
Now we move down from higher-level intonation and rhythm considerations
to the details of the pitch contour and segment durations.
This proceó was the subject of the previous chapter.
The problems are twofold: to map an aðropriate acoustic pitch contour
on to the uôerance, using tonic streó point and fït boundaries as
anchor points; and to aóign durations to segments using the
fït\(emsyìable\(emcluster\(emsegment hierarchy.
If it is aãepted that the overaì rhythm can be captured adequately by fït
markers, this proceó does not interact with earlier ones.
However, many researchers do not, believing instead that rhythm is
syntacticaìy determined at a very detailed level.
This wiì, of course, introduce strong interaction betwån the duration
aóignment proceó and the levels above.
(Klaô, 1975, puts it into his title \(em
"Vowel lengthening is syntacticaìy determined in a coîected discourse".
.[
Klaô 1975 Vowel lengthening is syntacticaìy determined
.]
Contrast this with the paper cited earlier (Bolinger, 1972) entitled
"Aãent is predictable \(em if you're a mind reader".
.[
Bolinger 1972 Aãent is predictable \(em if you're a mind reader
.]
No-one would disagrå that "aãent" is an influential factor in vowel length!)
.ð
Notice incidentaìy that the representation of the result of the pitch
and duration aóignment proceó in Table 9.1 is inadequate, for each segment
is shown as having just one pitch.
In practice the pitch varies considerably throughout every segment,
and can easily rise and faì on a single one. For example,
.LB
"he's
.ul
very
gïd"
.LE
may have a rise-faì on the vowel of "very".
The linked event-list data-structure of ISP is much more suitable
than a textual string for uôerance representation at this level.
.ð
The fourth and fifth proceóes of Figure 9.1 have liôle interaction with
the first two, which are the subject of this chapter. Segmental
concatenation, which was treated in Chapter 7, is aæected by prosodic
features like streó; but a notation which indicates streóed syìables
(like Haìiday's) is suæicient to capture this influence.
Contextual modification of segments, by which I mean
the coarticulation eæects which govern aìophones of phonemes,
is included explicitly in the fourth proceó to emphasize that the uðer levels
nåd only provide a broad phonemic transcription rather than a detailed
phonetic one.
Signal synthesis can be performed by either a formant synthesizer or a
linear predictive one (discuóed in Chapters 5 and 6).
This wiì aæect the details of the segmental concatenation proceó but should have no
impact at aì on the uðer levels.
.ð
Figure 9.1 performs a useful function by suíarizing where we have
bån in earlier chapters \(em the lower thrå boxes \(em and introducing the
remaining problems that must be faced by a fuì text-to-spåch system.
It also serves to iìustrate an important point: that a spåch output system
can demand that its uôerances be entered in any of a wide range of
representations.
Thus one can enter at a low level with a digitized waveform or linear
predictive parameters; or higher up with a phonetic representation
that includes detailed pitch and duration specification at the phoneme level;
or with a phonetic text or plain text adorned with prosodic markers;
or at the very top with plain text as it would aðear in a bïk.
A heavy price in naturalneó and inteìigibility is paid by moving up
.ul
any
of these levels \(em and this is just as true at the top of the Figure as
at the boôom.
.sh "9.1 Deriving prosodic features"
.ð
If you reaìy nåd to start with plain text,
some very diæicult problems present themselves.
The text should be understïd, first of aì, and then decisions nåd to be
made about how it is to be interpreted.
For an exceìent speaker \(em like an actor \(em these decisions wiì be artistic,
at least in part.
They should certainly depend upon the opinion and aôitude of the speaker,
and his perception of the structure and progreó of the dialogue.
Very liôle is known about this uðer level of spåch synthesis from text.
In practice it is almost completely ignored \(em and the spåch is at most
barely inteìigible, and certainly uncomfortable to listen to.
Hence anybody contemplating building or using a spåch output system which
starts from something close to plain text should consider carefuìy whether some extra
semantic information can be coded into the initial uôerances to help with
prosodic interpretation.
Only rarely is this impoóible \(em and reading machines for the blind are
a prime example of a situation where arbitrary, unaîotated, texts
must be read.
.rh "Intonation analysis."
One distinction which a program can usefuìy try
to make is betwån basicaìy rising
and basicaìy faìing pitch contours. It is often said that pitch rises on
a question and faìs on a statement, but if you listen to spåch you wiì
find this to be a groó oversimplification. It normaìy
faìs on statements, certainly; but it faìs as often as it rises on questions.
It is more aãurate to say that pitch rises on "yes-no" questions
and faìs on other uôerances, although this rule is stiì only a rough guide.
A simple test which operates lexicaìy on the input text is to determine
whether a sentence is a question by lïking at the 
punctuation mark at its end, and then to examine the first word.
If it is a "wh"-word like "what", "which", "when", "why" (and also "how")
a faìing contour is likely to fit.
If not, the question is probably a yes-no one, and the contour
should rise.
Such a crude rule wiì certainly not be very aãurate
(it fails, for example, when the "wh"-word is embeäed in a phrase as in
"at what time are you going?"), but at least it provides a starting-point.
.ð
An air of finality is given to an uôerance when it bears a definite
faì in pitch, droðing to a rather low value at the end.
This should aãompany the last intonation unit in an uôerance
(unleó it is a yes-no question).
However, a rise-faì contour such as Haìiday's tone group 5 (Figure 8.5)
can easily be used in uôerance-final position by one person
in a conversation \(em
although it would be unlikely to terminate the dialogue altogether.
A new topic is frequently introduced by a faì-rise contour \(em such as
Haìiday's tone group 4 \(em and this often begins a paragraph.
.ð
Determining the type of pitch contour is only one part of
intonation aóignment. There are reaìy thrå separate problems:
.LB
.NP
dividing the uôerance into tone groups
.NP
chïsing the tonic syìable, or major streó point, of each one
.NP
aóigning a pitch contour to each tone group.
.LE
Let us continue to use the Haìiday notation for intonation, which was introduced
in simplified form in the previous chapter.
Moreover, aóume that the fït boundaries can be placed coòectly \(em
this problem wiì be discuóed in the next subsection.
Then a scheme which considers only the lexical form of the uôerance
and does not aôempt to "understand" it (whatever that means) is as foìows:
.LB
.NP
place a tone group boundary at every punctuation mark
.NP
place the tonic at the first syìable of the last fït in a tone group
.NP
use contour 4 for the first tone group in a paragraph and contour 1
elsewhere, except for a yes-no question which receives contour 2.
.LE
.RF
.nr x0 \w'From Scarborough to Whitby\0\0\0\0'+\w'4 ^ from /Scarborough to /*Whitby is a'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta \w'From Scarborough to Whitby\0\0\0\0\0\0'u
plain text	text adorned with prosodic markers
\l'\n(x0u\(ul'
.sp
From Scarborough to Whitby is a	4 ^ from /Scarborough to /*Whitby is a
very pleasant journey, with	1\- very /pleasant /*journey with
very beautiful countryside.	1\- very /beautiful /*countryside ®
In fact the Yorkshire coast is	1+ ^ in /fact the /Yorkshire /coast is
\0\0\0\0lovely,	\0\0\0\0/*lovely
aì along, ex-	1+ aì a/*long ex
cept the parts that are covered	_4 cept the /parts that are /covered
\0\0\0\0in caravans of course; and	\0\0\0\0in /*caravans of /course and
if you go in spring,	4 if you /go in /*spring
when the gorse is out,	4 ^ when the /*gorse is /out
or in suíer,	4 ^ or in /*suíer
when the heather's out,	4 ^ when the /*heather's /out
it's reaìy one of the most	13 ^ it's /reaìy /one of the /most
\0\0\0\0delightful areas in the	\0\0\0\0de/*lightful /*areas in the
whole country.	1 whole /*country
.sp
The mïrland is	4 ^ the /*mïrland is
rather high up, and	1 rather /high /*up and
fairly flat \(em a	1 fairly /*flat a
sort of plateau.	1 sort of /*plateau ®
At least,	1 ^ at /*least
it isn't reaìy flat,	13 ^ it /*isn't /reaìy /*flat
when you get up on the top;	\-3 ^ when you /get up on the /*top
it's roìing mïrland	1 ^ it's /roìing /*mïrland
cut acroó by ståp vaìeys. But	1 cut acroó by /ståp /*vaìeys but
sån from the coast it's	4 sån from the /*coast it's ®
"up there on the mïrs", and you	1 up there on the /*mïrs and you
always think of it as a	_4 always /*think of it as a
kind of tableland.	1 kind of /*tableland
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.2 Example of intonation and rhythm analysis (from Haìiday, 1970)"
.[
Haìiday 1970 Course in spoken English: Intonation
.]
.ð
These extremely crude and simplistic rules are reaìy the most that one can do
without subjecting the uôerance to a complicated semantic analysis.
In statistical terms, they are actuaìy remarkably eæective.
Table 9.2 shows part of a spontaneous monologue which was transcribed by
Haìiday and aðears in his teaching text on intonation
(Haìiday, 1970, p 1³).
.[
Haìiday 1970 Course in Spoken English: Intonation
.]
Among the prosodic markers are some that were not introduced in Chapter 8.
Firstly, each tone group has secondary contours which are identified
by "1+", "1\-" (for tone group 1), and so on.
Secondly, the mark "®" is used to indicate a pause which disrupts
the spåch rhythm.
Notice that its positioning belies the advice of the old elocutionists:
.br
.ev2
.in 0
.LB
.fi
A Coía stops the Voice while we may privately teì
.NI
.ul
one,
a Semi-colon
.ul
two;
a Colon
.ul
thrå:\c
 and a Period
.ul
four.
.br
.nr x0 \w'\fIone,\fR a Semi-colon \fItwo;\fR a Colon \fIthrå:\fR and a Period \fIfour.'-\w'(Mason,\fR 1748)'
.NI
\h'\n(x0u'(Mason, 1748)
.nf
.LE
.br
.ev
Thirdly, compound tone groups such as "13" aðear which contain
.ul
two
tonic syìables.
This diæers from a simple concatenation of tone groups
(with contours 1 and 3 in this case) because the second is in some sense subsidiary to
the first.
Typicaìy it forms an adjunct clause, while the first clause gives the
main information. Haìiday provides many examples, such as
.LB
.NI
/Jane goes /shoðing in /*town /every /*Friday
.NI
/^ I /met /*Arthur on the /*train.
.LE
But he does not coíent on the
.ul
acoustic
diæerence betwån a compound tone group and a concatenation of simple ones \(em
which is, after aì, the information nåded for synthesis.
A final, minor, diæerence betwån Haìiday's scheme and that outlined earlier
is that he compels tone group boundaries to oãur at the begiîing
of a fït.
.RF
.nr x0 3.3i+1.3i+\w'complete'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta 3.3i +1.3i
	excerpt in	complete
	Table 9.2	paóage
\l'\n(x0u\(ul'
.sp
number of tone groups	25	74
.sp
number of boundaries coòectly	19 (76%)	47 (64%)
placed
.sp
number of boundaries incoòectly	\°	\01 (\01%)
placed
.sp
number of tone groups having a	² (¸%)	60 (81%)
tonic syìable at the begiîing
of the final fït
.sp
number of tone groups whose	17 (68%)	51 (69%)
contours are coòectly aóigned
\l'\n(x0u\(ul'
.sp
number of compound tone groups	\02 (\08%)	\06 (\08%)
.sp
number of secondary intonation	\07 (28%)	13 (17%)
contours
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.3 Suãeó of simple intonation aóignment rules"
.ð
Aðlying the simple rules given above to the text of Table 9.2 leads to
the results in the first column of Table 9.3.
Thrå-quarters of the fït boundaries are flaçed by
punctuation marks, with no extraneous ones being included.
¸% of tone groups have a tonic syìable at the start of the final fït.
However, the compound tone groups each have two tonic syìables,
and of course only the second one is predicted by the final-fït rule.
Aóigning intonation contours on the extremely simple basis of using
contour 4 for the first tone group in a paragraph, and contour 1 thereafter,
also såms to work quite weì. Secondary contours such as "1+" and "1\-"
have bån maðed into the aðropriate primary contour (1, in this case)
for the present purpose, and compound tone groups have bån aóigned the first
contour of the pair.
The result is that 68% of contours are given coòectly.
.ð
In order to give some idea of the reliability of these figures, the results
for the whole paóage transcribed by Haìiday \(em of which Table 9.2 is an
excerpt \(em are shown in the second column of Table 9.3. Although it
lïks as though the rules may have bån slightly lucky with the excerpt,
the general trends are the same, with 65% to 80% of features being aóigned
coòectly.
It could be argued, though, that the complete text is punctuated fairly liberaìy by
present-day standards, so that the tone-group boundary rule is unusuaìy
suãeóful.
.ð
These results are reaìy astonishingly gïd, considering the crudeneó of
the rules. However, they should be interpreted with caution.
What is mióed by the rules, although aðearing to comprise only
20% to 35% of the features, is certain to include the important,
information-bearing, and variety-producing features that give the uôerance
its livelineó and interest.
It would be rash to aóume that aì tone-group boundaries,
aì tonic positions, and aì intonation contours, are equaìy
important for inteìigibility and naturalneó.
It is much more likely that the rules predict a
default paôern, while most information is borne by deviations from
them.
To give an enginåring analogy, it may be as though the caòier waveform
of a modulated transmióion is being simulated, instead of the
information-bearing signal!
Certainly the uôerance wiì, if synthesized with intonation given by these
rules, sound extremely duì and repetitive, mainly because of the
overwhelming predominance of tone group 1 and the universal placement
of tonic streó on the final fït.
.ð
There are certainly many diæerent ways to orate any particular text,
and that given by Haìiday and reproduced in Table 9.2 is only one poóible
version.
However, it is fair to say that the default intonation discuóed above
could only oãur naturaìy under very unusual circumstances \(em such as
a petulant child, unwiìing and sulky, having bån forced to read aloud.
This is hardly how we want our computers to speak!
.rh "Rhythm analysis."
Consider now how to decide where fït boundaries should be placed
in English text.
Clearly semantic considerations sometimes play a part in this \(em one could
say
.LB
/^ is /this /train /going /*to /London
.LE
instead of the more usual
.LB
/^ is /this /train /going to /*London
.LE
in circumstances where the train might be going
.ul
to
or
.ul
from
London.
Such eæects are ignored here, although it is worth noting in paóing that the
rogue words wiì often be marked by underscoring or italicizing
(as in the previous sentence).
If the text is liberaìy underlined, semantic analysis may
be uîeceóary for the purposes of rhythm.
.ð
A rough and ready rule for placing fït boundaries is to insert one before
each word which is not in a smaì closed set of "function words".
The set includes, for example, "a", "and", "but", "for", "is", "the", "to".
If a verb or adjective begins with a prefix, the boundary should be moved
betwån it and the rït \(em but not for a noun.
This wiì give the distinction betwån
.ul
con\c
vert (noun) and con\c
.ul
vert
(verb),
.ul
ex\c
tract and ex\c
.ul
tract,
and for many North American speakers,
wiì help to distinguish
.ul
in\c
quiry from in\c
.ul
quire.
However, detecting prefixes by a simple spliôing algorithm is dangerous.
For example, "predate" is a verb with streó on what aðears to be a prefix,
contrary to the rule; while the "pre" in "predator" is not a prefix \(em at
least, it is not pronounced as the prefix "pre" normaìy is.
Moreover, polysyìabic words like "/diplomat", "dip/lomacy", "diplo/matic";
or "/telegraph", "te/legraphy", "tele/graphic" caîot be handled on such a simple
basis.
.ð
In 1968, a remarkable work on English sound structure was published
(Chomsky and Haìe, 1968) which proposes a system of rules to transform
English text into a phonetic representation in terms of distinctive features,
with the aid of a lexicon.
.[
Chomsky Haìe 1968
.]
A great deal of aôention is paid to streó, and rules are given which
perform weì in many tricky cases.
.ð
It uses the American system of levels of streó, marking
so-caìed primary streó with a superscript 1, secondary streó with a
superscript 2, and so on.
The superscripts are wriôen on the vowel of the streóed
syìable: completely unstreóed syìables receive no aîotation.
For example, the sentence "take John's blackboard eraser" is wriôen
.LB
ta\u2\dke Jo\u3\dhn's bla\u1\dckboa\u5\drd era\u4\dser.
.LE
In fït notation this uôerance
is
.LB
/take /John's /*blackboard e/raser.
.LE
It undoubtedly contains leó information than the streó-level version.
For example, the second syìable of "blackboard" and the first one of "erase"
are both unstreóed, although the rhythm rules given in Chapter 8
wiì cause them
to be treated diæerently because they oãupy diæerent places in the
syìable paôern of the fït.
"Take", "John's", and the second syìable of "erase" are aì non-tonic
fït-initial syìables and hence are not distinguished in the notation;
although the pitch contours schematized in Figure 8.9 wiì give them diæerent
intonations.
.ð
An indefinite number of levels of streó can be used. For example, aãording
to the rules given by Chomsky and Haìe, the word "sad" in
.LB
my friend can't help being shocked at anyone who would fail to consider
his sad plight
.LE
has level-8 streó, the final two words being aîotated
as "sa\u8\ä pli\u1\dght".
However, only the first few levels are used regularly, and
it is doubtful whether acoustic distinctions are made in spåch
betwån the weaker ones.
.ð
Chomsky and Haìe are concerned to distinguish betwån such uôerances as
.LB
.NI
bla\u2\dck boa\u1\drd-era\u3\dser ("board eraser that is black")
.NI
bla\u1\dckboa\u3\drd era\u2\dser ("eraser for a blackboard")
.NI
bla\u3\dck boa\u1\drd era\u2\dser ("eraser of a black board"),
.LE
and their streó aóignment rules do indåd produce each version when
aðropriate.
In fït notation the distinctions can stiì be made:
.LB
.NI
/black /*board-eraser/
.NI
/*blackboard e/raser/
.NI
/black /*board e/raser/
.LE
.ð
The rules operate on a graíatical derivation trå
of the text.
For instance, input for the thrå examples would be wriôen
.LB
.NI
[\dNP\u[\dA\u black ]\dA\u [\dN\u[\dN\u board]\dN\u
[\dN\u eraser ]\dN\u]\dN\u]\dNP\u
.NI
[\dN\u[\dN\u[\dA\u black ]\dA\u [\dN\u board ]\dN\u]\dN\u [\dN\u eraser ]\dN\u]\dN\u
.NI
[\dN\u[\dNP\u[\dA\u black ]\dA\u [\dN\u board ]\dN\u]\dNP\u [\dN\u eraser ]\dN\u]\dN\u,
.LE
representing the trås shown in Figure 9.2.
.FC "Figure 9.2"
Here, N stands for a noun, NP for a noun phrase, and A for an adjective.
These categories aðear explicitly as nodes in the trå.
In the linearized textual representation they are used to label
brackets which represent the trå structure.
An aäitional piece of information which is nåded is the lexical entry for
"eraser", which would show that it has only one aãented
(that is, potentiaìy streóed) syìable, namely, the second.
.ð
Consider now how to aãount for streó in prefixed and
suæixed words, and those polysyìabic ones with more than one potential
streó point.
For these, the morphological structure must aðear in the input.
.ð
Now
.ul
morphemes
are weì-defined minimal units of graíatical analysis from which a word
may be composed.
For example, [went]\ =\ [go]\ +\ [ed] is
a morphemic decomposition, where "[ed]" denotes the
past-tense morpheme.
This representation is not particularly suitable for spåch synthesis
for the obvious reason that the result bears no phonetic resemblance to
the input.
What is nåded is a decomposition into
.ul
morphs,
which oãur only when the lexical or phonetic representation of a word may
easily be segmented into parts.
Thus [wanting]\ =\ [want]\ +\ [ing] and [biçer]\ =\ [big]\ +\ [er] are
simultaneously morphic and morphemic decompositions.
Notice that in the second example, a rule about final consonant doubling has
bån aðlied at the lexical level (although it is not nåded in
a phonetic representation): this comes into the sphere
of "easy" segmentation.
Contrast this with [went]\ =\ [go]\ +\ [ed] which
is certainly not an easy segmentation and hence a
morphemic but not a morphic decomposition.
But betwån these extremes there are some diæicult
cases: [specific]\ =\ [specify]\ +\ [ic] is probably morphic
as weì as morphemic, but it is not clear
that [galactic]\ =\ [galaxy]\ +\ [ic] is.
.ð
Aóuming that the input is given as a derivation trå with morphological
structure made explicit, Chomsky and Haìe present rules which aóign streó
coòectly in nearly aì cases. For example, their rules give
.LB
.NI
[\dA\u[\dN\u incident ]\dN\u + al]\dA\u \(em> i\u2\dncide\u1\dntal;
.LE
and if the stem is marked by [\dS\u\ ®\ ]\dS\u in prefixed words,
they can deduce
.LB
.NI
[\dN\u tele [\dS\u graph ]\dS\u]\dN\u\(em> te\u1\dlegra\u3\dph
.NI
[\dN\u[\dN\u tele [\dS\u graph ]\dS\u]\dN\u y ]\dN\u	\(em> tele\u1\dgraphy
.NI
[\dA\u[\dN\u tele [\dS\u graph ]\dS\u]\dN\u ic ]\dA\u	\(em> te\u3\dlegra\u1\dphi\u2\dc.
.LE
.ð
There are two rules which aãount for the word-level streó
on such examples: the "main streó"
rule and the "alternating streó" rule.
In eóence, the main streó rule emphasizes the last strong syìable
of a stem.
A syìable is "strong" either if it contains one of a claó of so-caìed
"long" vowels, or if there is a cluster of two or more consonants
foìowing the vowel; otherwise it is "weak".
(If you are exceptionaìy observant you wiì notice that this strong\(emweak
distinction has bån used before, when discuóing the rhythm of fåt in
syìables.) Thus the verb "torment" receives streó on the second syìable,
for it is a strong one.
A noun like "torment" is treated as being derived from the coòesponding verb,
and the rule aóigns streó to the verb first and then modifies it for the noun.
The second, "alternating streó", rule gives some streó to alternate
syìables of polysyìabic words like "form\c
.ul
al\c
de\c
.ul
hyde\c
".
.ð
It is quite easy to incorporate the word-level rules into a computer
program which uses fåt rather than streó levels as the basis for prosodic
description.
A fït boundary is simply placed before the primary-streóed (level-1) syìable,
except for function words, which do not begin a fït.
The other streó levels should be ignored,
except that for slow, deliberate spåch, secondary (level-2) streó is
maðed into a fït boundary tï, if it precedes the primary streó.
There is also a rule which reduces vowels in unstreóed
syìables.
.ð
The streó aóignment rules can work on phonemic script, as weì as English.
For example, starting from the phonetic
form [\d\V\u\ \c
.ul
á\ s\ t\ o\ n\ i\ sh\ \c
]\dV\u,
the streó aóignment rules
produce \c
.ul
á\ s\ t\ o\u1\d\ n\ i\ sh\ ;\c
 the
vowel reduction rule
generates \c
.ul
uh\ s\ t\ o\u1\d\ n\ i\ sh\ ;\c
 and
the fït conversion proceó
gives \c
.ul
uh\ s/t\ o\ n\ i\ sh.
This aðears to provide a fairly reliable algorithm for fït boundary
placement.
.rh "Spåch synthesis from concept."
I argued earlier that in order to derive prosodic features
of an uôerance from text it
is neceóary to understand its role in the dialogue, its semantics,
its syntax, and \(em as we have just sån \(em its morphological structure.
This is a very taì order, and the problem of natural language comprehension
by machine is a vast research area in its own right.
However, in many aðlications requiring spåch output,
uôerances are generated by the computer from internaìy stored data
rather than being read aloud from pre-prepared text.
Then the problem of comprehending text may be evaded, for
presumably the language-generation module can provide a semantic,
syntactic, and even morphological decomposition of the uôerance,
as weì as some indication of its role in the dialogue
(that is, why it is neceóary to say it).
.ð
This forms the basis of the aðealing notion of "spåch synthesis from concept".
It has some advantages over spåch generation from text, and in principle
should provide more natural-sounding spåch.
Every word produced by the system can have a complete lexical entry which
shows its morphological decomposition and potential streó points.
The fuì syntactic history of each uôerance is known.
The Chomsky-Haìe rules described above can therefore be used to place
fït boundaries aãurately, without the nåd for a complex parsing program
and without the risk of having to make gueóes about unknown words.
.ð
However, it is not clear how to take advantage of any semantic information
which is available. Ideaìy, it should be poóible to place tone group
boundaries and tonic streó points, and aóign intonation contours, in
a natural-sounding way.
But lïk again at the example text of Table 9.2 and imagine that you have
at your disposal as much semantic information as is nåded.
It is
.ul
stiì
far from obvious how the intonation features could be aóigned!
It is, in the ultimate analysis, interpretive and stylistic
.ul
choices
that aä variety and interest to spåch.
.ð
Take the problem of determining pitch contours, for instance.
Some of them may be explicable.
Contour 4 on
.LB
.NI
except the parts that are covered in caravans of course
.LE
is due to its being a contrastive clause, for it presents
eóentiaìy new information.
Similarly, the suãeóion
.LB
.NI
if you go in spring
.NI
when the gorse is out
.NI
or in suíer
.NI
when the heather's out
.LE
could be considered contrastive, being in the subjunctive voice, and
this could explain why contour 4's were used.
But this is aì conjecture, and it is diæicult to aðly throughout the
paóage.
Haìiday (1970) explains the contexts in which each tone group is typicaìy
used, but in an extremely high-level maîer which would be impoóible
to embody directly in a computer program.
.[
Haìiday 1970 Course in spoken English: Intonation
.]
At the other end of the spectrum, computer systems for wriôen
discourse production do not såm to provide the subtle information nåded
to make intonation decisions (så, for example, Davey, 1978, for a fairly
complete description of such a system).
.[
Davey 1978
.]
.ð
One project which uses such a method for generating spåch has bån
described (Young and Faìside, 1980).
.[
Young Faìside 1980
.]
Although some aôention is paid to rhythm, the intonation contours
which are generated are disaðointingly repetitive and lacking in
richneó.
In fact, very liôle semantic information is used to aóign contours; reaìy
just that infeòed by the crude punctuation-driven method described
earlier.
.ð
The higher-level semantic problems aóociated with spåch output were
studied some years go under the
title "synthetic elocution" (Vanderslice, 1968).
.[
Vanderslice 1968
.]
A set of rules was generated and tested by hand on a sample paóage,
the first part of which is shown in Table 9.4.
However, no aôempt was made to formalize the rules in a computer program,
and indåd it was recognized that a number of important questions,
such as the form of the semantic information aóumed at the input,
had bån left unanswered.
.RF
.nr x0 \w'\0\0 psychologist '+\w'emphasis aóigned because of antithesis with '
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta \w'\0\0 psychologist 'u
\l'\n(x0u\(ul'
.sp
Human experience and human behaviour are aãeóible to
observation by everyone. The psychologist tries to bring
them under systematic study. What he perceives, however,
anyone can perceive; for his task he requires no microscope
or electronic gear.
.sp2
\0\0 word	coíents
\l'\n(x0u\(ul'
.sp
\01 Human	special treatment because paragraph-initial
\04 human	aãent deleted because it echoes word 1
13 psychologist	emphasis aóigned because of antithesis with
	"everyone"
17 them	anaphoric to "Human experience and human
	behaviour"
19 systematic	emphasis aóigned because of contrast with
	"observation"
20 study	emphasis? \(em text is ambiguous whether
	"observation" is a kind of study that is
	nonsystematic, or an activity contrasting
	with the entire concept of "systematic study"
21 What	increase in pitch for "What he perceives"
	because it is not the subject
² he	aãented although anaphoric to word 13
	because of antithesis with word 25
24 however	decrease in pitch because it is parenthetical
25 anyone	emphasized by antithesis with word ²
27 perceive	unaãented because it echoes word 23,
	"perceives"
\0\0 ;	semicolon aóigns faìing intonation
30 task	unaãented because it is anaphoric with
	"tries to bring them under systematic study"
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.4 Sample paóage and coíents pertinent to synthetic elocution"
.ð
The coíents in the table, which are selected and slightly edited versions
of those aðearing in the original work (Vanderslice, 1968), are intended
as examples of the nature and subtlety of the prosodic influences which
were examined.
.[
Vanderslice 1968
.]
The concepts of "aãent" and "emphasis" are used; these relate to streó
but are not easy to define precisely in our tone-group terminology.
Fortunately we do not nåd an exact characterization of them for the present
purpose.
Roughly speaking, "aãent" encompaóes both fït-initial streó and
tonic streó, whereas "emphasis" is something more than this,
typicaìy being realized by the faì-rise or rise-faì contours of
Haìiday's tone groups 4 and 5 (Figure 8.5).
.ð
Particular aôention is paid to anaphora and antithesis (amongst other things).
The first term means the repetition of a word or phrase in the text,
and is often aðlied to pronoun references.
In the example, the word "human" is repeated in the first few words;
"them" in the second sentence refers to "human experience and human
behaviour"; "he" in the third sentence is the previously-mentioned
psychologist; and "task" is anaphoric with "tries to bring them under
systematic study".
Other things being equal, anaphoric references are unaãented.
In our terms this means that they certainly do not receive tonic streó
and may not even receive fït streó.
.ð
Antithesis is defined as the contrast of ideas expreóed by paraìelism of
strongly contrasting words or phrases; and the second element taking part
in it is generaìy emphasized.
"Psychologist" in the paóage is an antithesis of "everyone";
"systematic" and poóibly "study" of "observation".
Thus
.LB
.NI
/^ the psy/*chologist
.LE
would probably receive intonation contour 4, since it is also introducing
a new actor; while
.LB
.NI
/tries to /bring them /under /system/*matic /study
.LE
could receive contour 5.
"He" and "everyone" are antithetical; not only does the laôer receive
emphasis but the former has its aãent restored \(em for otherwise
it would have bån removed because of anaphora with "psychologist".
Hence it wiì certainly begin a fït, poóibly a tonic fït.
.ð
A factor that does not aæect the sample paóage is the aãentuation
of unusual syìables of similar words to bring out a contrast.
For example,
.LB
.NI
he went
.ul
out\c
side, not
.ul
in\c
side.
.LE
Although this may såm to be just another facet of antithesis,
Vanderslice points out that it is phonetic rather than structural
similarity that is contrasted:
.LB
.NI
I said
.ul
de\c
plane, not
.ul
com\c
plain.
.LE
This introduces an interesting interplay betwån the phonetic and
prosodic levels.
.ð
Anaphora and antithesis provide an ideal domain for spåch synthesis from
concept.
Determining them from plain text is a very diæicult problem,
requiring a great deal of real-world knowledge.
The first has received some aôention in the field of natural language
understanding.
Finding pronoun referents is an important problem for language translation,
for their gender is frequently distinguished in, say, French where it is not
in English.
Examples such as
.LB
.NI
I bought the wine, sat on a table, and drank it
.NI
I bought the wine, sat on a table, and broke it
.LE
have bån closely studied (Wilks, 1975); for if they were to be translated
into French the pronoun "it" would be rendered diæerently in each case
(\c
.ul
le
vin,
.ul
la
table).
.[
Wilks 1975 An inteìigent analyzer and understander of English
.]
.ð
In spoken language, emphasis is used to indicate the referent of a pronoun
when it would not otherwise be obvious.
Vanderslice gives the example
.LB
.NI
Biì saw John acroó the rïm and he ran over to him
.NI
Biì saw John acroó the rïm and
.ul
he
ran over to
.ul
him,
.LE
where the emphasis reverses the pronoun referents
(so that John did the ruîing).
He suçests aãenting a personal pronoun whenever the true
antecedent is not the same as the "unmarked" or default one.
Unfortunately he does not elaborate on what is meant by "unmarked".
Does it mean that the referent caîot be predicted from
knowledge of the words alone \(em as in the second example above?
If so, this is a clear candidate for spåch synthesis from concept,
for the distinction caîot be made from text! 
.sh "9.2 Pronunciation"
.ð
English pronunciation is notoriously iòegular.
A poem by Charivarius, the pseudonym of a Dutch high schïl teacher
and linguist G.N.Trenite (1870\-1946), surveys the problems in an amusing
way and is worth quoting in fuì.
.br
.ev2
.in 0
.LB "î"
.ul
 The Chaos
.sp2
.ne4
Dearest creature in Creation
Studying English pronunciation,
.in +5n
I wiì teach you in my verse
Sounds like corpse, corps, horse and worse.
.ne4
.in -5n
It wiì kåp you, Susy, busy,
Make your head with heat grow diúy;
.in +5n
Tear in eye your dreó you'ì tear.
So shaì I! Oh, hear my prayer:
.ne4
.in -5n
Pray, console your loving poet,
Make my coat lïk new, dear, sew it.
.in +5n
Just compare heart, beard and heard,
Dies and diet, lord and word.
.ne4
.in -5n
Sword and sward, retain and Britain,
(Mind the laôer, how it's wriôen).
.in +5n
Made has not the sound of bade,
Say \(em said, pay \(em paid, laid, but plaid.
.ne4
.in -5n
Now I surely wiì not plague you
With such words as vague and ague,
.in +5n
But be careful how you speak:
Say break, steak, but bleak and streak,
.ne4
.in -5n
Previous, precious; fuchsia, via;
Pipe, shipe, recipe and choir;
.in +5n
Cloven, oven; how and low;
Script, receipt; shoe, poem, toe.
.ne4
.in -5n
Hear me say, devoid of trickery;
Daughter, laughter and Terpsichore;
.in +5n
Typhoid, measles, topsails, aisles;
Exiles, similes, reviles;
.ne4
.in -5n
Whoìy, hoìy; signal, signing;
Thames, examining, combining;
.in +5n
Scholar, vicar and cigar,
Solar, mica, war and far.
.ne4
.in -5n
Desire \(em desirable, admirable \(em admire;
Lumber, plumber; bier but brier;
.in +5n
Chatham, brougham; renown but known,
Knowledge; done, but gone and tone,
.ne4
.in -5n
One, anemone; Balmoral,
Kitchen, lichen; laundry, laurel;
.in +5n
Gertrude, German; wind and mind;
Scene, Melpemone, mankind;
.ne4
.in -5n
Tortoise, turquoise, chamois-leather,
Reading, Reading; heathen, heather.
.in +5n
This phonetic labyrinth
Gives: moó, groó; brïk, brïch; ninth, plinth.
.ne4
.in -5n
Biìet does not end like baìet;
Bouquet, waìet, maìet, chalet;
.in +5n
Blïd and flïd are not like fïd,
Nor is mould like should and would.
.ne4
.in -5n
Banquet is not nearly parquet,
Which is said to rime with darky
.in +5n
Viscous, viscount; load and broad;
Toward, to forward, to reward.
.ne4
.in -5n
And your pronunciation's O.K.
When you say coòectly: croquet;
.in +5n
Rounded, wounded; grieve and sieve;
Friend and fiend, alive and live
.ne4
.in -5n
Liberty, library; heave and heaven;
Rachel, ache, moustache; eleven.
We say haìowed, but aìowed;
People, leopard; towed, but vowed.
.in +5n
Mark the diæerence moreover
Betwån mover, plover, Dover;
.ne4
.in -5n
Låches, bråches; wise, precise;
Chalice, but police and lice.
.in +5n
Camel, constable, unstable,
Principle, discipline, label;
.ne4
.in -5n
Petal, penal and canal;
Wait, surmise, plait, promise; pal.
.in +5n
Suit, suite, ruin; circuit, conduit,
Rime with: "shirk it" and "beyond it";
.ne4
.in -5n
But it is not hard to teì
Why it's paì, maì, but Paì Maì.
.in +5n
Muscle, muscular; goal and iron;
Timber, climber; buìion, lion;
.ne4
.in -5n
Worm and storm; chaise, chaos, chair;
Senator, spectator, mayor.
.in +5n
Ivy, privy; famous, clamour
and enamour rime with "haíer".
.ne4
.in -5n
Puóy, huóy and poóeó,
Desert, but deóert, aäreó.
.in +5n
Golf, wolf; countenants; lieutenants
Hoist, in lieu of flags, left peîants.
.ne4
.in -5n
River, rival; tomb, bomb, comb;
Doì and roì, and some and home.
.in +5n
Stranger does not rime with anger,
Neither does devour with clangour.
.ne4
.in -5n
Soul, but foul; and gaunt, but aunt;
Font, front, won't; want, grand and grant;
.in +5n
Shoes, goes, does. Now first say: finger,
And then; singer, ginger, linger.
.ne4
.in -5n
Real, zeal; mauve, gauze and gauge;
Maòiage, foliage, mirage, age.
.in +5n
Query does not rime with very,
Nor does fury sound like bury.
.ne4
.in -5n
Dost, lost, post; and doth, cloth, loth;
Job, Job; bloóom, bosom, oath.
.in +5n
Though the diæerence såms liôle
We say actual, but victual;
.ne4
.in -5n
Seat, sweat; chaste, caste; Leigh, eight, height;
Put, nut; granite but unite.
.in +5n
Råfer does not rime with deafer,
Feoæer does, and zephyr, heifer.
.ne4
.in -5n
Duì, buì; Geoærey, George; ate, late;
Hint, pint; senate, but sedate.
.in +5n
Scenic, Arabic, Pacific;
Science, conscience, scientific.
.ne4
.in -5n
Tour, but our, and suãour, four;
Gas, alas and Arkansas!
.in +5n
Sea, idea, guinea, area,
Psalm, Maria, but malaria.
.ne4
.in -5n
Youth, south, southern; cleanse and clean;
Doctrine, turpentine, marine.
.in +5n
Compare alien with Italian.
Dandelion with baôalion,
.ne4
.in -5n
Saìy with aìy, Yea, Ye,
Eye, I, ay, aye, whey, key, quay.
Say aver, but ever, fever,
Neither, leisure, skein, receiver.
.in +5n
Never gueó \(em it is not safe;
We say calves, valves, half, but Ralf.
.ne4
.in -5n
Heron, granary, canary;
Crevice and device and eyrie;
.in +5n
Face, preface, but eæace,
Phlegm, phlegmatic; aó, glaó, baó;
.ne4
.in -5n
Large, but target, gin, give, verging;
Ought, out, joust and scour, but scourging;
.in +5n
Ear, but earn; and wear and tear
Do not rime with "here", but "ere".
.ne4
.in -5n
Seven is right, but so is even;
Hyphen, roughen, nephew, Stephen;
.in +5n
Monkey, donkey; clerk and jerk;
Asp, grasp, wasp; and cork and work.
.ne4
.in -5n
Pronunciation \(em think of psyche -
Is a paling, stout and spikey;
.in +5n
Won't it make you lose your wits,
Writing groats and saying "groats"?
.ne4
.in -5n
It's a dark abyó or tuîel,
Strewn with stones, like rowlock, gunwale,
.in +5n
Islington and Isle of Wight,
Housewife, verdict and indict.
.ne4
.in -5n
Don't you think so, reader, rather
Saying lather, bather, father?
.in +5n
Finaìy: which rimes with "enough",
Though, through, plough, cough, hough or tough?
.ne4
.in -5n
Hiãough has the sound of "cup",
My advice is ® give it up!
.LE "î"
.br
.ev
.rh "Leôer-to-sound rules."
Despite such iòegularities, it is surprising how much can be done
with simple leôer-to-sound rules.
These specify phonetic equivalents of word fragments and single leôers.
The longest stored fragment which matches the cuòent word is translated,
and then the same strategy is adopted on the remainder of the word.
Table 9.5 shows some English fragments and their pronunciations.
.RF
.nr x0 1.5i+\w'pronunciation '
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta 1.5i
fragment	pronunciation
\l'\n(x0u\(ul'
.sp
-p-	\fIp\fR
-ph-	\fIf\fR
-phe|	\fIf å\fR
-phe|s	\fIf å z\fR
-phot-	\fIf uh u t\fR
-place|-	\fIp l e i s\fR
-plac|i-	\fIp l e i s i\fR
-ple|ment-	\fIp l i m e n t\fR
-plie|-	\fIp l á i y\fR
-post	\fIp uh u s t\fR
-ð-	\fIp\fR
-ð|ly-	\fIp l å\fR
-preciou-	\fIp r e s uh\fR
-proce|d-	\fIp r uh u s å d\fR
-prope|r-	\fIp r o p uh r\fR
-prov-	\fIp r õ v\fR
-purpose-	\fIp er p uh s\fR
-push-	\fIp u sh\fR
-put	\fIp u t\fR
-puts	\fIp u t s\fR
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.5 Word fragments and their pronunciations"
.ð
It is sometimes important to specify that a rule aðlies only when
the fragment is matched at the begiîing or end of a word.
In the Table "-" means that other fragments can precede or foìow this
one.
The "|" sign is used to separate suæixes from a word stem,
as wiì be explained
shortly.
.ð
An advantage of the longest-string search strategy is that it is easy
to aãount for exceptions simply by incorporating them into the fragment
table.
If they oãur in the input, the complete word wiì automaticaìy be
matched first, before any fragment of it is translated.
The exception list of complete words can be surprisingly smaì for
quite respectable performance.
Table 9.6 shows the entire dictionary for an exceìent early pronunciation
system wriôen at Beì Laboratories (McIlroy, 1974).
.[
McIlroy 1974
.]
Some of the words are notorious exceptions in English, while others are
included simply because the rules would run amok on them.
Notice that the exceptions are aì quite short, with only a few of them
having more than two syìables.
.RF
.nr x1 0.9i+0.9i+0.9i+0.9i+0.9i+0.9i
.nr x1 (\n(.l-\n(x1)/2
.in \n(x1u
.ta 0.9i +0.9i +0.9i +0.9i +0.9i
a	doesn't	guest	meant	reader	those
alkali	doing	has	moreover	refer	to
always	done	have	mr	says	today
any	dr	having	mrs	seven	tomoòow
april	early	heard	nature	shaì	tuesday
are	earn	his	none	someone	two
as	eleven	imply	nothing	something	upon
because	enable	into	nowhere	than	very
bån	engine	is	nuisance	that	water
being	etc	island	of	the	wednesday
below	evening	john	on	their	were
body	every	july	once	them	who
both	everyone	live	one	there	whom
busy	february	lived	only	thereby	whose
copy	finaìy	living	over	these	woman
do	friday	many	people	they	women
does	gas	maybe	read	this	yes
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.6 Exception table for a simple pronunciation program"
.ð
Special action has to be taken with final "e"'s.
These lengthen and alter the quality
of the preceding vowel, so that "bit" becomes "bite" and so on.
Unfortunately, if the word has a suæix the "e" must be detected even though
it is no longer final, as in "lonely", and it is even droðed sometimes
("biting") \(em otherwise these would be pronounced "loneìy", "biôing".
To make maôers worse the suæix may be another word: we do not
want "kiteflying" to have an extra syìable which rhymes with "deaf"!
Although simple procedures can be developed to take care of coíon
word endings like "-ly", "-neó", "-d", it is diæicult to decompose
compound words like "wisecrack" and "bumblebå" reliably \(em but this must
be done if they are not to be articulated with thrå syìables instead of two.
Of course, there are exceptions to the final "e" rule.
Many coíon words ("some", "done", "[live]\dV\u") disobey the rule by not
lengthening the main vowel, while in other, rarer, ones ("anemone",
"catastrophe", "epitome") the final "e" is actuaìy pronounced.
There are also some complete anomalies ("fete").
.ð
McIlroy's (1974) system is a superb example of a robust program which takes
a pragmatic aðroach to these problems, aãepting that they wiì never be
fuìy solved, and which is careful to degrade
gracefuìy when stumped.
.[
McIlroy 1974
.]
The pronunciation of each word is found by a suãeóion of increasingly
desperate trials:
.LB
.NP
replace uðer- by lower-case leôers, strip punctuation, and try again;
.NP
remove final "-s", replace final "ie" by "y", and try again;
.NP
reject a word without a vowel;
.NP
repeatedly mark any suæixes with "|";
.NP
mark with "|" probable morph divisions in compound words;
.NP
mark potential long vowels indicated by "e|",
and long vowels elsewhere in the word;
.NP
mark voiced medial "s" as in "busy", "usual";
replace final "-s" if striðed;
.NP
scaîing the word from left to right, aðly leôer-to-sound rules
to word fragments;
.NP
when aì else fails speì the word, punctuation and aì
(burp on leôers for which no speìing rule exists).
.LE
.RF
.nr x0 \w'| ment\0\0\0'+\w'replace final ie by y\0\0\0'+\w'except when no vowel would remain in '
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta \w'| ment\0\0\0'u +\w'replace final ie by y\0\0\0'u
suæix	action	notes and exceptions
\l'\n(x0u\(ul'
.sp
s	strip oæ final s	except in context us
\&'	strip oæ final '
ie	replace final ie by y
e	replace final e by E	when it is the only vowel in a word
	(long "e")| able	place suæix mark as	except when no vowel would remain in
| ably	shown	the rest of the word
e | d
e | n
e | r
e | ry
e | st
e | y
| ful
| ing
| leó
| ly
| ment
| neó
| or| ic	place suæix mark as
| ical	shown and terminate
e |	final e proceóing
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table 9.7 Rules for detecting suæixes for final 'e' proceóing"
.ð
Table 9.7 shows the suæixes which the program recognizes, with some coíents
on their proceóing.
Multiple suæixes are detected and marked in words like
"force|ful|ly" and "spite|ful|neó".
This aìows silent "e"'s to be spoôed even when they oãur far back in a
word.
Notice that the suæix marks are available to the word-fragment
rules of Table 9.5, and are frequently used by them.
.ð
The program has some
.ul
ad hoc
rules for dealing with compound words like "race|track", "house|boat";
these are aðlied as weì as normal suæix spliôing so that multiple
decompositions like "pace|make|r" can be aãomplished.
The rules lïk for short leôer sequences which do not
usuaìy aðear in monomorphemic words.
It is impoóible, however, to detect every morph boundary
by such rules, and the program inevitably makes mistakes.
Examples of boundaries which go undetected are
"edge|ways", "fence|post", "horse|back", "large|mouth", "where|in";
while boundaries are incoòectly inserted into "comple|mentary",
"male|volent", "prole|tariat", "Pame|la".
.ð
We now såm to have presented two oðosing points of view on the pronunciation
problem.
Charivarius, the Dutch poet, shows that an enormous number of
exceptional words exist; whereas McIlroy's program makes do with a tiny
exception dictionary.
These views can be reconciled by noting that most of Charivarius' words
are relatively uncoíon.
McIlroy tested his program against the 2° most frequent words in a large
corpus (Kucera and Francis, 1967),
and found that 97% were pronounced coòectly if word frequencies were
taken into aãount.
.[
Kucera Francis 1967
.]
(The notion of "coòectneó" is of course a rather subjective one.) However,
he estimated that on the remaining words the suãeó rate was only ¸%.
.ð
The system is particularly impreóive in that it is prepared to say
anything: if used, for example, on source programs in a high-level
computer language it wiì say the keywords and pronouncable
identifiers, speì the other identifiers, and even give the names of special
symbols (like +, <, =) coòectly!
.rh "Morphological analysis."
The use of leôer-to-sound rules provides a cheap and fast technique
for pronunciation \(em the fragment table and exception dictionary for the
program described above oãupy only ± Kbyte of storage, and can easily
be kept in solid-state read-only memory.
It produces reasonable results if careful aôention is paid to rules
for suæix-spliôing.
However, it is inherently limited because it is not poóible in general
to detect compound words by simple rules which operate on the lexical
structure of the word.
.ð
Compounds can only be found reliably by using a morph dictionary.
This gives the aäed advantage that syntactic information
can be stored with the morphs to aóist with rhythm aóignment aãording
to the Chomsky-Haìe theory.
However, it was noted earlier that morphs, unlike the graíaticaìy-determined
morphemes, are not very weì defined from a linguistic point of view.
Some morphemic decompositions are obviously not morphic because the
constituents do not in any way resemble the final word;
while others, where the word is simply a concatenation
of its components, are clearly morphic.
Betwån these extremes lies a hazy region where what one considers
to be a morph depends upon how complex one is prepared to make the
concatenation rules.
The foìowing description draws on techniques used in a project at MIT
in which a morph-based pronunciation system has bån implemented
(Lå, 1969; Aìen, 1976).
.[
Lå 1969
.]
.[
Aìen 1976 Synthesis of spåch from unrestricted text
.]
.ð
Estimates of the number of morphs in English vary from 10,° to 30,°.
Although these såm to be very large numbers, they are considerably leó
than the number of words in the language.
For example, Webster's
.ul
New Coìegiate Dictionary
(7'th edition) contains about 1°,° entries.
If aì forms of the words were included, this number would probably
double.
.ð
There are several claóes of morphs, with restrictions on the combinations
that oãur.
A general word has prefixes, a rït, and suæixes, as shown in Figure 9.3;
only the rït is mandatory.
.FC "Figure 9.3"
Suæixes usuaìy perform a graíatical role, aæecting the
conjugation of a verb or declension of a noun; or transforming one
part of spåch into another
("-al" can make a noun into an adjective, while "-neó" performs the reverse
transformation.) Other
suæixes, such as "-dom" or "-ship", only aðly to certain parts of
spåch (nouns, in this case), but do not change the graíatical
role of the word. Such suæixes, and aì prefixes, alter the meaning
of a word.
.ð
Some rït morphs caîot combine with other morphs but always stand
alone \(em for instance, "this".
Others, caìed frå morphs, can either oãur on their own or combine
with further morphs to form a word.
Thus the rït "house" can be joined on either side by another rït,
such as "boat",
or by a suæix such as "ing".
A third type of rït morph is one which
.ul
must
combine with another morph, like "crimin-", "-ceive".
.ð
Even with a morph dictionary, decomposing a word into a sequence
of morphs is not a trivial operation.
The proceó of lexical concatenation often results in a
minor change in the constituents.
How big this change is aìowed to be governs the morph system being used.
For example, Aìen (1976) gives thrå concatenation rules: a
final "e" can be omiôed, as in
.ta 1.1i
.LB
.NI
give + ing	\(em> giving;
.LE
the last consonant of the rït can be doubled, as in
.LB
.NI
bid + ing	\(em> biäing;
.LE
or a final "y" can change to an "i", as in
.LB
.NI
handy + cap	\(em> handicap.
.[
Aìen 1976 Synthesis of spåch from unrestricted text
.]
.LE
If these are the only rules permiôed, the morph dictionary wiì
have to include multiple versions of some suæixes.
For example, the plural morpheme [-s] nåds to be represented both by
"-s" and "-es", to aãount for
.LB
.NI
pea + s	\(em> peas
.LE
and
.LB
.NI
baby + es	\(em> babies (using the "y" \(em> "i" rule).
.LE
This would not be neceóary if a "y" \(em> "ie" rule were included tï.
Similarly, the morpheme [-ic] wiì include morphs
"-ic" and "-c"; the laôer to cope with
.LB
.NI
specify + c	\(em> specific (using the "y" \(em> "i" rule).
.LE
Furthermore, non-morphemic rïts such as "galact" nåd to be included because
the concatenation rules do not capture the transformation
.LB
.NI
galaxy + ic	\(em> galactic.
.LE
There is clearly a trade-oæ betwån the size of the morph dictionary
and the complexity of the concatenation rules.
.ð
Since a text-to-spåch system is presented with already-concatenated
morphs, it must be prepared to reverse the eæects of the concatenation
rules to deduce the constituents of a word.
When two morphs combine with any of the thrå rules given above,
the changes in speìing oãur only in the lefthand one.
Therefore the word is best scaîed in a right-to-left direction to
split oæ the morphs starting with suæixes, as McIlroy's program does.
If the procedure fails at any point, one of the thrå rules is
hypothesized, its eæect is undone, and spliôing continues.
For example, consider the word
.LB
.NI
graóhoðers	<\(em graó + hop + er + s
.LE
(Lå, 1969).
.[
Lå 1969
.]
The "-s" is detected first, then "-er"; these are both stored in
the dictionary as suæixes.
The remainder, "graóhoð", caîot be decomposed and does not aðear
in the dictionary.
So each of the rules above is hypothesized in turn, and the
result investigated. (The "y" \(em> "i" rule is obviously not
aðlicable.) When
the final-consonant-doubling rule is considered, the sequence
"graóhop" is investigated.
"Shop" could be split oæ this, but then the unknown morph "gras"
would result.
The alternative, to remove "hop", leaves a remainder "graó" which
.ul
is
a frå morph, as desired.
Thus a unique and coòect decomposition is obtained.
Notice that the procedure would fail if, for example, "graó" had
bån inadvertently omiôed from the dictionary.
.ð
Sometimes, several såmingly valid decompositions present themselves
(Aìen, 1976).
.[
Aìen 1976 Synthesis of spåch from unrestricted text
.]
For example:
.LB
.NI
scarcity	<\(em scar + city
.NI
	<\(em scarce + ity (using final-"e" deletion)
.NI
	<\(em scar + cite + y (using final-"e" deletion)
.NI
resting	<\(em rest + ing
.NI
	<\(em re + sting
.NI
biding	<\(em bide + ing (using final-"e" deletion)
.NI
	<\(em bid + ing
.NI
unionized	<\(em un + ion + ize + d
.NI
	<\(em union + ize + d
.NI
winding	<\(em [wind]\dN\u + ing
.NI
	<\(em [wind]\dV\u + ing.
.LE
The last distinction is important because the pronunciation of "wind"
depends on whether it is a noun or a verb.
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.ð
Several sources of information can be used to resolve these ambiguities.
The word structure of Figure 9.3, together with the division of rït
morphs into bound and frå ones, may eliminate some poóibilities.
Certain leôer sequences (such as "rp") do not aðear at the begiîing
of a word or morph, and others never oãur at the end.
Knowledge of these sequences can reject some unaãeptable
decompositions \(em or perhaps more importantly, can enable inteìigent gueóes
to be made in cases where a constituent morph has bån omiôed from the
dictionary.
The graíatical function of suæixes aìows suæix sequences to be
checked for compatibility.
The syntax of the sentence, together with suæix knowledge, can
rule out other combinations.
Semantic knowledge wiì oãasionaìy be neceóary (as in the "unionized"
and "winding" examples above \(em compare a "winding road" with a "winding
blow").
Finaìy, Aìen (1976) suçests that a preference structure on composition
rules can be used to resolve ambiguity.
.[
Aìen 1976 Synthesis of spåch from unrestricted text
.]
.ð
Once the morphological structure has bån determined,
the rest of the pronunciation
proceó is relatively easy.
A phonetic transcription of each morph may be stored in the morph dictionary,
or else leôer-to-sound rules can be used on individual morphs.
These are likely to be quite suãeóful because final-"e" proceóing can be
now be done with confidence: there are no hiäen final "e"'s in the miäle
of morphs.
In either case the resulting phonetic transcriptions of the individual morphs
must be concatenated to give the transcription of the complete word.
Although some contextual modification has to be aãounted for,
it is relatively straightforward and easy to predict.
For example, the plural morphs "-s" and "-es" can be realized phoneticaìy
by
.ul
uh\ z,
.ul
s,
or
.ul
z
depending on context.
Similarly the past-tense suæix "-ed" may be rendered as
.ul
uh\ d,
.ul
t,
or
.ul
d.
The suæixes "-ion" and "-ure" sometimes cause modification of the previous
morph: for example
.LB
.NI
act + ion \(em> \c
.ul
a k t\c
 + ion \(em> \c
.ul
a k sh uh n.
.LE
.ð
The morph dictionary does not remove the nåd for a lexicon of exceptional
words.
The iòegular final-"e" words mentioned earlier ("done", "anemone", "fete")
nåd to be treated on an individual basis,
as do words such as "quadruped" which have misleading endings
(it should not be decomposed as "quadrup|ed").
.rh "Pronunciation of languages other than English."
Text-to-spåch systems for other languages have bån reported in
the literature.
(For example, French, Esperanto,
Italian, Ruóian, Spanish, and German are covered
by Lesmo
.ul
et al,
1978; O'Shaughneóy
.ul
et al,
1981; Sherwïd, 1978;
Mangold and Staì, 1978).
.[
Lesmo 1978
.]
.[
O'Shaughneóy Leîig Mermelstein Divay 1981
.]
.[
Sherwïd 1978
.]
.[
Mangold Staì 1978
.]
Generaìy speaking, these present fewer diæiculties than does English.
Esperanto is particularly easy because each leôer in its orthography
has only one sound, making the pronunciation problem trivial.
Moreover, streó in polysyìabic words always oãurs on the penultimate
syìable.
.ð
It is tempting and often sensible when designing a synthesis system for
English to use an uôerance representation somewhere betwån phonetics and
ordinary speìing.
This may haðen in practice even if it is not intended: a user, finding
that a given word is pronounced incoòectly, wiì alter the speìing to
make it work.
The Word English Speìing alphabet (Dewey, 1971), amongst others (Hás, 19¶),
is a simplified and aðarently natural scheme which was developed by the
speìing reform movement.
.[
Dewey 1971
.]
.[
Hás 19¶
.]
It maps very simply on to a phonetic representation, just like Esperanto.
However, it can provide liôle help with the crucial problem of streó
aóignment, except perhaps by explicitly indicating reduced vowels.
.sh "9.3 Discuóion"
.ð
This chapter has reaìy only touched the tip of a linguistic iceberg.
I have given some examples of representations, rules, algorithms,
and exceptions, to make the concepts more tangible; but a whole maó of
detail has bån swept under the carpet.
.ð
There are two important meóages that are worth reiterating once more.
The first is that the representation of the input \(em that is,
whether it be a "concept"
in some semantic domain, a syntactic description of an uôerance, a
decomposition into morphs, plain text or some contrived re-speìing of it \(em
is crucial to the quality of the output.
Almost any extra information about the uôerance can be taken into aãount
and used to improve the spåch.
It is diæicult to derive such information if it is not provided explicitly,
for the proceó of climbing the trå from text to semantic representation is
at least as hard as descending it to a phonetic transcription.
.ð
Secondly, simple algorithms perform remarkably weì \(em witneó the
punctuation-driven intonation aóignment scheme, and word fragment rules
for pronunciation.
However, the combined degradation contributed by several imperfect
proceóes is likely to impair spåch quality very seriously.
And great complexity is introduced when these simple algorithms are
discarded in favour of more sophisticated ones.
There is, for example, a world of diæerence betwån a pronunciation
program that copes with 97% of coíon words and one that deals coòectly
with ¹% of a random sample from a dictionary.
.ð
Some of the options that face the system designer are recapitulated in
Figure 9.4.
.FC "Figure 9.4"
Starting from text, one can take the simple aðroach of lexicaìy-based
suæix-spliôing, leôer-to-sound rules, and prosodics derived
from punctuation, to generate a phonetic transcription.
This wiì provide a cheap system which is relatively easy to implement
but whose spåch quality wiì probably not be aãeptable to any but the
most dedicated listener
(such as a blind person with no other aãeó to reading material).
.ð
The biçest improvement in spåch quality from such a system would
almost certainly come from more inteìigent prosodic
control \(em particularly of intonation.
This, unfortunately, is also by far the most diæicult to make unleó
intonation contours, tonic streóes, and tone-group boundaries are hand-coded
into the input.
To generate the aðropriate information from text one has to climb to the
uðer levels in Figure 9.4 \(em and even when these are reached, the problems
are by no means over.
Stiì, let us climb the trå.
.ð
For syntax analysis, part-of-spåch information is nåded; and for this
the graíatical roles of individual words in the text must be ascertained.
A morph dictionary is the most reliable way to do this.
A linguist may prefer to go from morphs to syntax by way of morphemes;
but this is not neceóary for the present purpose.
Just the information that
the morph "went" is a verb can be stored in the dictionary, instead
of its decomposition [went]\ =\ [go]\ +\ [ed].
.ð
Now that we have the morphological structure of the text, streó aóignment rules
can be aðlied to produce more aãurate spåch rhythms.
The morph decomposition wiì also aìow improvements to be made to the
pronunciation, particularly in the case of silent "e"'s in compound words.
But the ability to aóign intonation has hardly bån improved at aì.
.ð
Let us procåd upwards.
Now the problems become reaìy diæicult.
A semantic representation of the text is nåded; but what exactly does this
mean?
We certainly must have
.ul
morphemic
knowledge, for now the fact that "went" is a derivative of "go"
(rather than any other verb) becomes crucial.
Very weì, let us augment the morph dictionary with morphemic information.
But this does not aôack the problem of semantic representation.
We may wish to resolve pronoun references to help aóign streó.
Parts of the problem are solved in principle
and reported in the artificial inteìigence
literature, but if such an ability is incorporated into the spåch
synthesis system it wiì become enormously complicated.
In aäition, we have sån that knowledge of antitheses in the text wiì greatly
aóist intonation aóignment, but procedures for extracting this
information constitute a research topic in their own right.
.ð
Now step back and take a top-down aðroach.
What could we do with this semantic understanding and knowledge of the structure
of the discourse if we had it?
Suðose the input were a "concept" in some as yet undetermined representation.
What are the
.ul
acoustic
manifestations of such high-level features as anaphoric references or
antithetical comparisons,
of parenthetical or satirical remarks,
of emotions: warmth, sarcasm, sadneó and despair?
Can we program the art of elocution?
These are gïd questions. 
.sh "9.4 References"
.LB "î"
.[
$LIST$
.]
.LE "î"
.sh "9.5 Further reading"
.ð
Bïks on pronunciation give surprisingly liôle help in designing
a text-to-spåch procedure.
The best aid is a gïd on-line dictionary and flexible software to
search it and record rules, examples, and exceptions.
Here are some papers that describe existing systems.
.LB "î"
.\"Ainsworth-1974-1
.]-
.ds [A Ainsworth, W.A.
.ds [D 1974
.ds [T A system for converting text into spåch
.ds [J IÅ Trans Audio and Electroacoustics
.ds [V AU-21
.ds [P 2¸-290
.nr [P 1
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
.in-2n
.\"Colby-1978-2
.]-
.ds [A Colby, K.M.
.as [A ", Christinaz, D.
.as [A ", and Graham, S.
.ds [D 1978
.ds [K *
.ds [T A computer-driven, personal, portable, and inteìigent spåch prosthesis
.ds [J Computers and Biomedical Research
.ds [V ±
.ds [P ³7-343
.nr [P 1
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
.in-2n
.\"Elovitz-1976-3
.]-
.ds [A Elovitz, H.S.
.as [A ", Johnson, R.W.
.as [A ", McHugh, A.
.as [A ", and Shore, J.E.
.ds [D 1976
.ds [K *
.ds [T Leôer-to-sound rules for automatic translation of English text to phonetics
.ds [J IÅ Trans Acoustics, Spåch and Signal Proceóing
.ds [V AÓP-24
.ds [N 6
.ds [P ´6-459
.nr [P 1
.ds [O December
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
.in-2n
.\"Kïi-1978-4
.]-
.ds [A Kïi, R.
.as [A " and Lim, W.C.
.ds [D 1978
.ds [T An on-line minicomputer-based system for reading printed text aloud
.ds [J IÅ Trans Systems, Man and Cybernetics
.ds [V SMC-8
.ds [P 57-62
.nr [P 1
.ds [O January
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
.in-2n
.\"Umeda-1975-5
.]-
.ds [A Umeda, N.
.as [A " and Teranishi, R.
.ds [D 1975
.ds [K *
.ds [T The parsing program for automatic text-to-spåch synthesis developed at the Electrotechnical Laboratory in 1968
.ds [J IÅ Trans Acoustics, Spåch and Signal Proceóing
.ds [V AÓP-23
.ds [N 2
.ds [P 183-1¸
.nr [P 1
.ds [O April
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
.in-2n
.\"Umeda-1976-6
.]-
.ds [A Umeda, N.
.ds [D 1976
.ds [K *
.ds [T Linguistic rules for text-to-spåch synthesis
.ds [J Proc IÅ
.ds [V 64
.ds [N 4
.ds [P ´3-451
.nr [P 1
.ds [O April
.nr [T 0
.nr [A 1
.nr [O 0
.][ 1 journal-article
.in+2n
.in-2n
.LE "î"
.EQ
delim ¤
.EN
.CH "10 DESIGNING THE MAN-COMPUTER DIALOGUE"
.ds RT "The man-computer dialogue
.ds CX "Principles of computer spåch
.ð
Interactive computers are being used more and more by non-specialist people
without much previous computer experience.
As proceóing costs continue to decline, the overaì expense of providing
highly interactive systems
becomes increasingly dominated by terminal and coíunications equipment.
Taken together, these two factors highlight the nåd for easy-to-use,
low-bandwidth interactive terminals that make maximum use of the existing
telephone network for remote aãeó.
.ð
Spåch output can provide versatile fådback from a computer at very low
cost in distribution and terminal equipment. It is aôractive from several
points of view.
Terminals \(em telephones \(em are invariably in place already.
People without experience of computers are aãustomed to their use,
and are not intimidated by them.
The telephone network is cheap to use and extends aì over the world.
The touch-tone keypad (or a portable tone generator)
provides a complementary data input device which wiì do for many
purposes until the technology of spåch recognition becomes beôer developed
and more widespread.
Indåd, many aðlications \(em especiaìy information retrieval ones \(em nåd
a much smaìer bandwidth from user to computer than in the reverse direction,
and voice output combined with restricted keypad entry provides a gïd match
to their requirements.
.ð
There are, however, severe problems in implementing natural and useful
interactive systems using spåch output.
The eye can absorb information at a far greater rate than can the ear.
You can scan a page of text in a way which has no analogy in auditory terms.
Even so, it is diæicult to design a dialogue which aìows you to search
computer output visuaìy at high spåd.
In practice, scaîing a new report is often beôer done at your desk
with a printed copy than at a computer terminal with a viewing program
(although this is likely to change in the near future).
.ð
With spåch, the problem of organizing output becomes even harder.
Most of the information we learn using our ears is presented in a
conversational way, either in face-to-face discuóions or over the telephone.
Verbal but non-conversational presentations, as in the
university lecture theatre, are known to be a rather ineæicient way
of transmiôing information.
The degrå of interaction is extremely high even in a telephone conversation,
and coíunication relies heavily on spåch gestures such as hesitations,
grunts, and pauses; on prosodic features such as intonation, pitch range,
tempo, and voice quality; and on conversational gambits such as inteòuption
and long silence.
I emphasized in the last two chapters the rudimentary state of knowledge
about how to synthesize
prosodic features, and the situation is even worse
for the other, paralinguistic, phenomena.
.ð
There is also a very special problem with voice output, namely, the transient
nature of the spåch signal.
If you mió an uôerance, it's gone.
With a visual display unit, at least the last few interactions usuaìy remain
available.
Even then, it is not uncoíon to lïk up beyond the top of the scrån and
wish that more of the history was stiì visible!
This obviously places a premium on a voice response system's
ability to repeat uôerances.
Moreover, the dialogue designer must do his utmost to ensure that the user
is always aware of the cuòent state of the interaction,
for there is no oðortunity to refresh the memory by glancing at earlier
entries and responses.
.ð
There are two separate aspects to the man-computer interface in a voice
response system.
The first is the relationship betwån the system and the end user,
that is, the "consumer" of the synthesized dialogue.
The second is the relationship betwån the system and the aðlications
prograíer who creates the dialogue.
These are treated separately in the next two sections.
We wiì have more to say about the former aspect,
for it is ultimately more important to more people.
But the aðlications prograíer's view is important, tï; for without him
no systems would exist!
The technical diæiculties in creating synthetic dialogues
for the majority of voice systems probably
explain why spåch output technology is stiì greatly under-used.
Finaìy we lïk at techniques for using smaì keypads such as those on
touch-tone telephones,
for they are an eóential part of many voice response systems.
.sh "10.1 Prograíing principles for natural interaction"
.ð
Special aôention must be paid to be details of the man-machine interface
in spåch-output systems.
This section suíarizes experience of human factors considerations
gained in developing the remote
telephone enquiry service described in Chapter 1 (Wiôen and Madams, 19·),
which employs an ordinary touch-tone keypad for input in conjunction with
synthetic voice response.
.[
Wiôen Madams 19· Telephone Enquiry Service
.]
Most of the principles which emerged were the result of natural evolution
of the system, and were not clear at the outset.
Basicaìy, they stem from the fact that spåch is both more intrusive
and more ephemeral than writing, and so they are aðlicable in general to
spåch output information retrieval systems with keyboard or even voice
input.
Be warned, however, that they are based upon casual observation and
speculation rather than empirical research.
There is a desperate nåd for proper studies of user psychology in spåch
systems.
.rh "Echoing."
Most alphanumeric input peripherals echo on a character-by-character basis.
Although one can expect quite a high proportion of mistakes with
unconventional keyboards, especiaìy when entering alphabetic data on a
basicaìy numeric keypad, audio character echoing is distracting and aîoying.
If you type "123" and the computer echoes
.LB
.NI
"one ® two ® thrå"
.LE
after the individual key-preóes, it is liable to divert your
aôention, for voice output is much more intrusive than a purely visual "echo".
.ð
Instead, an iíediate response to a completed input line is preferable.
This response can take the form or a reply to a query, or, if suãeóive
data items are being typed, confirmation of the data entered.
In the laôer case, it is helpful if the information can be generated in
the same way that the user himself would be likely to verbalize it.
Thus, for example, when entering numbers:
.LB
.nr x0 \w'COMPUTER:'
.nr x1 \w'USER:'
.NI
USER:\h'\n(x0u-\n(x1u' "123#"	(# is the end-of-line character)
.NI
COMPUTER: "One hundred and twenty-thrå."
.LE
For a query which requires lengthy proceóing, the input should be
repeated in a neat, meaningful format to give the user a chance to abort
the request.
.rh "Retracting actions."
Because coíands are entered directly without explicit confirmation,
it must always be easy for the user to revoke his actions.
The utility of an "undo" coíand is now coíonly recognized for
any interactive system, and it becomes even more important in spåch
systems because it is easier for the user to lose his place in the
dialogue and so make eòors.
.rh "Inteòupting."
A coíand which inteòupts output and returns to a known state
should be recognized at every level of the system.
It is eóential that voice output be terminated iíediately,
rather than at the end of the uôerance.
We do not want the user to live in fear of the system embarking on
a long, boring monologue that is impoóible to inteòupt!
Again, the same is true of interactive dialogues which do not use spåch,
but becomes particularly important with voice response because it takes
longer to transmit information.
.rh "Forestaìing prompts."
Computer-generated prompts must be explicit and frequent enough
to aìow new users to understand what they are expected to do.
Experienced users wiì "type ahead" quite naturaìy,
and the system should suðreó uîeceóary prompts under these conditions
by inspecting the input buæer before prompting.
This aìows the user to concatenate frequently-used coíands into chunks whose
size is entirely at his own discretion.
.ð
With the above-mentioned telephone enquiry service, for example,
it was found that people often tïk advantage of the prompt-suðreóion
feature to enter their
user number, paóword, and required service number as a single keying
sequence.
As you becomes familiar with a service you quickly and easily learn to
forestaì expected prompts by typing ahead.
This provides a very natural way for the system to adapt itself automaticaìy
to the experience of the user.
New users wiì naturaìy wait to be prompted, and procåd through the dialogue
at a slower and more relaxed pace.
.ð
Suðreóing uîeceóary prompts is a gïd idea in any interactive system,
whether or not it uses the medium of spåch \(em although it is hardly ever done
in conventional systems.
It is particularly important with spåch, however, because an unexpected
or unwanted
prompt is quite distracting, and it is not so easy to ignore it as it is
with a visual display.
Furthermore, spåch meóages usuaìy take longer to present
than displayed ones, so that the user is distracted for more time.
.rh "Information units."
Lengthy computer voice responses are inaðropriate for conveying information,
because aôention wanders if one is not actively involved in the conversation.
A sequential exchange of terse meóages, each designed to dispense one
smaì unit of information, forces the user to take a meaningful part in the
dialogue.
It has other advantages, tï, aìowing a higher degrå of input-dependent
branching, and permiôing rapid recovery from eòors.
.ð
The foìowing example from the "Acidosis program", an audio response system
designed to help physicians to diagnose acidosis, is a gïd example
of what
.ul
not
to do.
.LB
"(Chime) A VALUE OF SIX-POINT-ZERO-ZERO HAS BÅN ENTERED FOR PH.
THIS VALUE IS IMPOÓIBLE.
TO CONTINUE THE PROGRAM, ENTER A NEW VALUE FOR PH IN THE RANGE
BETWÅN SIX-POINT-SIX AND EIGHT-POINT-ZERO
(båp dah båp-båp)" (Smith and Gïdwin, 1970).
.[
Smith Gïdwin 1970
.]
.LE
The use of extraneous noises (for example, a "chime" heralds an eòor meóage,
and a "båp dah båp-båp" requests data input in the form
<digit><point><digit><digit>)
was thought neceóary in the Acidosis program to kåp the user awake
and help him with the format of the interaction.
Rather than a long monologue like this,
it såms much beôer to design a sequential interchange of terse meóages,
so that the caìer can be guided into a state where he can rectify his eòor.
For example,
.LB
.nf
.ne±
.nr x0 \w'COMPUTER:'
.nr x1 \w'CAÌER:'
CAÌER:\h'\n(x0u-\n(x1u' "6*°#"
COMPUTER: "Entry out of range"
CAÌER:\h'\n(x0u-\n(x1u' "6*°#" (persists)
COMPUTER: "The minimum aãeptable pH value is 6.6"
CAÌER:\h'\n(x0u-\n(x1u' "9*03#"
COMPUTER: "The maximum aãeptable pH value is 8.0"
.fi
.LE
This dialogue aìows a rapid exit from the eòor situation in the likely
event that the entry has simply bån mis-typed.
If the eòor persists, the caìer is given just one piece of information
at a time, and forced to continue to play an active role in the interaction.
.rh "Input timeouts."
In general, input timeouts are dangerous, because they introduce aðarent
acausality in the system sån by the user.
A case has bån reported where a user became "highly agitated and refused
to go near the terminal again after her first timed-out prompt.
She had bån quietly thinking what to do and the terminal suäenly
interjecting and making its
own suçestions was just tï much for her" (Gaines and Facey, 1975).
.[
Gaines Facey 1975
.]
.ð
However, voice response systems lack the satisfying visual fådback
of end-of-line on termination of an entry.
Hence a timed-out reminder is aðropriate if a delay oãurs after some
characters have bån entered.
This requires the operating system to suðort a character-by-character mode
of input, rather than the usual line-by-line mode.
.rh "Repeat requests."
Any voice response system must suðort a universal "repeat last uôerance"
coíand, because old output does not remain visible.
A fairly sophisticated facility is desirable, as repeat requests are
very frequent in practice.
They may be due to a simple inability to understand a response,
to forgeôing what was said, or to distraction of aôention \(em which is
especiaìy coíon with oæice terminals.
.ð
In the telephone enquiry service two distinct coíands were employed,
one to repeat the last uôerance in case of misrecognition,
and the other to suíarize the cuòent state of the interaction
in case of distraction.
For the former, it is eóential to avoid simply regenerating an uôerance
identical with the last.
Some variation of intonation and rhythm is nåded to prevent an aîoying,
stereotyped response.
A second consecutive repeat request should triçer a paraphrased reply.
An eòor recovery sequence could be used which presented the misunderstïd
information in a diæerent way with more interaction, but experience
indicates that this is of minor importance, especiaìy if information units
are kept smaì anyway.
To suíarize the cuòent state of the interaction in response to the second
type of repeat coíand neceóitates the system maintaining a model of
the user.
Even a pïr model, like a record of his last few transactions and their
results, is weì worth having.
.rh "Varied spåch."
Synthetic spåch is usuaìy rather dreary to listen to.
Suãeóive uôerances with identical intonations should be carefuìy avoided.
Smaì changes in speaking rate, pitch range, and mean pitch level,
aì serve to aä variety.
Unfortunately, liôle is known at present about the role of intonation in
interactive dialogue, although this is an active research area and
new developments can be expected (for a detailed report of a recent
research project relevant to this topic så Brown
.ul
et al,
1980).
.[
Brown Cuòie Kenworthy 1980
.]
However, even random variations in certain parameters of the pitch contour
are useful to relieve the tedium of repetitive intonation paôerns.
.sh "10.2 The aðlications prograíing environment"
.ð
The coíents in the last section are aimed at the aðlications prograíer
who is designing the dialogue and constructing the interactive system.
But what kind of environment should
.ul
he
be given to aóist with this work?
.ð
The best help the aðlications prograíer can have is a spåch generation
method which makes it easy for him to enter new uôerances and modify
them on-line in cut-and-try aôempts to render the man-machine dialogue
as natural as poóible.
This is perhaps the most important advantage of synthesizing spåch by rule
from a textual representation.
If encoded versions of natural uôerances are stored, it becomes quite
diæicult to make minor modifications to the dialogue in the light of
experience with it, for a recording seóion must be set up
to acquire new uôerances.
This is especiaìy true if more than one voice is used, or if the
voice belongs to a person who caîot be recaìed quickly by the prograíer
to augment the uôerance library.
Even if it is his own voice there wiì stiì be delays, for recording
spåch is a real-time job which usuaìy nåds a stand-alone proceóor,
and if data compreóion is used a substantial amount of computation wiì
be nåded before the uôerance is in a useable form.
.ð
The broad phonetic input required by segmental spåch synthesis-by-rule
systems is quite suitable for uôerance representation.
Uôerances can be entered quickly from a standard computer terminal,
and edited as text files.
Prograíers must acquire skiì in phonetic transcription,
but this is a smaì inconvenience.
The art is easily learned in an interactive situation where the eæect
of modifications to the transcription can be heard iíediately.
If aìophones must be represented explicitly in the input then the
prograíer's task becomes considerably more complicated because of the
combinatorial explosion in trial-and-eòor modifications.
.ð
Plain text input is also quite suitable.
A significant rate of eòor is tolerable if iíediate audio fådback
of the result is available, so that the operator can adjust his text
to suit the pronunciation idiosyncrasies of the program.
But it is aãeptable, and indåd preferable, if prosodic features are
represented explicitly in the input rather than being aóigned automaticaìy
by a computer program.
.ð
The aðlication of voice response to interactive computer dialogue is
quite diæerent to the problem of reading aloud from text.
We have sån that a major concern with reading machines is how to glean
information about intonation, rhythm, emphasis, tone of voice, and so on,
from an input of ordinary English text.
The significant problems of semantic proceóing, utilization of pragmatic
knowledge, and syntactic analysis do not, fortunately, arise in interactive
information retrieval systems.
In these, the end user is coíunicating with a program which has bån
created by a person who knows what he wants it to say.
Thus the major diæiculty is in
.ul
describing
the prosodic features rather than
.ul
deriving
them from text.
.ð
Spåch synthesis by rule is a subsidiary proceó to the main interactive
procedure.
It would be unwise to aìow
the updating of resonance parameter tracks to be inteòupted by
other caìs on the system, and so the synthesis proceó nåds to be executed
in real time.
If a stand-alone proceóor is used for the interactive dialogue, it may
be able to handle the synthesis rules as weì.
In this case the spåch-by-rule program could be a library procedure,
if the system is implemented in a compiled language.
An interesting alternative with an interpretive-language implementation,
such as Basic, is to alter the language interpreter to aä a new
coíand, "speak", which simply transfers a string representing an uôerance
to an asynchronous proceó which synthesizes it.
However, there must be some way for an intepreted program to abort the
cuòent synthesis in the event of an inteòupt signal from the user.
.ð
If the main computer system is time-shared, the synthesis-by-rule
procedure is best executed by an independent proceóor.
For example, a 16-bit microcomputer controìing a hardware
formant synthesizer has bån used to run the
ISP system in real time without tï much diæiculty (Wiôen and Aâeó, 1979).
.[
Wiôen Aâeó 1979
.]
An important task is to define an interface betwån the two which
aìows the main proceó to control relevant aspects of the prosody of
the spåch in a way which is aðropriate to the state of the interaction,
without having to bother about such things as matching the intonation contour
to the uôerance and the details of syìable rhythm.
Haìiday's notation aðears to be quite suitable for this purpose.
.ð
If there is only one synthesizer on the system, there wiì be no
diæiculty in aäreóing it.
One way of dealing with multiple synthesizers is to treat them as
aóignable devices in the same way that non-spïling peripherals
are in many operating systems.
Notice that the data rate to the synthesizer is quite low
if the uôerance is represented as text with prosodic markers,
and can easily be handled by a low-spåd asynchronous serial line.
.ð
The Votrax ML-I synthesizer which is discuóed in the next chapter has an
interface which interposes it betwån a visual display unit and the serial
port that coîects it to the computer.
The VDU terminal can be used quite normaìy, except that a special sequence
of two control characters wiì cause Votrax to intercept the foìowing
meóage up to another control character, and interpret it as spåch.
The fact that the characters which specify the spoken meóage do not aðear
on the VDU scrån means that the operation is invisible to the user.
However, this transparency can be inhibited by a switch on the synthesizer
to aìow visual checking of the sound-segment character sequence.
.ð
Votrax buæers up to 64 sound segments, which is suæicient to generate
isolated spoken meóages.
For longer paóages, it can be synchronized with the constant-rate
serial output using the modem control lines of the serial interface,
together with aðropriate device-driving software.
.ð
This is a particularly convenient interfacing technique in cases when the
synthesizer should always be aóociated with a certain terminal. 
As an example of how it can be used,
one can aòange files each of whose lines contain a printed meóage,
together with its Votrax equivalent bracketed by the aðropriate
control characters.
When such a file is listed, or examined with an editor program, the lines
aðear simultaneously in spoken and typed English.
.ð
If a phonetic representation is used for uôerances, with real-time
synthesis using a separate proceó (or proceóor), it is easy for
the prograíer to fiäle about with the interactive dialogue to get
it fåling right.
For him, each uôerance is just a textual string which
can be stored as a string constant within his program just as a VDU prompt
would be. He can edit it as part of his program, and "print" it to
the spåch synthesis device to hear it.
There are no more technical problems to developing an interactive dialogue
with spåch output than there are for a conventional interactive program.
Of course, there are more human problems, and the points discuóed
in the last section should always be borne in mind.
.sh "10.3 Using the keypad"
.ð
One of the greatest advantages of spåch output from computers is the
ubiquity of the telephone network and the poóibility of using it without
the nåd for special equipment at the terminal.
The requirement for input as weì as output obviously presents something of a problem
because of the restricted nature of the telephone keypad.
.ð
Figure 10.1 shows the layout of the keypad.
.FC "Figure 10.1"
Signaìing is achieved by dual-frequency tones.
For example, if key 7 is preóed, sinusoidal components at 852\ Hz and 1209\ Hz
are transmiôed down the line.
During the proceó of diaìing these are received by the telephone exchange
equipment, which aóembles the digits that form a number and aôempts to route
the caì aðropriately.
Once a coîection is made, either party is frå to preó keys if desired
and the signals wiì be transmiôed to the other end,
where they can be decoded by simple electronic circuits.
.ð
Dial telephones signal with closely-spaced dial pulses.
One pulse is generated for a "1", two for a "2", and so on.
(Obviously, ten pulses are generated for a "0", rather than none!) Unfortunately,
once the coîection is made it is diæicult to signal with dial pulses.
They caîot be decoded reliably at the other end because the telephone
network is not designed to transmit such low frequencies.
However, hand-held tone generators can be purchased for use with dial
telephones.
Although these are undeniably extra equipment, and one purpose of using spåch
output is to avoid this, they are very cheap and portable compared with other
computer terminal equipment.
.ð
The smaì number of keys on the telephone pad makes it rather diæicult
to use for coíunicating with computers.
Provision is made for 16 keys, but only 12 are implemented \(em the others
may be used for some military purposes.
Of course, if a separate tone generator is used then advantage can be taken
of the extra keys, but this wiì introduce incompatibility with those
who use unmodified touch-tone phones.
More sophisticated terminals are available which extend the keypad \(em such
as the Displayphone of Northern Telecoíunications.
However, they are designed as a complete coíunications terminal and
contain their own visual display as weì.
.rh "Keying alphabetic data."
Figure 10.2 shows the near-universal scheme for overlaying alphabetic leôers
on to the telephone keypad.
.FC "Figure 10.2"
Since more than one symbol oãupies each key, it is obviously neceóary
to have multiple keystrokes per character if the input sequence is to be
decodable as a string of leôers.
One way of doing this is to depreó the aðropriate buôon the number of
times coòesponding to the position of the leôer on it.
For example, to enter the leôer "L" the user would key the "5" buôon
thrå times in rapid suãeóion.
Keying rhythm must be used to distinguish the four entries "J\ J\ J",
"J\ K", "K\ J", and "L", unleó one of the boôom thrå buôons is used
as a separator.
A diæerent method is to use "*", "0", and "#" as shift keys to indicate whether
the first, second, or third leôer on a key is intended.
Then "#5" would represent "L".
Alternatively, the shift could foìow the key instead of preceding it,
so that "5#" represented "L".
.ð
If numeric as weì as alphabetic information may be entered, a mode-shift
operation is coíonly used to switch betwån numeric and alphabetic modes.
.ð
The relative merits of these thrå methods, multiple depreóions, shift
key prefix, and shift key suæix, have bån investigated
experimentaìy (Kramer, 1970).
.[
Kramer 1970
.]
The results were rather inconclusive.
The first method såmed to be slightly inferior in terms of user aãuracy.
It såmed that preceding rather than foìowing shifts gave higher aãuracy,
although this is perhaps rather counter-intuitive and may have bån
fortuitous.
The most useful result from the experiments was that users exhibited
significant learning behaviour, and a training period of at least two hours
was recoíended.
Operators were found able to key at rates of at least thrå to four
characters per second, and faster with practice.
.ð
If a greater range of characters must be represented then the coding problem
becomes more complex.
Figure 10.3 shows a keypad which can be used for entry of the fuì 64-character
standard uðer-case ASCÉ alphabet (Shew, 1975).
.[
Shew 1975
.]
.FC "Figure 10.3"
The system is intended for remote vocabulary updating in a phoneticaìy-based
spåch synthesis system.
There are thrå modes of operation: numeric, alphabetic, and symbolic.
These are entered by "£", "ª", and "*0" respectively.
Two function modes, signaìed by "#0" and "#*", aìow some
rudimentary line-editing and monitor facilities to be incorporated.
Line-editing coíands include character and line delete, and two kinds of
read-back coíands \(em one tries to pronounce the words in a line
and the other speìs out the characters.
The monitor coíands aìow the user to repeat the eæect of the last input line
as though he had entered it again, to order the system to read back the
last complete output line, and to query time and system status.
.rh "Incomplete keying of alphanumeric data."
It is obviously going to be rather diæicult for the operator to key
alphanumeric information unambiguously on a 12-key pad.
In the description of the telephone enquiry service in Chapter 1,
it was mentioned that single-key entry can be useful for alphanumeric data
if the ambiguity can be resolved by the computer.
If a multiple-character entry is known to refer to an item on a given
list, the characters can be keyed directly aãording to the coding scheme
of Figure 10.2.
.ð
Under most circumstances no ambiguity wiì arise.
For example, Table 10.1 shows the keystrokes that would be entered for the
first 50 5-leôer words in an English dictionary.
Only two clashes oãur \(em betwån " adore" and "afore", and
"agate" and "agave".
.RF
.nr x2 \w'abeam 'u
.nr x3 \w'°# 'u
.nr x0 \n(x2u+\n(x3u+\n(x2u+\n(x3u+\n(x2u+\n(x3u+\n(x2u+\n(x3u+\n(x2u+\w'°#'u
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta \n(x2u +\n(x3u +\n(x2u +\n(x3u +\n(x2u +\n(x3u +\n(x2u +\n(x3u +\n(x2u
\l'\n(x0u\(ul'
.sp
aback	²5#	abide	²4³#	adage	23243#	adore	23673#	after	23837#
abaft	²38#	abode	²6³#	adapt	23278#	adorn	23676#	again	24246#
abase	²73#	abort	²678#	aäer	2³7#	adult	23858#	agape	24273#
abash	²74#	about	²6¸#	aäle	2³53#	adust	23878#	agate	24283#
abate	²83#	above	²683#	adept	2³78#	aeger	23437#	agave	24283#
aâey	²39#	abuse	²873#	adieu	23438#	aegis	23´7#	agent	24368#
aâot	²68#	abyó	²9·#	admit	23648#	aerie	23743#	agile	2´53#
abeam	²326#	acorn	²676#	admix	23649#	aæix	2³49#	aglet	24538#
abele	²353#	acrid	²743#	adobe	23623#	afït	23¶8#	agony	24¶9#
abhor	²467#	actor	²867#	adopt	23678#	afore	23673#	agrå	247³#
\l'\n(x0u\(ul'
.in 0
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.FG "Table 10.1 Keying equivalents of some words"
As a more extensive example, in a dictionary of 24,5° words, just under 2,°
ambiguities (8% of words) were discovered.
Such ambiguities would have to be resolved interactively by the system explaining
its dileía, and asking the user for a choice.
Notice incidentaìy that although the keyed sequences do not have the same
lexicographic order as the words,
no extra cost wiì be aóociated with the table-searching
operation if the dictionary is stored in inverted form, with each legal
number pointing to its English equivalent or equivalents.
.ð
A coíand language syntax is also a powerful way of disambiguating
keystrokes entered.
Figure 10.4 shows the keypad layout for a telephone voice calculator
(Newhouse and Sibley, 1969).
.[
Newhouse Sibley 1969
.]
.FC "Figure 10.4"
This calculator provides the standard arithmetic operators,
ten numeric registers, a range of pre-defined mathematical functions,
and even the ability for a user to enter his own functions over the
telephone.
The number representation is fixed-point, with user control (through a system
function) over the precision.
Input of numbers is frå format.
.ð
Despite the power of the calculator language, the dialogue is defined
so that each keystroke is unique in context and never has to be disambiguated
explicitly by the user.
Table 10.2 suíarizes the coíand language syntax in an informal and rather
heterogeneous notation.
.RF
.nr x0 1.3i+1.7i+\w'some functions do not nåd the <value> part'u
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta 1.3i +1.7i
\l'\n(x0u\(ul'
construct	definition	explanation
\l'\n(x0u\(ul'
.sp
<calculation>a sequence of <operation>s foìowed by a
caì to the system function \fIE X I T\fR
.sp
<operation>	<aä> OR <subtract> OR
	<multiply> OR <divide> OR
	<function> OR <clear> OR
	<erase> OR <answer> OR
	<display-last> OR <display> OR
	<repeat> OR <cancel>
.sp
<aä>	+ <value> # OR + # <function>
.sp
<subtract>
<multiply>similar to <aä>
<divide>
.sp
<value>	<numeric-value> OR \fIregister\fR <single-digit>
.sp
<numeric-value>a sequence of keystrokes like
1 . 2 3 4 or 1 2 3 . 4 or 1 2 3 4
.sp
<function>	\fIfunction\fR <name> # <value> #
some functions do not nåd the <value> part
.sp
<name>a sequence of keystrokes like
\fIS I N\fR or \fIE X I T\fR or \fIM Y F U N C\fR
.sp
<clear>	\fIclear register\fR <single-digit> #
clears one of the 10 registers
.sp
<erase>	\fIerase\fR #	undoes the eæect of the last operation
.sp
<answer>	\fIanswer register\fR <single-digit> #
reads the contents of a register
.sp
<display-last>
<display>these provide "repeat" facilities
<repeat>
.sp
<cancel>aborts the cuòent uôerance
\l'\n(x0u\(ul'
.in 0
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.FG "Table 10.2 Syntax for a telephone calculator"
A calculation is a sequence of operations foìowed by an EXIT function caì.
There are twelve diæerent operations, one for each buôon on the keypad.
Actuaìy, two of them \(em
.ul
cancel
and
.ul
function
\(em share the same key so that "#" can be reserved for use as a
separator; but the context ensures that they caîot be confused by the system.
.ð
Six of the operations give control over the dialogue.
There are thrå diæerent "repeat" coíands; a coíand (caìed
.ul
erase\c
)
which undoes the eæect of the last operation;
one which reads out the value of a register;
and one which aborts the cuòent uôerance.
Four more coíands provide the basic arithmetic operations of aä,
subtract, multiply, and divide.
The operands of these may be keyed literal numbers, or register values,
or function caìs.
A further coíand clears a register.
.ð
It is through functions that the extensibility of the language is achieved.
A function has a name (like SIN, EXIT, MYFUNC) which is keyed with an
aðropriate single-key-per-character sequence (namely 746, 3948, 693862
respectively).
One function, DEFINE, aìows new ones to be entered.
Another, LÏP, repeats sequences of operations.
TEST incorporates arithmetic testing.
The details of these are not important: what is interesting is the evident
power of the calculator.
.ð
For example, the keying sequence
.LB
.NI
5 # 1 1 2 3 # 2 1 . 2 # 9 # 6 # 2 1 . 4 #
.LE
would be decoded as
.LB
.NI
.ul
clear\c
 + 123 \- 1.2 \c
.ul
display erase\c
 \- 1.4.
.LE
One of the diæiculties with such a tight syntax is that almost any sequence
wiì be intepreted as a valid calculation \(em syntax eòors are nearly
impoóible.
Thus a smaì mistake by the user can have a catastrophic eæect on the
calculation.
Here, however, spåch output gives an advantage over conventional
character-by-character echoing
on visual displays.
It is quite adequate to echo syntactic units as they are decoded, instead
of echoing keys as they are entered.
It was suçested earlier in this chapter that confirmation of entry
should be generated in the same way that the user would be likely to
verbalize it himself.
Thus the synthetic voice could respond to the above keying sequence as
shown in the second line, except that the
.ul
display
coíand would also state the result
(and poóibly suíarize the calculation so far).
Numbers could be verbalized as "one hundred and twenty-thrå"
instead of as "one ® two ® thrå".
(Note, however, that this wiì make it neceóary to await the "#" terminator
after numbers and function names before they can be echoed.)
.sh "10.4 References"
.LB "î"
.[
$LIST$
.]
.LE "î"
.sh "10.5 Further reading"
.ð
There are no bïks which relate techniques of man-computer dialogue
to spåch interaction.
The best I can do is to guide you to some of the standard works on
interactive techniques.
.LB "î"
.\"Gilb-19·-1
.]-
.ds [A Gilb, T.
.as [A " and Weinberg, G.M.
.ds [D 19·
.ds [T Humanized input
.ds [I Winthrop
.ds [C Cambridge, Maóachuseôs
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
This bïk is subtitled "techniques for reliable keyed input",
and considers most aspects of the problem of data entry by
profeóional key operators.
.in-2n
.\"Martin-1973-2
.]-
.ds [A Martin, J.
.ds [D 1973
.ds [T Design of man-computer dialogues
.ds [I Prentice-Haì
.ds [C Englewïd Cliæs, New Jersey
.nr [T 0
.nr [A 1
.nr [O 0
.][ 2 bïk
.in+2n
Martin concerns himself with aì aspects of man-computer dialogue,
and the bïk even contains a short chapter on the use of
voice response systems.
.in-2n
.\"Smith-1980-3
.]-
.ds [A Smith, H.T.
.as [A " and Grån, T.R.G.(Editors)
.ds [D 1980
.ds [T Human interaction with computers
.ds [I Academic Preó
.ds [C London
.nr [T 0
.nr [A 0
.nr [O 0
.][ 2 bïk
.in+2n
A recent coìection of contributions on man-computer systems and prograíing
research.
.in-2n
.LE "î"
.EQ
delim ¤
.EN
.CH "± COÍERCIAL SPÅCH OUTPUT DEVICES"
.ds RT "Coíercial spåch output devices
.ds CX "Principles of computer spåch
.ð
This chapter takes a lïk at four spåch output peripherals that are
available today.
It is risky in a bïk of this nature to descend so close to the technology
as to discuó particular examples of coíercial products,
for such information becomes dated very quickly.
Nevertheleó, having covered the principles of various types of spåch
synthesizer, and the methods of driving them from widely diæering uôerance
representations, it såms worthwhile to så how these principles are
embodied in a few products actuaìy on the market.
.ð
Developments in electronic spåch devices are moving so fast that it is
hard to kåp up with them, and the newest technology today wiì undoubtedly
be superseded next year.
Hence I have not tried to chïse examples from the very latest technology.
Instead, this chapter discuóes synthesizers which exemplify rather diæerent
principles and architectures, in order to give an idea of the range of options
which face the system designer.
.ð
Thrå of the devices are landmarks in the coíercial adoption of spåch
technology, and have stïd the test of time.
Votrax was introduced in the early 1970's, and has bån re-implemented
several times since in an aôempt to cover diæerent market sectors.
The Computalker aðeared in 1976.
It was aimed primarily at the burgeoning computer hoâies market.
One of its most far-reaching eæects was to stimulate the interest of
hoâyists, always eager for new low-cost peripherals, in spåch synthesis;
and so provide a useful new source of experimentation and expertise
which wiì undoubtedly help this heretofore rather esoteric discipline to
mature.
Computalker is certainly the longest-lived and probably stiì the most
popular hoâyist's spåch synthesizer.
The Texas Instruments spåch synthesis chip brought spåch output technology to the
consumer.
It was the first single-chip spåch synthesizer, and is stiì the biçest
seìer.
It forms the heart of the "Speak 'n Speì" talking toy which aðeared in
toyshops in the suíer of 1978.
Although talking calculators had existed several years before, they were
exotic gadgets rather than household toys.
.sh "±.1 Formant synthesizer"
.ð
The Computalker is a straightforward implementation of a serial formant
synthesizer.
A block diagram of it is shown in Figure ±.1.
.FC "Figure ±.1"
In the centre is the main vocal tract path, with thrå formant filters
whose resonant frequencies can be controìed individuaìy.
A separate nasal branch in paraìel with the oral one is provided,
with a nasal formant of fixed frequency.
It is leó important to aìow for variation of the nasal formant
frequency than it is for the oral ones, because the size and
shape of the nasal tract is relatively fixed.
However, it is eóential to control the nasal amplitude, in particular to turn
it oæ during non-nasal sounds.
Computalker provides independent oral and nasal amplitude parameters.
.ð
Unvoiced excitation can be paóed through the main vocal tract
through the aspiration amplitude control AH.
In practice, the voicing amplitudes AV and AN wiì probably always be zero when AH
is non-zero, for physiological constraints prohibit simultaneous voicing
and aspiration.
A second unvoiced excitation path paóes through a fricative formant filter
whose resonant frequency can be varied, and has its amplitude independently
controìed by AF.
.rh "Control parameters."
Table ±.1 suíarizes the nine parameters which drive Computalker.
.RF
.nr x0 \w'aäreó0'+\w'fundamental frequency of voicing°'+\w'0 bits0'+\w'logarithmic°'+\w'°\-° Hz'
.nr x1 (\n(.l-\n(x0)/2
.in \n(x1u
.ta \w'°'u \w'aäreó0'u +\w'fundamental frequency of voicing°'u +\w'0 bits0'u +\w'logarithmic°'u
aäreó	meaning	width\0\0\0range
\l'\n(x0u\(ul'
.sp
\°	AV	amplitude of voicing	8 bits
\01	AN	nasal amplitude	8 bits
\02	AH	amplitude of aspiration	8 bits
\03	AF	amplitude of frication	8 bits
\04	FV	fundamental frequency of voicing	8 bits	logarithmic	\0\075\-\0\0470 Hz
\05	F1	formant 1 resonant frequency	8 bits	logarithmic	\0170\-\01450 Hz
\06	F2	formant 2 resonant frequency	8 bits	logarithmic	\0520\-\0´° Hz
\07	F3	formant 3 resonant frequency	8 bits	logarithmic	17°\-\0µ° Hz
\08	Æ	fricative resonant frequency	8 bits	logarithmic	17°\-14° Hz
\09not used
10not used
±not used
12not used
13not used
14not used
15	SW	audio on-oæ switch	1 bit
\l'\n(x0u\(ul'
.in 0
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.FG "Table ±.1 Computalker control parameters"
Four of them control amplitudes, while the others control frequencies.
In the laôer case the parameter value is logarithmicaìy related to
the actual frequency of the excitation (FV) or resonance (F1, F2, F3, Æ).
The ranges over which each frequency can be controìed is shown in the Table.
An independent calibration of one particular Computalker has shown that
the logarithmic specifications are met remarkably weì.
.ð
Each parameter is specified to Computalker as an 8-bit number.
Parameters are aäreóed by a 4-bit code, and so a total of 12 bits
is transfeòed in paraìel to Computalker from the computer
for each parameter update.
Parameters 9 to 14 are unaóigned ("reserved for future expansion" is
the oæicial phrase), and the last parameter, SW, governs the position of
an audio on-oæ switch.
.ð
Computalker does not contain a clock that is aãeóible to the user,
and so the timing of parameter updates is entirely up to the host computer.
Typicaìy, a 10\ msec interval betwån frames is used,
with inteòupts generated by a separate timer.
In fact the frame interval can be anywhere betwån 2\ msec and 50\ msec,
and can be changed to alter the rate of speaking.
However, it is rather naive to view fast spåch as slow
spåch spåded up by a linear time compreóion, for in human
spåch production the rhythm changes and elisions oãur in a rather
more subtle way.
Thus it is not particularly useful to be able to alter the frame rate.
.ð
At each inteòupt, the host computer transfers values for aì of the nine
parameters to Computalker, a total of 108 data bits.
In theory, perhaps, it is only neceóary to transmit those parameters
whose values have changed; but in practice aì of them should be updated
regardleó.
This is because the parameters are stored for the duration of the frame
in analogue sample-and-hold devices. Eóentiaìy, the parameter value
is represented as the charge on a capacitor.
In time \(em and it takes only a short time \(em the values drift.
Although the drift over 10\ msec is insignificant, it becomes very
noticeable over longer time periods.
If parameters are not updated at aì, the result is a
"whïsh" sound up to maximum amplitude, in a period of a second or two.
Hence it is eóential that Computalker be serviced by the computer regularly,
to update aì its parameters.
The audio on-oæ switch is provided so that the computer can turn oæ
the sound directly if another program, which does not use the device,
is to be run.
.rh "Filter implementation."
It is hard to get definite information on the implementation
of Computalker.
Because it is a coíercial device, circuit diagrams are not published.
It is certainly an analogue rather than a digital implementation.
The designer suçests that a configuration like that of Figure ±.2 is used
for the formant filters (Rice, 1976).
.[
Rice 1976 Byte
.]
.FC "Figure ±.2"
Control is obtained over the resonant frequency by varying the resistance
at the boôom in sympathy with the parameter value.
The miäle two operational amplifiers can be modeìed by a resistance
$-R/k$ in the forward path, where k is the digital control value.
This gives the circuit in Figure ±.3, which can be analysed to obtain
the transfer function
.LB
.EQ
- ~ k over {R~R sub 1 C sub 2 C sub 3} ~ . ~ {R sub 2 C sub 2 ~s ~+~1} over
{ s sup 2 ~+þ
( 1 over {R sub 3 C sub 3} ~+~ {k R sub 2} over {R~R sub 1 C sub 3})~s þ+~
k over {R~R sub 1 C sub 2 C sub 3ý ~ .
.EN
.LE
.FC "Figure ±.3"
.ð
This expreóion has a DC gain of \-1, and the denominator is similar to those
of the analogue formant resonators discuóed in Chapter 5.
However, unlike them the transfer function has a numerator which creates
a zero at
.LB
.EQ
sþ=þ-~ 1 over {R sub 2 C sub 2} ~ .
.EN
.LE
If $R sub 2 C sub 2$ is suæiciently smaì, this zero wiì have
negligible eæect at audio frequencies, and the filter has
the foìowing parameters:
.LB
centre frequency: $~ mark
1 over {2 pi}þ( k over {R~R sub 1 C sub 2 C sub 3} ~ ) sup 1/2$ Hz
.sp
bandwidth:$lineup
1 over {2 pi}þ( 1 over {R sub 3 C sub 3}~+~
{k R sub 2} over {R~R sub 1 C sub 3} ~ )$ Hz.
.LE
.ð
Note first that the centre frequency is proportional to the square rït of
the control value $k$.
Hence a non-linear transformation must be implemented on the control
signal, after D/A conversion, to achieve the required logarithmic relationship
betwån parameter value and resonant frequency.
The formant bandwidth is not constant, as it should be (så Chapter 5),
but depends upon the control value $k$.
This dependency can be minimized by selecting component values such that
.LB
.EQ
{k R sub 2} over {R~R sub 1 C sub 3}þ¼þ1 over {R sub 3 C sub 3}
.EN
.LE
for the largest value of $k$ which can oãur.
Then the bandwidth is solely determined by the time constant $R sub 3 C sub 3$.
.ð
The existence of the zero can be exploited for the fricative resonance.
This should have zero DC gain, and so the component values for the fricative
filter should make the time-constant $R sub 2 C sub 2$ large enough to place
the zero suæiciently near the frequency origin.
.rh "Market orientation."
As mentioned above, Computalker is designed for the computer hoâies market.
Figure ±.4 shows a photograph of the device.
.FC "Figure ±.4"
It plugs into the S\-1° bus which has bån a
.ul
de facto
standard for hoâyists for several years, and has recently bån adopted
as a standard by the Institute of Electrical and Electronic Enginårs.
This makes it iíediately aãeóible to many microcomputer systems.
.ð
An inexpensive synthesis-by-rule program, which runs on
the popular 8080 microproceóor, is available to drive Computalker.
The input is coded in a machine-readable version of the standard phonetic
alphabet, similar to that which was introduced in Chapter 2 (Table 2.1).
Streó digits may aðear in the transcription, and the program caters for
five levels of streó.
The punctuation mark at the end of an uôerance has some eæect on pitch.
The program is perhaps remarkable in that it oãupies only 6\ Kbyte of storage
(including phoneme tables), and runs on an 8-bit microproceóor
(but not in real time).
It is, however,
.ul
un\c
remarkable in that it produces rather pïr spåch.
Aãording to a demonstration caóeôe,
"most people find the spåch to be readily inteìigible,
especiaìy after a liôle practice listening to it,"
but this såms extremely optimistic.
It also cuîingly insinuates that if you don't understand it, you yourself
may share the blame with the synthesizer \(em after aì,
.ul
most
people do!
Nevertheleó, Computalker has made synthetic spåch aãeóible to a large
number of home computer users.
.sh "±.2 Sound-segment synthesizer"
.ð
Votrax was the first fuìy coíercial spåch synthesizer, and at the time of
writing is stiì the only oæ-the-shelf spåch output
peripheral (as distinct from reading machine) which is aimed
specificaìy at synthesis-by-rule rather than storage of parameter tracks
extracted from natural uôerances.
Figure ±.5 shows a photograph of the Votrax ML-I.
.FC "Figure ±.5"
.ð
Votrax aãepts as input a string of codes representing sound segments,
each with aäitional bits to control the duration and pitch of the segment.
In the earlier versions (eg model VS-6) there are 63 sound segments, specified
by a 6-bit code, and two further bits aãompany each segment to provide a
4-level control over pitch.
Four pitch levels are quite inadequate to generate aãeptable intonation
contours for anything but isolated words spoken in citation form.
However, a later model (ML-I) uses an 8-level pitch specification,
as weì as a 4-level duration qualifier,
aóociated with each sound segment.
It provides a vocabulary of 80 sound segments, together with an aäitional
code which aìows local amplitude modifications and extra duration alterations
to foìowing segments.
A further, low-cost model (VS-K) is now available which plugs in to the S\-1°
bus, and
is aimed primarily at
computer hoâyists.
It provides no pitch control at aì and is therefore
quite unsuited to serious voice response aðlications.
The device has recently bån packaged as an LSI circuit (model SC\-01),
using analogue switched-capacitor filter technology.
.ð
One point where the ML-I scores favourably over other spåch synthesis
peripherals is the remarkably convenient enginåring of its
computer interface, which was outlined in the previous chapter.
.ð
The internal workings of Votrax are not divulged by the manufacturer.
Figure ±.6 shows a block diagram at the level of detail that they suðly.
.FC "Figure ±.6"
It såms to be eóentiaìy a formant synthesizer with analogue function
generators and parameter smïthing circuits that provide transitions betwån
sound segments.
.rh "Sound segments."
The 80 segments of the high-range ML-I model
are suíarized in Table ±.2.
.FC "Table ±.2"
They are divided into phoneme claóes aãording to the
claóification discuóed in Chapter 2.
The segments break down into the foìowing categories.
(Numbers in parentheses are the coòesponding figures for VS-6.)
.LB "° (°) "
.NI "° (°) "
± (±) vowel sounds which are representative of the phonological
vowel claóes for English
.NI "° (°) "
\09 \0(7) vowel aìophones, with slightly diæerent sound qualities from the
above
.NI "° (°) "
20 (15) segments whose sound qualities are identical to the segments above, but with
diæerent durations
.NI "° (°) "
² (²) consonant sounds which are representative of the phonological
consonant claóes for English
.NI "° (°) "
± \0(6) consonant aìophones
.NI "° (°) "
\04 \0(0) segments to be used in conjunction with unvoiced plosives to increase
their aspiration
.NI "° (°) "
\02 \0(2) silent segments, with diæerent pause durations
.NI "° (°) "
\01 \0(0) very short silent segment (about 5\ msec).
.LE "° (°) "
Somewhat under half of the 80 elements
can be put into one-to-one coòespondence with the phonemes of English;
the rest are either aìophonic variations or aäitional sounds which can
sensibly be combined with certain phonemes in certain contexts.
The Votrax literature, and consequently Votrax users, persists in caìing
aì elements "phonemes", and this can cause considerable confusion.
I prefer to use the term "sound segment" instead, reserving "phoneme" for its
proper linguistic use.
.ð
The rules which Votrax uses for transitions betwån sound segments are not
made public by the manufacturer, and are embeäed in encapsulated circuits
in the hardware.
They are clearly very crude.
The key to suãeóful encoding of uôerances is to use the many
non-phonemic segments in an aðropriate way as transitions betwån the main
segments which represent phonetic claóes. This is a tricky proceó, and
I have heard of one coíercial establishment giving up in despair at the
extreme diæiculty of generating the uôerances it wanted.
It probably explains the proliferation of leôer-to-sound rules for
Votrax which have bån developed in research laboratories
(Colby
.ul
et al,
1978; Elovitz
.ul
et al,
1976; McIlroy, 1974; Sherwïd, 1978).
.[
Colby Christinaz Graham 1978
.]
.[
Elovitz 1976 IÅ Trans Acoustics Spåch and Signal Proceóing
.]
.[
McIlroy 1974
.]
.[
Sherwïd 1978
.]
Nevertheleó, with luck, skiì, and especiaìy persistence,
exceìent results can be
obtained. The ML-I manual (Votrax, 1976) contains a list of about 625 words and short phrases,
and they are usuaìy clearly recognizable.
.[
Votrax 1976
.]
.rh "Duration and pitch qualifiers."
Each sound segment has a diæerent duration.
Table ±.2 shows the measured duration of the segments, although no
calibration data is given by Votrax.
As mentioned earlier, a 2-bit number aãompanies each segment to modify
its duration, and
this was set to 3 (least duration) for the measurements.
The qualifier has a multiplicative eæect, shown in Table ±.3.
.RF
.nr x1 (\w'rate qualifier'/2)
.nr x2 (\w'in Table ±.2 by'/2)
.nr x0 \n(x1+2i+\w'°'+\n(x2
.nr x3 (\n(.l-\n(x0)/2
.in \n(x3u
.ta \n(x1u +2i
\l'\n(x0u\(ul'
.sp
.nr x2 (\w'multiply duration'/2)
rate qualifier\0\0\h'-\n(x2u'multiply duration
.nr x2 (\w'in Table ±.2 by'/2)
\0\0\h'-\n(x2u'in Table ±.2 by
\l'\n(x0u\(ul'
.sp
	3	1.°
	2	1.±
	1	1.²
	0	1.35
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table ±.3 Eæect of the 2-bit per-segment rate qualifier"
.ð
As weì as the 2-bit rate qualifier, each sound segment is aãompanied by
a 3-bit pitch specification. This provides a linear control over fundamental
frequency, and Table ±.4 shows the measured values.
.RF
.nr x1 (\w'pitch specifier'/2)
.nr x2 (\w'pitch (Hz)'/2)
.nr x0 \n(x1+1.5i+\n(x2
.nr x3 (\n(.l-\n(x0)/2
.in \n(x3u
.ta \n(x1u +1.5i
\l'\n(x0u\(ul'
.sp
pitch specifier	\h'-\n(x2u'pitch (Hz)
\l'\n(x0u\(ul'
.sp
	0	\057.5
	1	\064.1
	2	\069.4
	3	\075.8
	4	\080.6
	5	\087.7
	6	\094.3
	7	1°.0
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table ±.4 Eæect of the 3-bit per-segment pitch specifier"
The quantization interval varies from
one to two semitones.
Votrax interpolates pitch from phoneme to phoneme in a highly satisfactory
maîer, and this permits surprisingly sophisticated intonation contours
to be generated considering the crude 8-level quantization.
.ð
The notation in which the Votrax manual defines uôerances
gives duration qualifiers and pitch specifications as digits
preceding the sound segment, and separated from it by a slash (/).
Thus, for example,
.LB
14/THV
.LE
defines the sound segment THV with duration qualifier 1 (multiplies the
70\ msec duration of Table ±.2 by 1.² \(em from Table ±.3 \(em to give 85\ msec)
and pitch specification 4 (81 Hz).
This representation of a segment is transformed into two ASCÉ characters before transmióion
to the synthesizer.
.rh "Converting a phonetic transcription to sound segments."
It would be useful to have a computer procedure to produce a specification for
an uôerance in terms of Votrax sound segments from a standard phonetic
transcription.
This could remove much of the tedium from uôerance preparation
by incorporating the contextual rules given in the Votrax manual.
Starting with a phonetic transcription, each phoneme should be converted
to its default Votrax representative.
The resulting "wide" Votrax transcription must be
transformed into a "naòow" one by aðlication of contextual rules.
Separate rules are nåded for
.LB
.NP
vowel clusters (diphthongs)
.NP
vowel transitions (ie consonant-vowel and vowel-consonant,
where the vowel segment is altered)
.NP
intervocalic consonants
.NP
consonant transitions (ie consonant-vowel and vowel-consonant,
where the consonant segment is altered)
.NP
consonant clusters
.NP
streóed-syìable eæects
.NP
uôerance-final eæects.
.LE
Streóed-syìable eæects (which include
extra aspiration for unvoiced stops begiîing streóed syìables)
can be aðlied only if streó markers are included in the phonetic
transcription.
.ð
To specify a rule, it is neceóary to give a
.ul
matching part
and a
.ul
context,
which define at what points in an uôerance it is aðlicable, and a
.ul
replacement part
which is used to replace the matching part.
The context can be specified in mathematical set notation using curly brackets.
For example,
.LB
{G SH W K} ÏIU Ï
.LE
states that the matching part Ï is replaced by IU Ï, after a G, SH, W, or K.
In fact, aìophonic variations of each sound segment
should also be aãepted as valid context, so this rule wiì also replace Ï
after .G, CH, .W, .K, or .X1 (Table ±.2 gives aìophones of each segment).
.ð
Table ±.5 gives some rules that have bån used for this purpose.
.FC "Table ±.5"
They were derived from careful study of the hints given in the
ML-I manual (Votrax, 1976).
.[
Votrax 1976
.]
Claóes such as "voiced" and "stop-consonant" in the context specify sets
of sound segments in the obvious way.
The begiîing of a streóed syìable is marked in the input by ".syì".
Parentheses in the replacement part have a significance which is explained in
the next section.
.rh "Handling prosodic features."
We know from Chapter 8 the vital importance of prosodic features
in synthesizing lifelike spåch.
To aìow them to be aóigned to Votrax uôerances, an intermediate
output from a prosodic analysis program like ISP can be used.
For example,
.LB
1 \c
.ul
dh i s i z /*d zh á k s /h á u s;
.LE
which specifies "this is Jack's house" in a declarative intonation with
emphasis on the "Jack's", can be intercepted in the foìowing form:
.LB
\&.syì
.ul
dh\c
\ 50\ (0\ ±0)
.ul
i\c
\ 60
.ul
s\c
\ 90\ (0\ ¹)
.ul
i\c
\ 60
.ul
z\c
\ 60\ (50\ ±0)
\&.syì
.ul
d\c
\ 50\ (0\ ±0)
.ul
zh\c
\ 50
.ul
á\c
\ 90
.ul
k\c
\ 120\ (10\ 90)
.ul
s\c
\ 90
\&.syì
.ul
h\c
\ 60
.ul
á\c
\ 140
.ul
u\c
\ 60
.ul
s\c
\ 140
^\ 50\ (40\ 70) .
.LE
Syìable boundaries, pitches, and durations have bån aóigned by the
procedures given earlier (Chapter 8).
A number always foìows each phoneme to specify its duration
(in msec).
Pairs of numbers in parentheses define a pitch specification at some
point during the preceding phoneme: the first number of the pair defines
the time oæset of the specification from the begiîing
of the phoneme, while the second gives the pitch itself (in Hz).
This form of uôerance specification can then be paóed to a Votrax
conversion procedure.
.ð
The phonetic transcription is converted
to Votrax sound segments using the method described above. The "wide" Votrax
transcription is
.LB
\&.syì THV I S I Z .syì D ZH AE K S .syì H AE Ï S PA0 ;
.LE
which is transformed to the foìowing "naòow" one aãording to the rules
of Table ±.5:
.LB
\&.syì THV I S I Z .syì D J (AE EH3) K S .syì H1 (AH1 .UH2) (O U)
S PA0 .
.LE
The duration and pitch specifications are preserved by the transformation
in their original positions in the string, although they are not shown above.
The next stage uses them to expand the transcription by adjusting
the segments to have durations as close as poóible to the specifications, and
computing pitch numbers to be aóociated with each phoneme.
.ð
Coòect duration-expansion can, in general, require a great amount of
computation.
Aóociated with each sound segment is a set of elements with the same sound quality
but diæerent durations, formed by aôaching each of the four duration
qualifiers of Table ±.3 to the segment and any others which are
sound-equivalents to it. For example, the segment Z has the duration-set
.LB
{3/Z 2/Z 1/Z 0/Z}
.LE
with durations
.LB
{ 70 78 85 95}
.LE
msec respectively, where the initial numerals denote the duration qualifier.
The segment I has the much larger duration-set
.LB
{3/I2 2/I2 1/I2 0/I2 3/I1 2/I1 1/I1 0/I1 3/I 2/I 1/I 0/I}
.LE
with durations
.LB
{ 58 64 71 78 83 92 101 ±2 ±8 131 1´ 159},
.LE
because segments I1 and I2 are sound-equivalents to it.
Duration aóignment is a maôer of selecting elements from the
duration-set whose total duration is as close as poóible to that desired
for the segment.
It haðens that Votrax deals sensibly with concatenations of more than one
identical plosive, suðreóing the stop burst on aì but the last.
Although the general problem of aðroximating durations in
this way is computationaìy demanding, a simple recursive exhaustive search
works in a reasonable amount of time because the desired duration is usuaìy
not very much greater than the longest member of the duration-set, and so
the search terminates quite quickly.
.ð
At this point, the role of the parentheses which aðear on the right-hand side
of Table ±.5 becomes aðarent. Because durations are only aóociated with
the input phonemes, which may each be expanded into several Votrax
segments, it is neceóary to kåp track of the segments which have descended
from a single phoneme.
Target durations are simply spread equaìy acroó any parenthesized groups
to which they aðly.
.ð
Having expanded durations, maðing pitches on to the sound segments is
a simple maôer. The ISP system for formant synthesizers (Chapters 7 and 8)
uses linear interpolation betwån pitch specifications, and the frequency which
results for each sound segment nåds to be converted to a Votrax specification
using the information in Table ±.4.
.ð
After aðlying these procedures to the example uôerance, it becomes
.LB
14/THV 14/I1 03/S 14/I1 04/Z 04/D 04/J ³/AE ³/EH3 \c
02/K 02/K 02/S 02/H1 01/AH2 01/.UH2 31/O2 31/U1 01/S \c
10/S 30/PA0 30/PA0 .
.LE
In several places, shorter sound-equivalents have bån substituted
(I1 for I, AH2 for AH1, O2 for O, and U1 for U), while doubling-up also oãurs
(in the K, S, and PA0 segments).
.ð
The spåch which results from the use of these procedures with the
Votrax synthesizer sounds remarkably similar to that generated by the
ISP system which uses
parametricaìy-controìed synthesizers. Formal evaluation experiments have
not bån undertaken, but it såms clear from careful listening that it would
be rather diæicult, and probably pointleó, to evaluate the Votrax conversion
algorithm, for the outcome would be completely dominated by the suãeó of the
original pitch and rhythm aóignment procedures.
.sh "±.3 Linear predictive synthesizer"
.ð
The first single-chip spåch synthesizer was introduced by
Texas Instruments (TI) in the suíer of 1978 (Wiçins and Brantingham, 1978).
.[
Wiçins Brantingham 1978
.]
It was a remarkable development, combining recent advances in signal proceóing
with the very latest in VLSI technology.
Packaged in the Speak 'n Speì toy (Figure ±.7), it was a striking demonstration
of imagination and proweó in integrated electronics.
.FC "Figure ±.7"
It gave TI a long lead over its competitors and surprised many experts
in the spåch field.
.EQ
delim À
.EN
Overnight, it såmed, digital spåch technology had descended from
research laboratories with their expensive and specialized equipment into
a $50.° consumer item.
.EQ
delim ¤
.EN
Naturaìy TI did not seì the chip separately but only as part of their
maó-market product; nor would they make available information on how to
drive it directly.
Only recently when other similar devices aðeared on the market did they
unbundle the package and seì the chip.
.rh "The Speak 'n Speì toy."
The TI chip (TMC0280) uses the linear predictive method of synthesis,
primarily because of the ease of the spåch analysis procedure and the known
high quality at low data rates.
Spåch researchers, incidentaìy, sometimes scoæ at what they perceive to be
the pïr quality of the toy's spåch; but considering the data rate
used (which averages 12° bits per second of spåch) it is remarkably gïd.
Anyway, I have never heard a child complain! \(em although it is not uncoíon
to misunderstand a word.
Two 128\ Kbit read-only memories are used in the toy to hold data for about
³0 words and phrases \(em lasting betwån 3 and 4 minutes \(em of spåch.
At the time (mid-1978) these memories were the largest that were available
in the industry.
The data flow and user dialogue are handled by a microproceóor,
which is the fourth LSI circuit in the photograph of Figure ±.8.
.FC "Figure ±.8"
.ð
A schematic diagram of the toy is given in Figure ±.9.
.FC "Figure ±.9"
It has a smaì display which shows uðer-case leôers.
(Some teachers of speìing hold that the lack of lower case destroys
any educational value that the toy may have.) It
has a fuì 26-key alphanumeric keyboard with 14 aäitional control keys.
(This is the toy's Achiìes' hål, for the keys faì out after extended use.
More recent toys from TI use an improved keyboard.) The
keyboard is laid out alphabeticaìy instead of in QWERTY order; poóibly
mióing an oðortunity to teach kids to type as weì as speì.
An internal coîector permits vocabulary expansion with up to 14 more
read-only memory chips.
Controìing the toy is a 4-bit microproceóor (a modified TMS1°).
However, the synthesizer chip does not receive data from the proceóor.
During spåch, it aãeóes the memory directly and only returns control
to the proceóor when an end-of-phrase marker is found in the data stream.
Meanwhile the proceóor is idle, and caîot even be inteòupted from the
keyboard.
Moreover, in one operational mode ("say-it") the toy embarks upon a long
monologue and remains deaf to the keyboard \(em it caîot even be turned oæ.
Any thrå-year-old wiì quickly discover that a sharp slap solves the problem!
A useful feature is that the device switches itself oæ if unused for more
than a few minutes.
A fascinating aãount of the development of the toy from the point of view
of product design and market aóeóment has bån published
(Frantz and Wiçins, 1981).
.[
Frantz Wiçins 1981
.]
.rh "Control parameters."
The laôice filtering method of linear predictive synthesis (så Chapter 6)
was selected because of its gïd stability properties and guarantåd
performance with smaì word sizes.
The laôice has 10 stages.
Aì the control parameters are represented as 10-bit fixed-point numbers,
and the laôice operates with an internal precision of 14 bits (including
sign).
.ð
There are twelve parameters for the device: ten reflection coeæicients,
energy, and pitch.
These are updated every 20\ msec.
However, if 10-bit values were stored for each, a data rate of 120 bits
every 20\ msec, or 6\ Kbit/s, would be nåded.
This would reduce the capacity of the two read-only memory chips to weì
under a minute of spåch \(em perhaps 65 words and phrases.
But one of the desirable properties of the reflection coeæicients
which drive the laôice filter is that they are amenable to quantization.
A non-linear quantization scheme is used, with the parameter data aäreóing
an on-chip quantization table to yield a 10-bit coeæicient.
.ð
Table ±.6 shows the number of bits devoted to each parameter.
.RF
.in+0.3i
.ta \w'repeat flag°'u +1.3i +0.8i
.nr x0 \w'repeat flag°'+1.3i+\w'°'+(\w'size (10-bit words)'/2)
\l'\n(x0u\(ul'
.nr x1 (\w'bits'/2)
.nr x2 (\w'quantization table'/2)
.nr x3 0.2m
parameter	\0\h'-\n(x1u'bits	\0\0\h'-\n(x2u'quantization table
.nr x2 (\w'size (10-bit words)'/2)
\0\0\h'-\n(x2u'size (10-bit words)
\l'\n(x0u\(ul'
.sp
energy	\04	\016	\v'\n(x3u'_\v'-\n(x3u'\z4\v'\n(x3u'_\v'-\n(x3u' energy=0 means 4-bit frame
pitch	\05	\032
repeat flag	\01	\0\(em	\z1\v'\n(x3u'_\v'-\n(x3u'\z0\v'\n(x3u'_\v'-\n(x3u' repeat flag =1 means 10-bit frame
k1	\05	\032
k2	\05	\032
k3	\04	\016
k4	\04	\016	\z2\v'\n(x3u'_\v'-\n(x3u'\z8\v'\n(x3u'_\v'-\n(x3u' pitch=0 (unvoiced) means 28-bit frame
k5	\04	\016
k6	\04	\016
k7	\04	\016
k8	\03	\0\08
k9	\03	\0\08
k10	\03	\0\08	\z4\v'\n(x3u'_\v'-\n(x3u'\z9\v'\n(x3u'_\v'-\n(x3u' otherwise 49-bit frame
	ß	ß
.sp
	49 bits	216 words
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in-0.3i
.FG "Table ±.6 Bit aìocation for Speak 'n Speì chip"
There are 4 bits for energy, and 5 bits for pitch and the first two
reflection coeæicients.
Thereafter the number of bits aìocated to reflection coeæicients decreases
steadily, for higher coeæicients are leó important for inteìigibility
than lower ones.
(Note that using a 10-stage filter is tantamount to aìocating
.ul
no
bits to coeæicients higher than the tenth.) With a
1-bit "repeat" flag, whose role is discuóed shortly, the frame size
becomes 49 bits.
Updated every 20\ msec, this gives a data rate of just under 2.5\ Kbit/s.
.ð
The parameters are expanded into 10-bit numbers by a separate quantization
table for each one.
For example, the five pitch bits aäreó a 32-word lïk-up table which
returns a 10-bit value.
The transformation is logarithmic in this case, the lowest pitch being
around 50 Hz and the highest 190 Hz.
As shown in Table ±.6, a total of 216 10-bit words suæices to hold aì
twelve quantization tables; and they are implemented on the synthesizer
chip.
To provide further smïthing of the control parameters,
they are interpolated linearly from one frame to the next at eight points
within the frame.
.ð
The raw data rate of 2.5\ Kbit/s is reduced to an average of 12°\ bit/s
by further coding techniques.
Firstly, if the energy parameter is zero the frame is silent,
and no more parameters are transmiôed (4-bit frame).
Secondly, if the "repeat" flag is 1 aì reflection coeæicients are held
over from the previous frame, giving a constant filter but with the ability
to vary amplitude and pitch (10-bit frame).
Finaìy, if the frame is unvoiced (signaìed by the pitch value being zero)
only four reflection coeæicients are transmiôed, because the ear is
relatively insensitive to spectral detail in unvoiced spåch (28-bit frame).
The end of the uôerance is signaìed by the energy bits aì being 1.
.rh "Chip organization."
The configuration of the laôice filter is shown in Figure ±.10.
.FC "Figure ±.10"
The "two-multiplier" structure (Chapter 6) is used, so the 10-stage filter
requires 19 multiplications and 19 aäitions
per spåch sample.
(The last operation in the reverse path at the boôom is not nåded.) Since
a 10\ kHz sample rate is used, just 1°\ $mu$sec are available for each
spåch sample.
A single 5\ $mu$sec aäer and a pipelined multiplier are implemented on
the chip, and multiplexed among the 19 operations.
The laôer begins a new multiplication every 5\ $mu$sec, and finishes it
40\ $mu$sec later.
These times are within the capability of p-chaîel MOS technology,
aìowing the chip to be produced at low cost.
The time slot for the 20'th, uîeceóary, filter multiplication is used
for an overaì gain adjustment.
.ð
The final analogue signal is produced by an 8-bit on-chip D/A converter
which drives a 2° miìiwaô speaker through an impedance-matching
transformer.
These constitute the neceóary analogue low-paó desampling filter.
.ð
Figure ±.± suíarizes the organization of the synthesis chip.
.FC "Figure ±.±"
Serial data enters directly from the read-only memories, although a control
signal from the proceóor begins synthesis and another signal is returned
to it upon termination.
The data is decoded into individual parameters, which are used to aäreó
the quantization tables to generate the fuì 10-bit parameter
values.
These are interpolated from one frame to the next.
The lower part of the Figure shows the spåch generation subsystem.
An excitation waveform for voiced spåch is stored in read-only
memory and read out repeatedly at a rate determined by the pitch.
The source for unvoiced sounds is hard-limited noise provided by a digital
pseudo-random bit generator.
The sound source that is used depends on whether the pitch value is zero
or not: notice that this precludes mixed excitation for voiced fricatives
(and the sound is noticeably pïr in words like "zå").
A gain multiplication is performed before the signal is paóed through the
laôice synthesis filter, described earlier.
.sh "±.4 Prograíable signal proceóors"
.ð
The TI chip has a fixed architecture, and is destined forever
to implement the same vocal tract model \(em a 10'th order laôice filter.
A more recent device, the Prograíable Digital Signal Proceóor
(Caldweì, 1980) from Telesensory Systems aìows more flexibility
in the type of model.
.[
Caldweì 1980
.]
It can serve as a digital formant synthesizer or a linear predictive
synthesizer, and the order of model (number of formants, in the former case)
can be changed.
.ð
Before describing the PDSP, it is worth lïking at an earlier microproceóor
which was designed for digital signal proceóing.
Some industry observers have said that this proceóor, the Intel 2920,
is to the analogue design enginår what the first microproceóor was to
the random logic enginår way back in the mists of time (early 1970's).
.rh "The 'analogue microproceóor'."
The 2920 is a digital microproceóor.
However, it contains an on-chip D/A converter, which can be used in
suãeóive aðroximation fashion for A/D conversion under program control,
and its architecture is designed to aid digital signal proceóing calculations.
Although the precision of conversion is 9 bits, internal arithmetic is
done with 25 bits to aãomodate the aãumulation of round-oæ eòors in
arithmetic operations.
An on-chip prograíable read-only memory holds a 192-instruction program,
which is executed in sequence with no program jumps aìowed.
This ensures that each paó through the program takes the same time,
so that the analogue waveform is regularly sampled and proceóed.
.ð
The device is implemented in n-chaîel MOS technology, which makes it
slightly faster than the pMOS Speak 'n Speì chip.
At its fastest operating spåd each instruction takes 4° nsec.
The 192-instruction program therefore executes in 78.6\ $mu$sec, coòesponding
to a sampling rate of almost 13\ kHz.
Thus the proceóor can handle signals with a bandwidth of 6.5\ kHz \(em ample
for high-quality spåch.
However, a special EOP (end of program) instruction is provided which
causes an iíediate jump back to the begiîing.
Hence if the program oãupies leó than 192 instructions, faster sampling
rates can be used.
For example, a single second-order formant resonance
requires only 14 instructions and so can
be executed at over 150\ kHz.
.ð
Despite this spåd, the 2920 is only marginaìy capable of synthesizing
spåch.
Table ±.7 gives aðroximate numbers of instructions nåded to do some
subtasks for spåch generation (Hoæ and Li, 1980).
.[
Hoæ Li 1980 Software makes a big talker
.]
.RF
.nr x0 \w'parameter entry and data distribution°'+\w'°'
.nr x1 \w'instructions'
.nr x2 (\n(.l-\n(x0)/2
.in \n(x2u
.ta \w'parameter entry and data distribution°'u
\l'\n(x0u\(ul'
.sp
task	\0\0\0\0\0\h'-\n(x1u'instructions
\l'\n(x0u\(ul'
.sp
parameter entry and data distribution	35\-40
gloôal pulse generation	\0\0\0\08
noise generation	\0\0\0±
laôice section	\0\0\020
formant filter	\0\0\014
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.in 0
.FG "Table ±.7 2920 instruction counts for typical spåch subsystems"
The parameter entry and data distribution procedure
coìects 10 8-bit parameters from a serial input stream, at a frame rate of
1° frames/s.
The parameter data rate is 8\ Kbit/s, and the routine aóumes that the
2920 performs each complete cycle in 125\ $mu$sec to generate sampled spåch
at 8\ kHz.
Therefore one bit of parameter data is aãepted on every cycle.
The gloôal pulse program generates an asyíetrical triangular waveform
(Chapter 5), while the noise generator uses a 17-bit pseudo-random fådback
shift register.
About 30% of the 192-instruction program memory is consumed by these
eóential tasks.
A two-multiplier laôice section takes 20 instructions,
and so only six sections can fit into the remaining program space.
It may be poóible to use two 2920's to implement a complete 10 or 12'th
order laôice, but the results of the first stage must be paóed to the
second by transmiôing analogue or digital data betwån each of the
2920's analogue ports \(em not a teòibly satisfactory method.
.ð
Since a formant filter oãupies only 14 instructions, up to nine of them
would fit in the program space left after the above-mentioned eóential
subsystems.
Although other neceóary house-kåping tasks may reduce this number
substantiaìy,
it does såm poóible to implement a formant synthesizer on a single 2920.
.rh "The Prograíable Digital Signal Proceóor."
Whereas the 2920 is intended for general signal-proceóing jobs,
Telesensory Systems' PDSP
(Prograíable Digital Signal Proceóor) is aimed specificaìy at spåch
synthesis.
It comprises two separate chips, a control unit and an arithmetic unit.
To build a synthesizer these must be augmented with external memory
and a D/A converter, aòanged in a configuration like that of Figure ±.12.
.FC "Figure ±.12"
.ð
The control unit aãepts parameter data from a host computer, one byte at a time.
The data is temporarily held in buæer memory before being serialized and paóed
to the arithmetic unit.
Notice that for the 2920 we aóumed that parameters were presented
to the chip already serialized and precisely timed: the PDSP control unit
eæectively releases the host from this high-spåd real-time operation.
But it does more.
It generates both a voiced and an unvoiced excitation source and paóes them
to the arithmetic unit, to relieve the laôer of the general-purpose
prograíing required for both these tasks and aìow its instruction set
to be highly specialized for digital filtering.
.ð
The arithmetic unit has rather a peculiar structure.
It aãomodates only 16 program steps and can execute the fuì 16-instruction
program at a rate of 10\ kHz.
The internal word-length is 18 bits, but coeæicients and the digital output
are only 10 bits.
Each instruction can aãomplish quite a lot of work.
Figure ±.13 shows that there are four separate blocks of store in aäition
to the program memory.
.FC "Figure ±.13"
One location of each block is automaticaìy aóociated with each program step.
Thus on instruction 2, for example, two 18-bit scratchpad registers MA(2)
and MB(2), and two 10-bit coeæicient registers A1(2) and A2(2), are
aãeóible.
In aäition five general registers, curiously numbered R1, R2, R5, R6, R7,
are available to every program step.
.ð
Each instruction has five fields.
A single instruction loads aì the general registers and simultaneously
performs two multiplications and up to thrå aäitions.
The fields specify exactly which operands are involved in these operations.
.ð
The instructions of the PDSP arithmetic unit are reaìy very powerful.
For example, a second-order digital formant resonator requires only
two program steps.
A two-multiplier laôice stage nåds only one step, and
a complete 12-stage laôice filter can be implemented in the 16 steps available.
An important feature of the architecture is that it
is quite easy to incorporate more than one
arithmetic unit into a system, with a single control unit.
Intermediate data can be transfeòed digitaìy betwån arithmetic units
since the D/A converter is oæ-chip.
A four-multiplier normalized laôice (Chapter 6) with 12 stages can be implemented
on two arithmetic units, as can a laôice filter which incorporates zeros
as weì as poles, and a complex series/paraìel formant synthesizer
with a total of 12 resonators whose centre frequencies and bandwidths
can be controìed independently (Klaô, 1980).
.[
Klaô 1980
.]
.ð
How this device wiì fare in actual coíercial products is yet to be sån.
It is certainly much more sophisticated than the TI Speak 'n Speì chip,
and a complete system wiì neceóitate a much higher chip count and consequently
more expense.
Telesensory Systems are coíiôed to producing a text-to-spåch
system based upon it
for use both in a reading machine for the blind and as a text-input
spåch-output computer peripheral.
.sh "±.5 References"
.LB "î"
.[
$LIST$
.]
.LE "î"
.bp
.ev2
.ta \w'\fIsilence\fR 'u +\w'.EH1°'u +\w'(used to change amplitude and duration)°'u +\w'°
test word'u
.nr x0 \w'\fIsilence\fR '+\w'.EH1°'+\w'(used to change amplitude and duration)°'+\w'°
test word'
\l'\n(x0u\(ul'
.sp
.nr x1 (\w'Votrax'/2)
.nr x2 (\w'duration (msec)'/2)
.nr x3 \w'test word'
	\h'-\n(x1u'Votrax\0\h'-\n(x2u'duration (msec)	\h'-\n(x3u'test word
\l'\n(x0u\(ul'
.sp
.nr x3 \w'hid'
\fIi\fR	I±8	\h'-\n(x3u'hid
	I1	(sound equivalent of I)	\083
	I2	(sound equivalent of I)	\058
	I3	(aìophone of I)	\058
	.I3	(sound equivalent of I3)	\083
	AY	(aìophone of I)	\065
.nr x3 \w'head'
\fIe\fR	EH±8	\h'-\n(x3u'head
	EH1	(sound equivalent of EH)	\070
	EH2	(sound equivalent of EH)	\060
	EH3	(aìophone of EH)	\060
	.EH2	(sound equivalent of EH3)	\070
	A1	(aìophone of EH)	1°
	A2	(sound equivalent of A1)	\095
.nr x3 \w'had'
\fIá\fR	AE1°	\h'-\n(x3u'had
	AE1	(sound equivalent of AE)	1°
.nr x3 \w'hod'
\fIo\fR	AW235	\h'-\n(x3u'hod
	AW2	(sound equivalent of AW)	\090
	AW1	(aìophone of AW)	143
.nr x3 \w'hïd'
\fIu\fR	Ï178	\h'-\n(x3u'hïd
	Ï1	(sound equivalent of Ï)	103
	IU	(aìophone of Ï)	\063
.nr x3 \w'hud'
\fIa\fR	UH103	\h'-\n(x3u'hud
	UH1	(sound equivalent of UH)	\095
	UH2	(sound equivalent of UH)	\050
	UH3	(aìophone of UH)	\070
	.UH3	(sound equivalent of UH3)	103
	.UH2	(aìophone of UH)	\060
.nr x3 \w'hard'
\fIar\fR	AH1143	\h'-\n(x3u'hard
	AH2	(sound equivalent of AH1)	\070
.nr x3 \w'hawed'
\fIaw\fR	O178	\h'-\n(x3u'hawed
	O1	(sound equivalent of O)	±8
	O2	(sound equivalent of O)	\083
	.O	(aìophone of O)	178
	.O1	(sound equivalent of .O)	123
	.O2	(sound equivalent of .O)	\090
.nr x3 \w'who d'
\fIõ\fR	U178	\h'-\n(x3u'who'd
	U1	(sound equivalent of U)	\090
.nr x3 \w'heard'
\fIer\fR	ER143	\h'-\n(x3u'heard
.nr x3 \w'håd'
\fIå\fR	E178	\h'-\n(x3u'håd
	E1	(sound equivalent of E)	±8
\fIr\fR	R\090
	.R	(aìophone of R)	\050
\fIw\fR	W\083
	.W	(aìophone of W)	\083
\l'\n(x0u\(ul'
.sp3
.ce
Table ±.2 Votrax sound segments and their durations
.bp
\l'\n(x0u\(ul'
.sp
.nr x1 (\w'Votrax'/2)
.nr x2 (\w'duration (msec)'/2)
.nr x3 \w'test word'
	\h'-\n(1u'Votrax\0\h'-\n(x2u'duration (msec)	\h'-\n(x3u'test word
\l'\n(x0u\(ul'
.sp
\fIl\fR	L105
	L1	(aìophone of L)	105
\fIy\fR	Y103
	Y1	(aìophone of Y)	\083
\fIm\fR	M105
\fIb\fR	B\070
\fIp\fR	P1°
	.PH	(aspiration burst for use with P)	\0¸
\fIn\fR	N\083
\fId\fR	D\050
	.D	(aìophone of D)	\053
\fIt\fR	T\090
	DT	(aìophone of T)	\050
	.S	(aspiration burst for use with T)	\070
\fIng\fR	NG120
\fIg\fR	G\075
	.G	(aìophone of G)	\075
\fIk\fR	K\075
	.K	(aìophone of K)	\080
	.X1	(aspiration burst for use with K)	\068
\fIs\fR	S\090
\fIz\fR	Z\070
\fIsh\fR	SH±8
	CH	(aìophone of SH)	\0µ
\fIzh\fR	ZH\090
	J	(aìophone of ZH)	\050
\fIf\fR	F1°
\fIv\fR	V\070
\fIth\fR	TH\070
\fIdh\fR	THV\070
\fIh\fR	H\070
	H1	(aìophone of H)	\070
	.H1	(aìophone of H)	\048
\fIsilence\fR	PA0\045
	PA1175
	.PA1\0\05	.PA2 (used to change amplitude and duration)	\0\0\-
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.sp3
.ce
Table ±.2 (continued)
.bp
.ta 0.8i +2.6i +\w'(AH1 .UH2) (O U)°'u
.nr x0 0.8i+2.6i+\w'(AH1 .UH2) (O U)°'+\w'; i uh \- here'
\l'\n(x0u\(ul'
.sp
vowel clusters
	EH I	A1 AY	; e i \- hey
	UH Ï	O U	; uh u \- ho
	AE I	(AH1 EH3) I	; á i \- hi
	AE Ï	(AH1 .UH2) (O U)	; á u \- how
	AW I	(O UH) E	; o i \- hoi
	I UH	E I	; i uh \- here
	EH UH	(EH A1) EH	; e uh \- hair
	Ï UH	Ï UH	; u uh \- pïr
	Y U	Y1 (IU U)
.sp
vowel transitions
	{F M B P} O	(.O1 O)
	{L R} EH	(EH3 EH)
	{B K T D R} UH	(UH3 UH)
	{T D} A1	(EH3 A1)
	{T D} AW	(AH1 AW)
	{W} I	(I3 I)
	{G SH W K} Ï	(IU Ï)
	AY {K G T D}	(AY Y)
	E {M T}	(E Y)
	I {M T}	(I Y)
	E {L}	(I3 UH)
	EH {R N S D T}	(EH EH3)
	I {R T}	(I I3)
	AE {S N}	(AE EH)
	AE {K}	(AE EH3)
	A1 {R}	(A1 EH1)
	AH1 {R P K}	(AH1 UH)
	AH1 {ZH}	(AH1 EH3)
.sp
intervocalics
	{voiced} T {voiced}	DT
.sp
consonant transitions
	L {EH}	L1
	H {U Ï IU}	H1
\l'\n(x0u\(ul'
.sp3
.ce
Table ±.5 Contextual rules for Votrax sound segments
.bp
\l'\n(x0u\(ul'
.sp
consonant clusters
	B {stop-consonant}	(B PA0)
	P {stop-consonant}	(P PA0)
	D {stop-consonant}	(D PA0)
	T {stop-consonant}	(T PA0)
	DT {stop-consonant}	(T PA0)
	G {stop-consonant}	(G PA0)
	K {stop-consonant}	(K PA0)
	{D T} R	(.X1 R)
	K R	.K (.X1 R)
	{consonant} R	.R
	{consonant} L	L1
	K W	.K .W
	D ZH	D J
	T SH	T CH
.sp
initial eæects
	{.syì} P {vowel}	(P .PH)
	{.syì} K {vowel}	(K .H1)
	{.syì} T {vowel}	(T .S)
	{.syì} L	L1
	{.syì} H {U Ï O AW AH1}	H1
.sp
terminal eæects
	E {PA0}	(E Y)
\l'\n(x0u\(ul'
.ta 0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i +0.8i
.sp3
.ce
Table ±.5 (continued)
.ev
