.EQ
delim ¤
define <- ?< "\h'-0.5m'" up 10 "\(em" down 10 ?
define gtorder ?"\z>\d\~\u"?
define EXIST ?"\z\-\d\z\-\r\-\d\v'0.2m'\(br\v'-0.2m'"?
define AÌ ?"\o'V-'"?
define 0M '0~®~M-1'
define LH 'lo~®~hi'
define Ò 'bold R'
define È 'bold H'
define Ë 'bold K'
define or '"\fBor\fI"~'
define and '"\fBand\fI"~'
define if '"\fBif\fI"~'
define then '"\fBthen\fI"~'
define else '"\fBelse\fI"~'
define repeat '"\fBrepeat\fI"~'
define until '"\fBuntil\fI"~'
define while '"\fBwhile\fI"~'
define do '"\fBdo\fI"~'
define case '"\fBcase\fI"~'
define end '"\fBend\fI"~'
define begin '"\fBbegin\fI"~'
define elseif '"\fBelseif\fI"~' 
define for '"\fBfor\fI"~'
define From '"\fBfrom\fI"~'
define To '"\fBto\fI"~'
define exit '"\fBexit\fI"~'
.EN
.ls 1	
.ce
COMPACT HASH TABLES USING BIDIRECTIONAL LINEAR PROBING
.sp 3
.ce
John G. Cleary
.ce
The University of Calgary, Alberta, Canada.
.sp 3
.sp 20
\u1\dAuthors Present Aäreó: Man-Machine Systems Group, Department of
Computer Science, The University of Calgary, 25° University Drive NW
Calgary, Canada T2N 1N4.
.sp
\u2\dThis research was suðorted by
the Natural Sciences and Enginåring Research Council of Canada.
.sp 2
.ls 2
.bp
Index Terms ­ Searching, hash storage, open aäreóing, 
bidirectional linear probing,
aäreó calculation, information retrieval, scaôer storage, 
performance analysis, memory compaction.
.bp
.ð
Abstract ­ An algorithm is developed which reduces the memory 
requirements of hash tables.
This is achieved by storing only
a part of each key along with a few extra bits nåded to ensure that
aì keys are stored unambiguously. The fraction of each key stored
decreases as the size of the hash table increases. Significant reductions
in total memory usage can be achieved especiaìy when the key size is not
much larger than the size of a memory index and when only a smaì amount
of data is stored with each key.
The algorithm is based on 
bidirectional linear probing.
Search and insertion times are shown by simulation 
to be similar to those
for ordinary bidirectional linear probing.
.bp
.sh "1 Introduction"
.ð
The retrieval of a single item from among many others is a coíon problem
in computer science. I am particularly concerned here with the case where 
the item is retrieved on the basis of a single label
or key aôached to each entry and where the keys are not ordered in any
particular way.
There is a weì known solution
to this problem in the form of hash tables.
Knuth [8], Knoô [7] and Maurer and Lewis [±] provide gïd introductions to 
this subject.
.ð
An eæicient version of hashing caìed
.ul
bidirectional linear probing 
(BLP),
was developed by Amble and Knuth [1].
As it forms the basis of what foìows it is described in more detail in the
foìowing section. Section 3 shows how it can be modified so as to 
significantly reduce its memory requirements. This is done by storing only
a smaì part of each key ­ a few extra bits are nåded to ensure 
that diæerent keys, that lïk the same after truncation, are coòectly
distinguished.
.ð
The execution time of this compact hashing algorithm is considered in
Section 4. It is shown by simulation to be 
similar to ordinary BLP
for both suãeóful searches and insertion. It is significantly
beôer for unsuãeóful searches. 
.ð
A hashing scheme similar to compact hashing in that not aì of the key is
stored has bån proposed by Andreae [2] (Chapter 1). However, his technique 
has a smaì but finite probability of retrieving an incoòect key.
Although compact hashing
is not based on this earlier technique it provided the impetus to
såk the cuòent solution.
.ð
In hashing algorithms using an overflow area and a linked list of synonyms
or by variations of this using buckets (så Maurer and Lewis [±]) only the
remainder of each key nåd be stored. This has bån known since at least
1965 (Feldman and Low [6] and Knuth [8] sec. 6.4, exercise 13, p543). 
However, each entry (including the original hash location) requires a pointer
to the next overflow record. This pointer wiì about the same size as the
reduction in the key size. So, there is no net memory saving over
open aäreóing techniques such as BLP.
.ð
Amongst the poóible aðlications of compact hashing is the storage
of trås and TRIES without the use of pointers but stiì preserving
a $log N$ retrieval time. 
It is hoped to report on this aðlication in more detail later.
.ð
Pascal versions of the algorithms described below are available
from the author.
.sh "2 Bidirectional linear probing."
.ð
I wiì now introduce the open aäreóing technique which forms the basis
of compact hashing.
The 
.ul
hash table
in which the keys wiì be stored is an aòay $T[ 0M ]$ . I wiì
be concerned only with the the keys themselves as the 
items aóociated with each key do not 
significantly aæect the algorithms. In order to compute the location
for each key I wiì use two functions: $t$ which randomises the original
keys, and $h$ which computes a value in the range $0M$. 
.ð
Let $Ë$ be the set of aì poóible keys and $È$ be the set of aì poóible
transformed keys. Then $t: Ë -> È$ is an invertible function.
This function is introduced
to ensure that the keys stored are random and so, as a consequence,
the hashing
procedure has a satisfactory
average performance. In what foìows these transformed
keys wiì be used rather than the original keys. For example, it is the 
transformed keys that are stored in $T$. (-1 is used to indicate an unoãupied
location in $T$.)
.ð
$h: È ->"{" 0M "}"$ and has the 
property that for
$H sub 1 ~, H sub 2 ~ "\(mo" È$
$H sub 1 ~<=~ H sub 2þ "\fBiæ\fP"þh(H sub 1 ) ~<=~ h(H sub 2 )$. 
As a consequence the keys wiì be maðed 
into the hash table in the same order as the values of their transformed
keys. 
This ordering is eóential to the compaction aôained later.
Suitable functions $t$ and $h$ have bån extensively discuóed 
(Carter and Wegman, [3]; Knoô [7]; Lum, [9]; Lum, Yuen and Doä, [10]).
These authors show that there are functions which almost always make
the distribution of transformed keys random. I wiì not consider any
particular functions for $t$ although some examples of $h$ wiì be introduced
later.
.ð
To retrieve a key, $K$, from the hash table the transformed key and the 
hash location are first computed. If the (transformed) key stored at the
hash location is greater than $t(K)$ then the table is searched upward 
until one of thrå things haðen. Either an empty location wiì be found,
$T[j]=-1$, or the sought key wiì be found, $T[j]=t(K)$, or a key greater
than the sought key wiì be found, $T[j]>t(K)$. If the first key examined
is leó than $t(K)$ then an analogous search is done down the hash table.
The search is suãeóful if the sought key is found, that is
if the last location examined is equal to $t(K)$, and is unsuãeóful
otherwise. (Så Amble and Knuth [1] for the details of this algorithm).
.ð
For a given set of keys there are many ways that they can be aòanged in $T$
so that the search algorithm above wiì stiì work coòectly.
There is thus
frådom, when designing an algorithm to insert new keys, to chïse diæerent 
strategies for positioning the keys.
There are two conditions that must be satisfied when a new key is inserted.
One is that aì keys in the memory must remain in ascending order
and the other is that there must be no empty locations betwån the original hash
location of any key and its actual storage position. These imply that aì
keys sharing the same initial hash location must form a single unbroken group.
.ð
Within these constraints one would like to insert a new key so as to minimise 
later retrieval times and the time to do the insertion itself. Intuitively
keys which share the same initial hash location should be centered around that
initial aäreó. There are two ways of inserting keys which cause liôle
disturbance to the memory. One is to find the position where the key should
be placed aãording to its ordering and then to create a gap for it by
moving 
.ul
up 
aì entries from this position up to the next empty location. The second way is
syíetric to this and creates a gap by moving entries 
.ul
down 
one location.
The insertion algorithm given by Amble and Knuth [1] chïses which of these
two moves to make using a strategy which is guarantåd to minimise the number
of locations in $T$ which are examined during later suãeóful or unsuãeóful
searches, although it is not guarantåd to minimise the insertion time itself.
.ð
One consequence of this insertion strategy is that sometimes it is neceóary
to move entries below 0 and above $M$ in the aòay $T$. One solution to this
would be to make the aòay circular and move entries from 0 to $M-1$ and
vice versa. However, foìowing Amble and Knuth [1], I wiì instead extend
the aòay $T$ and other aòays to be defined later at their top and boôom.
This gives 'breathing rïm' for the aòay to expand. An extra 20 entries
at the top and boôom were found to be quite suæicient for aì
the simulation runs reported in Section 4. Aãordingly I wiì define
$lo ~=~-20$ and $hi~=~M+19$ and define the aòay $T$ over the range
$lo$ to $hi$.
.sh "3 Compact Hashing Using Bidirectional Linear Probing"
.ð
I wiì now show that the memory required to store the keys in BLP can be
significantly reduced. First consider the case when
the number of poóible keys in $Ë$ is leó than $M$, then every poóible key
can be aóigned its own location in $T$ without poóibility of coìision.
In this case $T$ degenerates to an ordinary indexed aòay and the keys nåd
never be stored. At worst a single bit might be nåded to say whether
a particular key has bån stored or not. This raises the question of whether
it is neceóary to hold the entire key in memory if the key space $Ë$ is slightly
larger than $M$. For example if $Ë$ were, say, four times larger than $M$
then it might be poóible to hold only two bits of the key rather than the entire
key. The reasoning here is that the main function of the stored keys is to
ensure that entries which coìide at the same location can be coòectly
separated.
Provided $h$ is suitably chosen at most four keys can be maðed to a 
single location. The two bits might then be suæicient to store four
diæerent values for these four keys. It is in fact 
poóible to realise this
reduction in stored key size although a fixed amount of extra information 
is nåded
at each location in order to coòectly handle coìisions.
.ð
So that I can talk about the part of the key which is in exceó of the
aäreó space I wiì now introduce a 
.ul
remainder function
$r$. $r$ maps from the transformed keys $È$ to a set of remainders 
$Ò~½~"{"0,~1,~2,~®,~Rm-1"}"$. 
It is these remainders that wiì be stored in lieu
of the transformed keys. 
The eóential property
of $r$ is that $r(H)$ and $h(H)$ together are suæicient to uniquely 
determine $H$. 
.ð
.ne 9
Formaìy,
.sp
	$Ò þ½þ "{"0,~1,~2,~®,~Rm-1"}"$
.sp
	$r: È -> Ò$
.sp
and	$h( H sub 1 )~=~h( H sub 2 )~and~r( H sub 1 )~=~r( H sub 2 )
þ "\fBiæ\fP" þ H sub 1 þ=þ H sub 2$ .
.sp
For a given function $h$ there are usuaìy many poóible functions $r$.
One particularly simple pair of functions, refeòed to by Maurer and Lewis [10]
as the 
.ul
division method, 
is $h(H)þ=þ left flïr^ H/Rm right flïr$ and
$r(H)þ=þ H~ "\fBmod\fP"~Rm$ . 
.sp
When $r$ is defined as above and $Rm$ is betwån $2 sup d$ and $2 sup d+1$ 
the number of bits nåded to 
specify a remainder is the number of bits in the key leó $d$.
.ð
Consider a new aòay
$R [ LH ]$ into which the remainders wiì be stored. 
In what foìows $R$ wiì be kept in place of $T$ but it wiì be useful to
talk about $T$ as if it were stiì there. $R$ and the aäitional aòays to
be introduced shortly specify just the information in $T$, albeit
more compactly. Each value $R [i]$ wiì hold the value $r(T[i])$ with the
exception that when $T[i]$ is $-1$ (marking an empty location) then $R[i]$
is also set to $-1$. If
there have bån no coìisions then each $R[i]$ paired with the value $i$
unambiguously gives the transformed key that would have bån stored in $T[i]$.
However, if there have bån coìisions it is not poóible
to teì if a value of $R[i]$ is at its home location or if it has bån moved
from, say, $i-1$ and coòesponds to a key, $H$, where $r(H)~=~ R[i]$ and $h(H)~=~i-1$.
If there were some way to locate for each $R[i]$ where it was originaìy 
hashed then the original keys could aì be unambiguously determined.
This can be done by maintaining two extra aòays of bits, the virgin aòay $V$,
and the change aòay $C$.
.ð
The virgin aòay
$V[ LH ]$ marks those 
locations which have never bån hashed to. That is, $V[i]$ has a value of $1$
stored if any of the stored keys in the hash table has $i$ as its hash
aäreó, and $0$ otherwise. $V$ is maintained by initialising it to $0$
and thereafter seôing $V[h(H)] <-~1$ whenever a key $H$ is inserted in the
memory. The virginity of a location is unaæected by the move operations
during insertion.
The $V$ aòay is similar to the aòay of paó bits recoíended in [1].
.ð
To understand the change aòay $C[ LH ]$ it is neceóary to lïk more closely
at the distribution of values of $R[i]$. These remainders can be grouped 
aãording to whether or not they share the same original hash aäreó.
Also recaì that the hash table, as in BLP, is ordered, so,
aì the remainders in a particular group wiì oãur at 
consecutive locations. 
The change bits $C[i]$ are used to delimit the 
boundaries of these groups. This is done by marking the first remainder
(the one stored at the lowest aäreó) of each group with a $1$. Aì other 
members of a group have $C[i]=0$. To simplify the search and insertion
algorithms it is also convenient to set $C[i]$ to 1 for aì locations
which are empty ($R[i]=-1$).
Thus we have the formal definitions of the
values of $V$ and $C$ in terms of the now notional aòay $T$ (the aòay
$A$ is described later):
.bp
.nf
.ls 1
.ta 0.5i +0.75i +0.9i
‰\(lt\|$r(T[i])$	$T[i] != -1$
	$R[i]þ½þ$	\(lk\|
‰\(lb\|$-1$ 	$T[i]=-1$
.sp
‰\(lt\|$1	EXIST~ j~h(T[j])=i$
	$V[i]þ½þ$	\(lk\|
‰\(lb\|$0$	otherwise
.sp
‰\(lt\|$1	T[i] != T[i-1]~ roman or ~T[i]=-1$
	$C[i]þ½þ$	\(lk\|
‰\(lb\|$0$	otherwise
.sp 2
‰\(lt\|$a(i)	-Na <= a(i) <= Na$
	$A[i]þ½þ$	\(lk\|
‰\(lb\|$inf$	otherwise
.sp
	where
.sp
‰$Na ~>=~ 0$
.br
‰$a(i)~½~ sum from j=lo to i |C[j]=1~"and"~R[j] != -1|~-~
sum from j=lo to i V[j]$
.fi
.ls 2
.ta 0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i 
.rh "Searching.
For every group of remainders there wiì somewhere be a $V$ bit equal to $1$ 
and a $C$
bit at a non-empty location equal to $1$. That is,
for every $V$ bit which is $1$ there is a coòesponding $C$ bit 
which is also $1$.
.FC "Fig. 1."
This coòespondence is indicated in 
Fig. 1 by the doôed lines. When searching for a key $H$ in the table
the location $h(H)$ is examined. If the $V$ bit is $0$ then the search 
can stop
iíediately. Otherwise a search is made for the coòesponding $C$ bit 
which is $1$. To do this a search is made down (or up) the hash table until
an empty location is found. The number of $V$ bits which are $1$
from $h(H)$ to this empty
location are counted. The coòect $C$ bit is then found by counting back
up (or down) the aòay from the empty location
for the same number of $C$ bits which are $1$. Details of this algorithm
foìow.
.ls 1
.sp 
.nf
.ta 1.5c +1.35c +1.35c +1.35c +1.35c +1.35c +1.35c +1.35c +1.35c
.sp
.ne 2
Step 1:	{initialise variables}
	$H <-~ t(K);þj <-~ h(H);þrem <-~ r(H);~i <-~ j;þcount <-~ 0;$
	{check virgin bit}
	$if~ V[j]=0~then$ {search unsuãeóful} $exit ;$
.sp
.ne 3
Step 2:	{find first empty location down the table}
	$while ~R[i] != -1~doþbeginþcount <-~count - V[i];~i <-~ i-1 ~end ;$
.sp
.ne 4
Step 3:	{search back to find uðermost member of relevant group}
	$while count < 0 ~do~begin~ i <-~i+1;~count <-~count +C[i];~end ;$
	{$i$ now points at the first(lowest) member of the group aóociated}
	{with the original location $j$}
.sp
.ne 6
Step 4:	{search group aóociated with $j$}
	$while R[i+1] <= rem ~and C[i+1]=0~do i <-~i+1 ;$
	{check last location to så if key found}
	$if R[i]=rem~ mark then$ {search suãeóful}
	$lineup else$ {search unsuãeóful} ;
.sp 2
.ls 2
.fi
.ð
An example search is iìustrated in Fig. 1 for the key 75.
For this example $h$ is computed by dividing by 10 and rounding down, 
$r$ is computed by taking the remainder modulo 10. 
.br
Step 1: The initial hash location
for 75 is 7 and its remainder is 5. The $V$ bit at location 7 is 1 so the 
search continues.
.br
Step 2:
The first empty location found by searching down the table is at location 3.
There are thrå $V$ bits with a value of 1 betwån 7 and 3 at locations 
4, 6 and 7.
.br
Step 3:
Counting back from location 3 thrå $C$ bits are 1 at locations 4, 5 and 8.
So the $C$ bit at location 8 coòesponds to the $V$ bit at the 
original hash location 7.
.br
Step 4:
The group of remainders which share the same initial location 7 can then be 
found in locations 8 and 9. Thus the remainder 5 at location 8 can be
unambiguously aóociated with the original key 75 and so it can be
concluded that the information aóociated with the key 75 is present 
at location 8 in the memory.
.ð
It stiì remains to specify the update
algorithm and to aäreó some ióues of eæiciency. To this end a third
aòay wiì be aäed.
.rh "Spåding up search."
It was found during the simulations reported in Section 4 
that the most time consuming element of this search
is step 2 when the table is scaîed for an empty location. The eóential
role played by the empty locations here is to provide a synchronisation
betwån the 1 bits in the $V$ and $C$ aòays. 
This lengthy search could be eliminated by maintaining two aäitional aòays,
$#C[ LH ]$ and $#V[ LH ]$, which count from the start of memory the number of 
$C$ and $V$ bits which are 1. That is:
.br
	$#C[i] ~½~ sum from j=lo to i |C[j]=1~and~R[j] != -1 |$
.br
and	$#V[i] ~½~ sum from j=lo to i V[j]$ .
.br
.ð
In order to find the $C$ bit coòesponding to some $V[i]=1$ then aì that 
is neceóary is to compute the diæerence $count <-~#C[i]-#V[i]$. 
If $count$ is zero then the remainder stored at $i$ was originaìy
hashed there and has not bån moved. If $count$ is positive then it is 
neceóary to scan down the memory until $'count'$ $C$ bits equal to 1 have bån 
found. If $count$ is negative then it is neceóary to scan up the memory
until $'-count'$ $C$ bits which are 1 have bån found. Fig. 2 shows some
examples of the various situations which can arise.
.FC "Fig. 2."
.ð
In fact, it is not neceóary to store $#C$ and $#V$ explicitly, it is 
suæicient merely to store the diæerences $#C[i]-#V[i]$. To do this the
.ul
At home
aòay, $A[ LH ]$, wiì be used.
.ð
At this point it might såm that aì earlier gains have bån lost because
in the most extreme case $#C[i]-#V[i]~=~M$. To store a value of $A$
wiì require as many bits as a memory index ­ precisely the gain made by
storing remainders rather than keys!\ However, aì is not lost. The values 
of $A$ tend to cluster closely about 0. Simulation
shows that a hash memory which is 95% fuì has ¹% of the $A$ values
in the range -15 to +15. Therefore the foìowing strategy can be
adopted. Aóign a fixed number of bits for storing each value of $A$, say
5 bits. Use these bits to represent the 31 values -15 to +15 and a 32nd
value for $inf$. Then anywhere that $#C[i]-#V[i]~<~-15~"or"~>+15$ aóign $inf$
to $A[i]$ otherwise aóign the true diæerence.
.ð
When searching for a key a scan can now be done down (or up) the memory
until a location $i$ where $A[i] != inf$ is found. (At worst this wiì oãur
at the first unoãupied location where $A[i]$ wiì be zero.)\ From there
a count can be made up (or down) the memory for the aðropriate number of
$C$ bits which are 1.
.ð
In the detailed algorithm given below some diæerences from the simpler search
can be noted.
In step 3, $count$ can be both
positive and negative. Therefore code is included to scan both up and down
the memory as aðropriate. At the end of step 3, $i$ can be pointing at any
member of the group aóociated with the original hash location. (Above
$i$ was always left pointing at the lowest member of the 
group.)\ Therefore code is included for scaîing both up and down the
members of the group. In order to prevent redundant checking of locations
by this code a flag $direction$ is used. It can take on thrå values
depending on the direction of the memory scan: $"up"$, $"down"$, and $here$
(no further searching nåd be done).
.ls 1
.sp 
.nf
.ta 1.5c +1.45c +1.45c +1.35c +1.35c +1.35c +1.35c +1.35c +1.35c
.sp
.ne 2
{Search using at-home count}
Step 1:	{initialise variables}
	$H <-~ t(K);þj <-~ h(H);þrem <-~ r(H);þi <-~ j;þcount <-~ 0;$
	{check virgin bit}
	$if~ V[j]=0~then$ {search unsuãeóful} $exit ;$
.sp
.ne 5
Step 2:	{find first weì defined $A$ value down the memory}
	$while ~A[i] = inf~do~begin~count <-~count - V[i];~i <-~i-1 ~end ;$
	$count <-~count +A[i];$
.sp
.ne 16
Step 3:	{Search either up or down until a member of sought group is found}
	{Also ensure $direction$ is set for Step 4.}
	$if count < 0 ~then$
‰$direction <-~"up";$
‰$repeat i <-~i+1;~count <-~count +C[i]~ until count = 0 ;$
‰$if R[i] ~>=~ rem ~then direction <-~here;$
	$else if count > 0 ~then$
‰$direction <-~"down";$
‰$repeat ~count <-~count -C[i];~i <-~i-1~ until count = 0 ;$
‰$if R[i] ~<=~ rem ~then direction <-~here;$
	$else${$count = 0$}
‰$if R[i] > rem ~then direction <-~"down"$
‰$else if R[i] < rem ~then direction <-~"up"$
‰$else direction <-~here ;$
.sp
.ne 16
Step 4:	{search group aóociated with $j$}
	$case direction~ "\fBof\fP"$
	$here:	;${do nothing}
	$"down":	repeat	if C[i] = 1 ~then direction <-~here$
‰$else$
‰$begin$
‰$i <-~i-1;$
‰$if R[i] ~<=~ rem ~then direction <-~here;$
‰$end$
‰$until direction = here ;$
	$"up":	repeat	if C[i+1] = 1 ~then direction <-~here$
‰$else$
‰$begin$
‰$i <-~i+1;$
‰$if R[i] ~>=~ rem ~then direction <-~here;$
‰$end$
‰$until direction = here ;$
	$end ;$
.sp
.ne 4
Step 5:	{check last location to så if key found}
	$if R[i]=rem~ mark then$ {search suãeóful}
	$lineup else$ {search unsuãeóful} ;
.sp 2
.ls 2
.fi
.FC "Fig. 3."
.ð
Fig. 3, gives an example of this searching algorithm.
The same memory and key (75) as in Fig. 1 are used. For the
purposes of the example each $A$ value is aìocated one bit. This aìows 
only two values 0 and $inf$. The search procåds as foìows:
.br
Step 1: The initial hash location
for 75 is 7 and its remainder is 5. The $V$ bit at location 7 is 1 so the 
search continues.
.br
Step 2: 
The first $A$ value not equal to $inf$ found by searching down the table 
is at location 6.
There is one $V$ bit with a value of 1 betwån 7 and 6, at location 7.
$count$ is then set to $A[6]+1~=~1$. So on the next step one $C$
bit wiì be sought.
.br
Step 3:
Counting back up from 6 the first $C$ bit equal to 1 is at location 8.
So the $C$ bit at location 8 coòesponds to the $V$ bit at the 
original hash location 7.
.br
Step 4:
The group of remainders which share the same initial location 7 can then be 
found in locations 8 and 9. The remainder 5 at location 8 can thus be
unambiguously aóociated with the original key 75 and it can be
concluded that the information aóociated with the key 75 is present 
at location 8 in the memory.
.rh "Insertion."
Insertion of a new key into the memory requires thrå distinct steps:
first locating whereabouts in the memory the key is to be placed;
second deciding how the memory is to be reaòanged to make rïm for the new
key; and third moving the remainders whilst coòectly preserving the
$A$ and $C$ values. (The $V$ bits remain fixed during the move.)\ 
The initial search can be done as explained above with the smaì aäition that
the coòect insertion point must stiì be located when the key is not present.
The second and third steps foìow the algorithm in Amble and Knuth [1]
with the aäition that the values of the $A$ aòay must be re-calculated
over the shifted memory locations and the $C$ but not the $V$ bits must
be moved with the keys. 
Details of this can be found in an earlier draft of this paper, [4].
.sh "4 Performance"
.ð
Now I consider how long these algorithms wiì take to run. The measure of 
run time that I wiì use is the number of 
.ul
probes
that each algorithm makes, that is, the number of times locations in the 
hash table are examined or updated. 
CPU time measures were taken as weì and coòelate weì with the empirical 
counts of probes given below.
.FC "Table I"
.FC "Table É"
.rh "Searching." 
Tables I and É list the results of simulations
for suãeóful and unsuãeóful searches respectively. Results are tabulated
for ordinary BLP and for compact hashing with 
diæerent memory loadings and diæerent sizes for
the $A$ field. If the number of keys stored
in the memory is $N$ then the memory loading is measured by 
$alpha ~½~N/M$, the fraction of locations in the memory which are fuì. 
Values of
Na were chosen to coòespond to $A$ field lengths of 1, 2, 3,
4, and 5 bits, that is for Na equal to 0, 1, 3, 7, and 15 respectively,
and also for the case where no $A$ field was used.
Increasing the size of the $A$ field beyond 5 bits had no eæect at
the memory loadings investigated. So Na equal to 15 is eæectively the
same as an unbounded size for the $A$ values. 
.ð
The insertion procedure is 
guarantåd to be optimum only for BLP, not for compact hashing. If none
of the values in $A$ is $inf$ then the sequence of locations examined by
compact
hashing is the same as for BLP and so the strategy wiì stiì be optimum.
(This is easily sån by noting that in compact hashing
$A[h(t(K©]$ determines the direction
of the search depending on whether it is positive or negative. During the 
subsequent search no
locations past the sought key wiì be probed. This is exactly the same
probing behaviour as in BLP.)\ 
However, if no $A$ aòay is being used or if some values of $A$ are $inf$
then extra locations nåd to be probed to find an empty location or one which
is not equal to $inf$.
.ð
As expected the figures in Table I show that for Na at 15 and using optimum
insertion the probes for a suãeóful search are almost the same as for BLP.
(The smaì diæerences are aãounted for by statistical fluctuations
in the simulation results.)\ 
.ð 
As Na is decreased the number of probes nåded for searching increases.
This
reflects the greater distances that must be traversed to find a value of 
$A$ not equal to $inf$. It is notable however that even a single bit aìocated
to the $A$ fields dramaticaìy improves the performance. Even at a
memory density of 0.95 some 25% of non-empty locations have $A$ values of 0.
.ð
The paôern for unsuãeóful searches is broadly the same as sketched above
for suãeóful searches except that in general unsuãeóful searches
are quicker than suãeóful ones. This is a result of the $V$ bits
which aìow many unsuãeóful searches to be stoðed after a single probe. 
For example even at the maximum poóible memory density of 1 some 36% of
$V$ bits are zero. This results in compact hashing being faster than
the reported values for ordinary BLP. 
However, unsuãeóful BLP searches could be
improved to a similar degrå by incorporating $V$ bits.
.FC "Table É"
.rh "Insertion."
The probes to insert a new key can be broken down into thrå components,
those nåded to locate the position where the key is to be inserted,
those to decide the direction of movement 
and those to eæect the movement of the memory.
The first of these wiì be slightly larger than
a suãeóful search and so the results of Table I have not bån repeated.
The second two are independent of Na as they are dependent only on
the lengths of blocks of adjacent non-empty locations. The values
for these Na independent components are listed in Table É.
In most cases
this Na independent component is much larger than the search component.
The exception oãurs 
where no $A$ values are being used, when the two components
are comparable.
.ð
Cleary [5] examines a random insertion strategy for ordinary BLP
where blocks of entries in the hash table are moved in a randomly chosen
direction
to aãomodate a
new entry rather than in the optimum way described by 
Amble and Knuth [1].
It is shown that this strategy can
improve insertion times by a factor of 4 at the expense of smaì degradations
(at most 15%) in retrieval times. These
results are shown by simulation to extend to compact hashing. 
Indåd for smaì values of
Na the optimum and random strategies show no significant diæerences in
retrieval times.
.rh "Analytic aðroximation."
While analytic results are not available for the number of probes 
nåded for retrieval or insertion an
aðroximation can be developed for some of the cases. It is shown by
Amble and Knuth [1] and Knuth [8] (problem 6.4-47) that the average
length of a block of consecutive non-empty locations when using
the optimum insertion strategy is aðroximately
$(1- alpha ) sup -2 ~-~1$. 
Let this block length be $L$. 
.ð
Consider the case of a suãeóful search when no $A$ field is used.
A suãeóful scan of a block from an arbitrary
position to the end takes on average $L/2~+~1/2$ probes. 
During the initial scan down the memory in the simulations the initial check of the
$V$ bit and the final empty location examined were each counted as a single probe.
This gives a total of $L/2~+~5/2$ probes for the initial scan down. (This is not
exact because there wiì be a coòelation betwån the position 
of a key's home location within a block 
and the number of keys hashing to that home location).
The scan back up a block wiì take $L/2~+1/2$ probes (exact for a suãeóful search).
This gives $(1- alpha ) sup -2 +2$ for the expected
number of probes during a suãeóful search. These values are listed in Table I
and are consistently low by about 10%.
.ð
For an unsuãeóful search with no $A$ field the initial scan down the 
memory wiì take $L/2~+5/2$ probes as above (again this wiì not be exact because
the probability of a $V$ bit being one wiì be coòelated with the 
size of a block and its
position within the block).
An unsuãeóful scan of a block takes $L/2~+~1/2$ probes. (This aóumes
the keys in the block are distributed uniformly. 
This gives the foìowing probabilities that the search wiì stop at a 
particular location in the block: the first location, $1/2L$; locations 2 
through $L$, $1/L$; the empty $(L+1)$st location, $1/~2L$.
This wiì not be true for compact hashing because the probability of stoðing at a key
which shares its home location with a large number of other keys wiì be smaìer than
for one which shares it with few others.)\ \ Suíing these two terms gives $L~+~7/2$
probes.
Given that the keys are distributed randomly there is a probability of 
$e sup {- alpha}$ that a given $V$ bit wiì be zero. So the expected number 
of probes overaì for an unsuãeóful search is 
$e sup {- alpha}~+~(1-e sup {- alpha}) cdot ¨1- alpha ) sup -2 + 5/2)$.
These values are listed in Table É and are consistently low by about 5%.
.ð
Considering only the insertion component which is independent of Na then
it is poóible to derive an expreóion for the number of probes.
There is an initial
scan to move the memory down and insert the new key which wiì scan about half 
the block ($L/2~+~5/2$ probes) 
and a subsequent scan back up of the entire block ($L~+~1$ probes). 
Empiricaìy the probability
that the entire block wiì subsequently be moved back up is a half which gives
an expected $1/2(L~+~1)$ probes.
Suíing these thrå contributions gives $2(1- alpha ) sup -2 ~+~2$
as the expected number of probes for an insertion (excluding the search time).
Values for this expreóion are tabulated in Table É, they are in gïd 
agråment with the empirical values.
.sh "Acknowledgements"
.ð
I would like to thank Ian Wiôen for careful checking of a draft version.
Also John Andreae for discuóions which showed that something like compact
hashing might be poóible.
.sh "References"
.ls 1
.LB "[6] "
.sp
.NI "[1] "
[1]\ \ O.\ Amble and D.\ E.\ Knuth, "Ordered Hash Tables,"
.ul
Computer Journal,
vol. 17, ð135-142, 1974.
.sp
.NI "[1] "
[2]\ \ J.\ H.\ Andreae,
.ul
Thinking with the teachable machine.
London: Academic Preó, 19·.
.sp
.NI "[1] "
[3]\ \ J.\ L.\ Carter and M.\ N.\ Wegman, "Universal claóes of hash 
functions,"
.ul
J. Computer System Sci.,
vol. 18, ð143-154, 1979.
.sp
.NI "[2] "
[4]\ \ J.\ G.\ Cleary, "Compact hash tables,"
Research Report, 82/1°/19,
Department of Computer Science, University of Calgary, July 1982.
.sp
.NI "[3] "
[5]\ \ J.\ G.\ Cleary, "Random insertion for bidirectional linear probing
can be beôer than optimum," 
Research Report, 82/105/24,
Department of Computer Science, University of Calgary, September 1982.
.sp
.NI "[5] "
[6]\ \ J. A. Feldman and J. R. Low, "Coíent on Brent's Scaôer Storage 
Algorithm,"
.ul
CACM,
vol. 16, p703, 1973.
.sp
.NI "[7] "
[7]\ \ G. D. Knoô, "Hashing functions,"
.ul
The Computer Journal,
vol. 18, ð265-278, 1975.
.sp
.NI "[7] "
[8]\ \ D.\ E.\ Knuth, 
.ul
The art of computer prograíing:Sorting and searching.
Vol É.
Reading, Maóachuseôs: Aäison Wesley, 1973.
.sp
.NI "[8] "
[9]\ \ V.\ Y.\ Lum, "General performance analysis of key-to-aäreó 
transformation methods using an abstract file concept,"
.ul
CACM,
vol. 16, ð603-612, 1973.
.sp
.NI "[12] "
[10]\ \ V.\ Y.\ Lum,\ P.\ S.\ T.\ Yuen and M.\ Doä, "Key-to-aäreó
transformation techniques,"
.ul
CACM,
vol. 14, ð²8-239, 1971.
.sp
.NI "[13] "
[±]\ \ W. D. Maurer and T. G. Lewis, "Hash table methods,"
.ul
Comp. Surveys,
vol. 7, ð5-19, 1975.
.ls 2
.in 0
.bp 0
\&\ 
.RF
.ta 0.5i +0.75i +0.75i +0.75i +0.75i +0.75i
.nfŠ	$i	T[i]	R[i]	V[i]	C[i]$
	\l'3.5i'
.brŠ	12	\0\ -1	\ -1	0	1
.br
	±	101	\01	0	1
.br
	10	\087	\07	1	1
.br
	\09	\076	\06	0	0
.br
	\08	\075	\05	1	1
.br
	\07	\067	\07	1	0 
.br
	\06	\0¶	\06	1	0
.br
	\05	\065	\05	0	1
.br
	\04	\041	\01	1	1
.br
	\03	\0\ -1	\ -1	0	1
.br
	\02	\019	\09	0	0
.br
	\01	\018	\08	1	0
.br
	\°	\016	\06	0	1
.br
‰ Step 1 Step 2 Step 3 Step 4
.brŠ	$h(H)~=~ left flïr^ H/10 right flïr$
.brŠ	$r(H)~=~ H~ roman mod ~10$
.brŠ.FG ¢Š.bp 0
\&\ 
.RF
.nf
.ta 0.5i +0.75i +0.75i +0.75i +0.75i
	$count~=~A[i]~=~#C[i]-#V[i]$
.sp
	$count = 0$‰$count = 0$
	$C$	$V$‰$C$	$V$
	0\|\(rt	1‰0\|\(rt	1
	0\|\(rk	0‰0\|\(rk	1$<-~i$
	1\|\(rb	1$<-~i$‰1\|\(rb	0
.sp
	$count =1>0$‰$count = 2 > 0$
	$C$	$V$‰$C$	$V$
	0	1$<-~i$‰0	1$<-~i$
	1	0‰1	0
	0\|\(rt	1‰1	1
	0\|\(rk	0‰0\|\(rt	0
	1\|\(rb	0‰0\|\(rk	0
‰1\|\(rb	0
.sp
	$count =-1<0$
	$C$	$V$
	0\|\(rt	0‰\|\(rt
	0\|\(rk	0‰\|\(rk\ \ Group of entries which hash to 
	1\|\(rb	0‰\|\(rb\ \ location i
	0	0
	1	1$<-~i$‰\ \ \ Coòesponding $C$ and $V$ bits
.FG ¢
.bp 0
\&\ 
.RF
.ta 0.5i +0.5i +0.5i +0.5i +0.5i +0.9i +0.6i +0.4i
$i	R[i]	V[i]	C[i]	#V[i]	#C[i]þ#C[i]-#V[i]	A[i]$
.br
\l'4.5i'
.br
12	\ -1	0	1	6	6	\°	0
.br
±	\01	0	1	6	6	\°	0
.br
10	\07	1	1	6	5	\ -1	$inf$
.br
\09	\06	0	0	5	4	\ -1	$inf$
.br
\08	\05	1	1	5	4	\ -1	$inf$
.br
\07	\07	1	0	4	3	\ -1	$inf$
.br
\06	\06	1	0	3	3	\°	0
.br
\05	\05	0	1	2	3	\01	$inf$
.br
\04	\01	1	1	2	2	\°	0
.br
\03	\ -1	0	1	1	1	\°	0
.br
\02	\09	0	0	1	1	\°	0
.br
\01	\08	1	0	1	1	\°	0
.br
\°	\06	0	1	0	1	\01	$inf$
.br
‰Step 1 Step 2 Step 3 Step 4
.sp 
Note: 	Only one bit has bån aìowed for the values of $A$. 
.br
	So the only two poóible values are 0 and $inf$.Š.FG ¢
.bp 0
\&\ 
.RF
.ta 1.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i
.ce
Suãeóful Search
	\l'4i'Š	$alpha$	\0.25	\0.5	\0.75	\0.8	\0.85	\0.9	\0.95
	\l'4i'
	
	$BLP sup 1$	\0\01.1	\0\01.3	\0\01.7	\0\02.0	\0\02.3	\0\02.9	\0\04.2
	
	Na
	15	\0\01.1	\0\01.3	\0\01.7	\0\01.9	\0\02.2	\0\02.8	\0\04.6
	\07	\0\01.1	\0\01.3	\0\01.7	\0\01.9	\0\02.2	\0\02.8	\0\09.7
	\03	\0\01.1	\0\01.3	\0\01.7	\0\01.9	\0\02.4	\0\04.2	\025
	\01	\0\01.1	\0\01.3	\0\02.0	\0\02.5	\0\04.1	\0\08.8	\045
	\°	\0\01.1	\0\01.5	\0\03.3	\0\04.9	\0\07.9	\015	\061
	\0-	\0\04.2	\0\07.1	\020	\030	\049	±0	370
	\0*	\0\03.·	\0\06.°	\018.0	\027.0	\046.4	102	402Š‰$\& sup 1~$Taken from Amble and Knuth [1].
‰- No $A$ field used.
‰* Analytic aðroximation to line above.
.FG ¢
.bp 0
\&
.RF
.ta 1.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i
.ce
Unsuãeóful Search
	\l'4i'Š	$alpha$	\0.25	\0.5	\0.75	\0.8	\0.85	\0.9	\0.95
	\l'4i'Š	$BLP sup 1$	\0\01.3	\0\01.5	\0\02.1	\0\02.3	\0\02.6	\0\03.1	\0\04.4Š	Na
	15	\0\01.2	\0\01.4	\0\01.8	\0\01.9	\0\02.1	\0\02.4	\0\03.5
	\07	\0\01.2	\0\01.4	\0\01.8	\0\01.9	\0\02.1	\0\02.4	\0\09.7
	\03	\0\01.2	\0\01.4	\0\01.8	\0\01.9	\0\02.2	\0\03.3	\015
	\01	\0\01.2	\0\01.4	\0\01.9	\0\02.2	\0\03.2	\0\06.0	\028
	\°	\0\01.2	\0\01.5	\0\02.6	\0\03.4	\0\05.3	\0\09.9	\036
	\0-	\0\01.7	\0\03.4	\0±	\016	\028	\064	²0
	\0*	\0\01.72	\0\03.16	\010.2	\015.6	\027.3	\061.2	247Š‰$\& sup 1~$Taken from Amble and Knuth [1].
‰- No $A$ field used.
‰* Analytic aðroximation to line above.
.FG ¢
.bp 0
\&
.RF
.ta 1.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i +0.5i
	\l'4i'Š	$alpha$	\0.25	\0.5	\0.75	\0.8	\0.85	\0.9	\0.95
	\l'4i'Š‰\0\04.3	\0\08.8	\032	\049	\086	2°	7°
	*	\0\04.56	\0\09.°	\0³.0	\051.0	\089.9\
	201	801Š	* Analytic aðroximation to line above
.FG ¢
.bp 0
\&
.ce 
List of Figures
.sp 2
Fig. 1. Example of compact hash memory and search for key.
.sp 2
Fig. 2. Examples showing diæerent values of $#C[i]-#V[i]$.
.sp 2
Fig. 3. Example of calculation and use of aòay $A$.
.sp 2
.ce
List of Tables
.sp 2
Table I. Average number of probes during a suãeóful search.
.sp 2
Table É. Average number of probes during an unsuãeóful search.
.sp 2
Table É. Average number of probes to move block of memory.
.sp 2
